2025-09-10 09:00:28 (root:426) INFO: [logger] file handler -> exp/tldr/choice_qwen/pfl/fedbiscuit_u3_1.0/plain/exp_print.log
2025-09-10 09:00:28 (root:51) INFO: [main] outdir=exp/tldr/choice_qwen/pfl/fedbiscuit_u3_1.0/plain
2025-09-10 09:00:51 (federatedscope.core.data.base_translator:233) INFO: Main process: Completion file found. Skipping generation.
2025-09-10 09:01:33 (federatedscope.core.data.base_translator:263) INFO: [Final Split Summary][loaded][server=0][rank=0/4] Train=92858, Val=33082, Test=50715, Total=176655
2025-09-10 09:01:33 (federatedscope.core.data.base_translator:272) INFO: [Final Split Summary][loaded][client=1][rank=0/4] Train=2793, Val=146, Test=40, Total=2979
2025-09-10 09:01:33 (federatedscope.core.data.base_translator:272) INFO: [Final Split Summary][loaded][client=2][rank=0/4] Train=214, Val=11, Test=40, Total=265
2025-09-10 09:01:33 (federatedscope.core.data.base_translator:272) INFO: [Final Split Summary][loaded][client=3][rank=0/4] Train=691, Val=36, Test=40, Total=767
2025-09-10 09:01:33 (federatedscope.core.data.base_translator:272) INFO: [Final Split Summary][loaded][client=4][rank=0/4] Train=213, Val=11, Test=40, Total=264
2025-09-10 09:01:33 (federatedscope.core.data.base_translator:272) INFO: [Final Split Summary][loaded][client=5][rank=0/4] Train=285, Val=14, Test=40, Total=339
2025-09-10 09:01:33 (federatedscope.core.data.base_translator:272) INFO: [Final Split Summary][loaded][client=6][rank=0/4] Train=2547, Val=134, Test=40, Total=2721
2025-09-10 09:01:33 (federatedscope.core.data.base_translator:272) INFO: [Final Split Summary][loaded][client=7][rank=0/4] Train=1088, Val=57, Test=40, Total=1185
2025-09-10 09:01:33 (federatedscope.core.data.base_translator:272) INFO: [Final Split Summary][loaded][client=8][rank=0/4] Train=1316, Val=69, Test=40, Total=1425
2025-09-10 09:01:33 (federatedscope.core.data.base_translator:272) INFO: [Final Split Summary][loaded][client=9][rank=0/4] Train=3572, Val=188, Test=40, Total=3800
2025-09-10 09:01:33 (federatedscope.core.data.base_translator:272) INFO: [Final Split Summary][loaded][client=10][rank=0/4] Train=1209, Val=63, Test=40, Total=1312
2025-09-10 09:01:33 (federatedscope.core.data.base_translator:272) INFO: [Final Split Summary][loaded][client=11][rank=0/4] Train=621, Val=32, Test=40, Total=693
2025-09-10 09:01:33 (federatedscope.core.data.base_translator:272) INFO: [Final Split Summary][loaded][client=12][rank=0/4] Train=2605, Val=137, Test=40, Total=2782
2025-09-10 09:01:33 (federatedscope.core.data.base_translator:272) INFO: [Final Split Summary][loaded][client=13][rank=0/4] Train=1372, Val=72, Test=40, Total=1484
2025-09-10 09:01:33 (federatedscope.core.data.base_translator:272) INFO: [Final Split Summary][loaded][client=14][rank=0/4] Train=3055, Val=160, Test=40, Total=3255
2025-09-10 09:01:33 (federatedscope.core.data.base_translator:272) INFO: [Final Split Summary][loaded][client=15][rank=0/4] Train=14550, Val=200, Test=40, Total=14790
2025-09-10 09:01:33 (federatedscope.core.data.base_translator:272) INFO: [Final Split Summary][loaded][client=16][rank=0/4] Train=2589, Val=136, Test=40, Total=2765
2025-09-10 09:01:33 (federatedscope.core.data.base_translator:272) INFO: [Final Split Summary][loaded][client=17][rank=0/4] Train=5883, Val=200, Test=40, Total=6123
2025-09-10 09:01:33 (federatedscope.core.data.base_translator:272) INFO: [Final Split Summary][loaded][client=18][rank=0/4] Train=2576, Val=135, Test=40, Total=2751
2025-09-10 09:01:33 (federatedscope.core.data.base_translator:272) INFO: [Final Split Summary][loaded][client=19][rank=0/4] Train=2102, Val=110, Test=40, Total=2252
2025-09-10 09:01:33 (federatedscope.core.data.base_translator:272) INFO: [Final Split Summary][loaded][client=20][rank=0/4] Train=2399, Val=126, Test=40, Total=2565
2025-09-10 09:01:33 (federatedscope.core.data.base_translator:272) INFO: [Final Split Summary][loaded][client=21][rank=0/4] Train=2915, Val=153, Test=40, Total=3108
2025-09-10 09:01:33 (federatedscope.core.data.base_translator:272) INFO: [Final Split Summary][loaded][client=22][rank=0/4] Train=224, Val=11, Test=40, Total=275
2025-09-10 09:01:33 (federatedscope.core.data.base_translator:272) INFO: [Final Split Summary][loaded][client=23][rank=0/4] Train=583, Val=30, Test=40, Total=653
2025-09-10 09:01:33 (federatedscope.core.data.base_translator:272) INFO: [Final Split Summary][loaded][client=24][rank=0/4] Train=4944, Val=200, Test=40, Total=5184
2025-09-10 09:01:33 (federatedscope.core.data.base_translator:272) INFO: [Final Split Summary][loaded][client=25][rank=0/4] Train=4647, Val=200, Test=40, Total=4887
2025-09-10 09:01:33 (federatedscope.core.data.base_translator:272) INFO: [Final Split Summary][loaded][client=26][rank=0/4] Train=3063, Val=161, Test=40, Total=3264
2025-09-10 09:01:33 (federatedscope.core.data.base_translator:272) INFO: [Final Split Summary][loaded][client=27][rank=0/4] Train=2342, Val=123, Test=40, Total=2505
2025-09-10 09:01:33 (federatedscope.core.data.base_translator:272) INFO: [Final Split Summary][loaded][client=28][rank=0/4] Train=1434, Val=75, Test=40, Total=1549
2025-09-10 09:01:33 (federatedscope.core.data.base_translator:272) INFO: [Final Split Summary][loaded][client=29][rank=0/4] Train=6191, Val=200, Test=40, Total=6431
2025-09-10 09:01:33 (federatedscope.core.data.base_translator:272) INFO: [Final Split Summary][loaded][client=30][rank=0/4] Train=3247, Val=170, Test=40, Total=3457
2025-09-10 09:01:33 (federatedscope.core.data.base_translator:272) INFO: [Final Split Summary][loaded][client=31][rank=0/4] Train=3679, Val=193, Test=40, Total=3912
2025-09-10 09:01:33 (federatedscope.core.data.base_translator:272) INFO: [Final Split Summary][loaded][client=32][rank=0/4] Train=2144, Val=112, Test=40, Total=2296
2025-09-10 09:01:33 (federatedscope.core.data.base_translator:272) INFO: [Final Split Summary][loaded][client=33][rank=0/4] Train=1409, Val=74, Test=40, Total=1523
2025-09-10 09:01:33 (federatedscope.core.data.base_translator:272) INFO: [Final Split Summary][loaded][client=34][rank=0/4] Train=4486, Val=200, Test=40, Total=4726
2025-09-10 09:01:33 (federatedscope.core.data.base_translator:272) INFO: [Final Split Summary][loaded][client=35][rank=0/4] Train=4736, Val=200, Test=40, Total=4976
2025-09-10 09:01:33 (federatedscope.core.data.base_translator:272) INFO: [Final Split Summary][loaded][client=36][rank=0/4] Train=1030, Val=54, Test=40, Total=1124
2025-09-10 09:01:33 (federatedscope.core.data.base_translator:272) INFO: [Final Split Summary][loaded][client=37][rank=0/4] Train=4273, Val=200, Test=40, Total=4513
2025-09-10 09:01:33 (federatedscope.core.data.base_translator:272) INFO: [Final Split Summary][loaded][client=38][rank=0/4] Train=6171, Val=200, Test=40, Total=6411
2025-09-10 09:01:33 (federatedscope.core.data.base_translator:272) INFO: [Final Split Summary][loaded][client=39][rank=0/4] Train=1594, Val=83, Test=40, Total=1717
2025-09-10 09:01:33 (federatedscope.core.data.base_translator:272) INFO: [Final Split Summary][loaded][client=40][rank=0/4] Train=4005, Val=200, Test=40, Total=4245
2025-09-10 09:01:33 (federatedscope.core.data.base_translator:272) INFO: [Final Split Summary][loaded][client=41][rank=0/4] Train=2275, Val=119, Test=40, Total=2434
2025-09-10 09:01:33 (federatedscope.core.data.base_translator:272) INFO: [Final Split Summary][loaded][client=42][rank=0/4] Train=5772, Val=200, Test=40, Total=6012
2025-09-10 09:01:33 (federatedscope.core.data.base_translator:272) INFO: [Final Split Summary][loaded][client=43][rank=0/4] Train=1694, Val=89, Test=40, Total=1823
2025-09-10 09:01:33 (federatedscope.core.data.base_translator:272) INFO: [Final Split Summary][loaded][client=44][rank=0/4] Train=7916, Val=200, Test=40, Total=8156
2025-09-10 09:01:33 (federatedscope.core.data.base_translator:272) INFO: [Final Split Summary][loaded][client=45][rank=0/4] Train=1901, Val=100, Test=40, Total=2041
2025-09-10 09:01:33 (federatedscope.core.data.base_translator:272) INFO: [Final Split Summary][loaded][client=46][rank=0/4] Train=2100, Val=110, Test=40, Total=2250
2025-09-10 09:01:33 (federatedscope.core.data.base_translator:272) INFO: [Final Split Summary][loaded][client=47][rank=0/4] Train=2812, Val=147, Test=40, Total=2999
2025-09-10 09:01:33 (federatedscope.core.data.base_translator:272) INFO: [Final Split Summary][loaded][client=48][rank=0/4] Train=880, Val=46, Test=40, Total=966
2025-09-10 09:01:33 (federatedscope.core.data.base_translator:272) INFO: [Final Split Summary][loaded][client=49][rank=0/4] Train=2521, Val=132, Test=40, Total=2693
2025-09-10 09:01:33 (federatedscope.core.data.base_translator:272) INFO: [Final Split Summary][loaded][client=50][rank=0/4] Train=2527, Val=133, Test=40, Total=2700
2025-09-10 09:01:33 (federatedscope.core.data.base_translator:272) INFO: [Final Split Summary][loaded][client=51][rank=0/4] Train=1580, Val=83, Test=40, Total=1703
2025-09-10 09:01:33 (federatedscope.core.data.base_translator:272) INFO: [Final Split Summary][loaded][client=52][rank=0/4] Train=3589, Val=188, Test=40, Total=3817
2025-09-10 09:01:33 (federatedscope.core.data.base_translator:272) INFO: [Final Split Summary][loaded][client=53][rank=0/4] Train=6791, Val=200, Test=40, Total=7031
2025-09-10 09:01:33 (federatedscope.core.configs.config:256) INFO: the used configs are: 
adapter:
  use: False
aggregator:
  BFT_args:
    
  byzantine_node_num: 0
  inside_weight: 1.0
  num_agg_groups: 1
  num_agg_topk: []
  outside_weight: 0.0
  robust_rule: fedavg
asyn:
  use: False
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  edge_num: 100
  edge_path: edge_data/
  freq: 10
  info_diff_type: l2
  inject_round: 0
  insert_round: 100000
  label_type: dirty
  max_ite: 400
  mean: [0.9637]
  mia_is_simulate_in: False
  mia_simulate_in_round: 20
  pgd_eps: 2
  pgd_lr: 0.1
  pgd_poisoning: False
  poison_ratio: 0.5
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  scale_para: 1.0
  scale_poisoning: False
  self_epoch: 6
  self_lr: 0.05
  self_opt: False
  setting: fix
  std: [0.1592]
  target_label_ind: -1
  trigger_path: trigger/
  trigger_type: edge
backend: torch
cfg_file: 
check_completeness: False
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 64
  cSBM_phi: [0.5, 0.5, 0.5]
  cache_dir: 
  consistent_label_distribution: True
  drop_last: False
  file_path: 
  hetero_data_name: []
  hetero_synth_batch_size: 32
  hetero_synth_feat_dim: 128
  hetero_synth_prim_weight: 0.5
  is_debug: False
  load_splits: False
  loader: 
  max_query_len: 128
  max_seq_len: 384
  max_tgt_len: 128
  num_contrast: 0
  num_of_client_for_data: []
  num_steps: 30
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  save_data: False
  save_splits: False
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.9, 0.09, 0.01]
  splits_path: ./final_data_splits
  splitter: meta
  splitter_args: []
  subsample: 1.0
  target_transform: []
  test_pre_transform: []
  test_target_transform: []
  test_transform: []
  transform: []
  trunc_stride: 128
  type: reddit-tldr-comparison-choice@llm
  val_pre_transform: []
  val_target_transform: []
  val_transform: []
  walk_length: 2
dataloader:
  batch_size: 2
  drop_last: False
  num_steps: 30
  num_workers: 0
  pin_memory: False
  shuffle: True
  sizes: [10, 5]
  theta: -1
  type: base
  walk_length: 2
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 0
eval:
  baseline_before_ft: True
  best_res_update_round_wise_key: val_loss
  count_flops: False
  every_n_train_steps: 10
  freq: 1
  metrics: ['loss', 'acc']
  monitoring: []
  outdir: exp/tldr/choice_qwen/pfl/fedbiscuit_u3_1.0/plain/raw
  report: ['weighted_avg', 'avg', 'fairness', 'raw']
  split: ['val', 'test']
expname: 
expname_tag: 
feat_engr:
  num_bins: 5
  scenario: hfl
  secure:
    dp:
      
    encrypt:
      type: dummy
    key_size: 3072
    type: encrypt
  selec_threshold: 0.05
  selec_woe_binning: quantile
  type: 
federate:
  atc_load_from: 
  atc_vanilla: False
  client_idx_for_local_train: 0
  client_num: 53
  data_weighted_aggr: False
  ignore_weight: True
  join_in_info: []
  make_global_eval: False
  master_addr: 127.0.0.1
  master_port: 29500
  merge_test_data: False
  merge_val_data: False
  method: FedAvg
  mode: standalone
  online_aggr: False
  process_num: 1
  resource_info_file: 
  restore_from: 
  sample_client_num: 53
  sample_client_rate: -1.0
  sampler: uniform
  save_client_model: False
  save_freq: -1
  save_to: 
  share_local_model: True
  total_round_num: 1
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  use: False
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
fedswa:
  use: False
finetune:
  batch_or_epoch: epoch
  before_eval: False
  epoch_linear: 10
  freeze_param: 
  local_param: []
  local_update_steps: 1
  lr_linear: 0.005
  optimizer:
    lr: 0.1
    type: SGD
  scheduler:
    type: 
    warmup_ratio: 0.0
  simple_tuning: False
  weight_decay: 0.0
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_accum_count: 1
  grad_clip: -1.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    pi_lr: 0.01
    psn: False
    sched: auto
    ss: 
    use: False
  fts:
    M: 100
    M_target: 200
    allow_load_existing_info: True
    diff: False
    fed_bo_max_iter: 50
    g_var: 1e-06
    gp_opt_schedule: 1
    local_bo_epochs: 50
    local_bo_max_iter: 50
    ls: 1.0
    obs_noise: 1e-06
    ss: 
    target_clients: []
    use: False
    v_kernel: 1.0
    var: 0.1
  init_cand_num: 16
  larger_better: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  pfedhpo:
    discrete: False
    ss: 
    target_fl_total_round: 1000
    train_anchor: False
    train_fl: False
    use: False
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    iter: 0
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
  trial_index: 0
  working_folder: hpo
llm:
  accelerator:
    config: 
    use: True
  adapter:
    args: [{'adapter_package': 'peft', 'adapter_method': 'lora', 'r': 8, 'lora_alpha': 16, 'lora_dropout': 0.05, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']}]
    balance: False
    count: 3
    grouping:
      round: 0
      use: True
    local_only: False
    mv_to_cpu: False
    use: True
    warmup:
      round: 10
      use: False
  cache:
    model: 
  chat:
    max_history_len: 10
    max_len: 1024
  deepspeed:
    ds_config: 
    use: False
  fedrlhf:
    config_file: 
    frequency: 100
    pretrained: False
    train:
      batch_or_epoch: batch
      local_update_steps: 10
    use: False
  grad_accum_step: 2
  max_new_token: 60
  num_completions: 2
  offsite_tuning:
    emu_align:
      data:
        root: data
        splits: [0.8, 0.1, 0.1]
        type: alpaca@llm
      exit_after_align: False
      init_enable_ground_truth: False
      initial_only: True
      kl_divergence: raw
      layerwise_distill: False
      restore_from: 
      save_to: 
      sim_loss: l2
      train:
        batch_or_epoch: batch
        enable_ground_truth: False
        initial_update_rounds: 50
        kd_loss_weight: 0.9
        lm_loss_weight: 0.1
        local_update_steps: 10
        optimizer:
          lr: 0.01
          type: SGD
      use: False
    emu_l: 1
    emu_r: 10
    eval_type: emu
    kwargs: [{}]
    llm_generated:
      ratio: 0.1
      use: False
    save_full_model: False
    strategy: drop_layer
    use: False
  retry_on_nan_loss: False
  reward_coeff: 0.1
  rlhf: False
  tok_len: 1024
model:
  contrast_temp: 1.0
  contrast_topk: 100
  downstream_tasks: []
  dropout: 0.5
  embed_size: 8
  gamma: 0
  graph_pooling: mean
  hidden: 256
  in_channels: 0
  input_shape: ()
  label_smoothing: 0.1
  lambda_: 0.1
  layer: 2
  length_penalty: 2.0
  llm_kwargs: [{}]
  llm_type: CausalLM
  load_from_local_pretrained_fs_config: 
  load_from_local_pretrained_model_path: checkpoints_1.0/final_tldr_choice_qwen_fedbiscuit_u3_round_175.ckpt
  max_answer_len: 30
  max_length: 200
  max_tree_depth: 3
  min_length: 1
  model_num_per_trainer: 1
  model_type: google/bert_uncased_L-2_H-128_A-2
  n_best_size: 20
  no_repeat_ngram_size: 3
  null_score_diff_threshold: 0.0
  num_beams: 5
  num_item: 0
  num_labels: 1
  num_of_trees: 10
  num_user: 0
  out_channels: 1
  pretrain_tasks: []
  stage: 
  task: node
  type: Qwen/Qwen2-0.5B@huggingface_llm
  use_bias: True
  use_contrastive_loss: False
nbafl:
  use: False
outdir: exp/tldr/choice_qwen/pfl/fedbiscuit_u3_1.0/plain
personalization:
  K: 5
  beta: 1.0
  epoch_feature: 1
  epoch_linear: 2
  local_param: []
  local_update_steps: 100
  lr: 1e-05
  lr_feature: 0.1
  lr_linear: 0.1
  regular_weight: 0.1
  share_non_trainable_para: False
  weight_decay: 0.0
print_decimal_digits: 6
quantization:
  method: none
  nbits: 8
regularizer:
  mu: 0.0
  type: 
seed: 0
sgdmf:
  use: False
train:
  batch_or_epoch: batch
  data_para_dids: []
  is_enable_half: True
  local_update_steps: 100
  optimizer:
    betas: (0.9, 0.95)
    lr: 1e-05
    type: AdamW
  scheduler:
    type: 
    warmup_ratio: 0.0
trainer:
  choices: ['A', 'B']
  disp_freq: 50
  local_entropy:
    alpha: 0.75
    eps: 0.0001
    gamma: 0.03
    inc_factor: 1.0
  sam:
    adaptive: False
    eta: 0.0
    rho: 1.0
  type: llmrewardchoicetrainer
  val_freq: 100000000
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False
2025-09-10 09:01:35 (federatedscope.core.auxiliaries.utils:175) INFO: The device information file is not provided
2025-09-10 09:01:35 (federatedscope.core.auxiliaries.model_builder:139) WARNING: The input shape is None. Please specify the `data.input_shape`(a tuple) or give the representative data to `get_model` if necessary
2025-09-10 09:01:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-build][rank=0] tok_len=151643 | base=Qwen2ForCausalLM | in_emb=(Embedding) num=151646 ptr=140169377046592 | out_emb=(Linear) num=151646 ptr=140169377046592 | lora_ptr=None
2025-09-10 09:01:51 (federatedscope.llm.model.model_builder:187) INFO: [Warmup-Init] loaded from checkpoints_1.0/final_tldr_choice_qwen_fedbiscuit_u3_round_175.ckpt (round=175) | missing=291 unexpected=0
2025-09-10 09:01:51 (federatedscope.core.fed_runner:211) INFO: Server has been set up ... 
2025-09-10 09:01:52 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-10 09:01:54 (federatedscope.core.fed_runner:275) INFO: Client 1 has been set up ... 
2025-09-10 09:01:55 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-10 09:01:57 (federatedscope.core.fed_runner:275) INFO: Client 2 has been set up ... 
2025-09-10 09:01:57 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-10 09:02:00 (federatedscope.core.fed_runner:275) INFO: Client 3 has been set up ... 
2025-09-10 09:02:00 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-10 09:02:02 (federatedscope.core.fed_runner:275) INFO: Client 4 has been set up ... 
2025-09-10 09:02:02 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-10 09:02:05 (federatedscope.core.fed_runner:275) INFO: Client 5 has been set up ... 
2025-09-10 09:02:05 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-10 09:02:08 (federatedscope.core.fed_runner:275) INFO: Client 6 has been set up ... 
2025-09-10 09:02:08 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-10 09:02:10 (federatedscope.core.fed_runner:275) INFO: Client 7 has been set up ... 
2025-09-10 09:02:11 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-10 09:02:13 (federatedscope.core.fed_runner:275) INFO: Client 8 has been set up ... 
2025-09-10 09:02:13 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-10 09:02:15 (federatedscope.core.fed_runner:275) INFO: Client 9 has been set up ... 
2025-09-10 09:02:16 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-10 09:02:18 (federatedscope.core.fed_runner:275) INFO: Client 10 has been set up ... 
2025-09-10 09:02:18 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-10 09:02:21 (federatedscope.core.fed_runner:275) INFO: Client 11 has been set up ... 
2025-09-10 09:02:21 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-10 09:02:23 (federatedscope.core.fed_runner:275) INFO: Client 12 has been set up ... 
2025-09-10 09:02:23 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-10 09:02:26 (federatedscope.core.fed_runner:275) INFO: Client 13 has been set up ... 
2025-09-10 09:02:26 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-10 09:02:28 (federatedscope.core.fed_runner:275) INFO: Client 14 has been set up ... 
2025-09-10 09:02:29 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-10 09:02:31 (federatedscope.core.fed_runner:275) INFO: Client 15 has been set up ... 
2025-09-10 09:02:31 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-10 09:02:34 (federatedscope.core.fed_runner:275) INFO: Client 16 has been set up ... 
2025-09-10 09:02:34 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-10 09:02:36 (federatedscope.core.fed_runner:275) INFO: Client 17 has been set up ... 
2025-09-10 09:02:36 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-10 09:02:39 (federatedscope.core.fed_runner:275) INFO: Client 18 has been set up ... 
2025-09-10 09:02:39 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-10 09:02:42 (federatedscope.core.fed_runner:275) INFO: Client 19 has been set up ... 
2025-09-10 09:02:43 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-10 09:02:45 (federatedscope.core.fed_runner:275) INFO: Client 20 has been set up ... 
2025-09-10 09:02:46 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-10 09:02:48 (federatedscope.core.fed_runner:275) INFO: Client 21 has been set up ... 
2025-09-10 09:02:48 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-10 09:02:51 (federatedscope.core.fed_runner:275) INFO: Client 22 has been set up ... 
2025-09-10 09:02:51 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-10 09:02:54 (federatedscope.core.fed_runner:275) INFO: Client 23 has been set up ... 
2025-09-10 09:02:54 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-10 09:02:56 (federatedscope.core.fed_runner:275) INFO: Client 24 has been set up ... 
2025-09-10 09:02:56 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-10 09:02:59 (federatedscope.core.fed_runner:275) INFO: Client 25 has been set up ... 
2025-09-10 09:02:59 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-10 09:03:02 (federatedscope.core.fed_runner:275) INFO: Client 26 has been set up ... 
2025-09-10 09:03:02 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-10 09:03:04 (federatedscope.core.fed_runner:275) INFO: Client 27 has been set up ... 
2025-09-10 09:03:05 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-10 09:03:07 (federatedscope.core.fed_runner:275) INFO: Client 28 has been set up ... 
2025-09-10 09:03:07 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-10 09:03:10 (federatedscope.core.fed_runner:275) INFO: Client 29 has been set up ... 
2025-09-10 09:03:10 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-10 09:03:13 (federatedscope.core.fed_runner:275) INFO: Client 30 has been set up ... 
2025-09-10 09:03:13 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-10 09:03:15 (federatedscope.core.fed_runner:275) INFO: Client 31 has been set up ... 
2025-09-10 09:03:15 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-10 09:03:18 (federatedscope.core.fed_runner:275) INFO: Client 32 has been set up ... 
2025-09-10 09:03:18 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-10 09:03:21 (federatedscope.core.fed_runner:275) INFO: Client 33 has been set up ... 
2025-09-10 09:03:21 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-10 09:03:23 (federatedscope.core.fed_runner:275) INFO: Client 34 has been set up ... 
2025-09-10 09:03:24 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-10 09:03:26 (federatedscope.core.fed_runner:275) INFO: Client 35 has been set up ... 
2025-09-10 09:03:26 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-10 09:03:29 (federatedscope.core.fed_runner:275) INFO: Client 36 has been set up ... 
2025-09-10 09:03:29 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-10 09:03:31 (federatedscope.core.fed_runner:275) INFO: Client 37 has been set up ... 
2025-09-10 09:03:31 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-10 09:03:34 (federatedscope.core.fed_runner:275) INFO: Client 38 has been set up ... 
2025-09-10 09:03:34 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-10 09:03:38 (federatedscope.core.fed_runner:275) INFO: Client 39 has been set up ... 
2025-09-10 09:03:38 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-10 09:03:40 (federatedscope.core.fed_runner:275) INFO: Client 40 has been set up ... 
2025-09-10 09:03:40 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-10 09:03:43 (federatedscope.core.fed_runner:275) INFO: Client 41 has been set up ... 
2025-09-10 09:03:43 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-10 09:03:45 (federatedscope.core.fed_runner:275) INFO: Client 42 has been set up ... 
2025-09-10 09:03:46 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-10 09:03:48 (federatedscope.core.fed_runner:275) INFO: Client 43 has been set up ... 
2025-09-10 09:03:48 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-10 09:03:50 (federatedscope.core.fed_runner:275) INFO: Client 44 has been set up ... 
2025-09-10 09:03:51 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-10 09:03:53 (federatedscope.core.fed_runner:275) INFO: Client 45 has been set up ... 
2025-09-10 09:03:53 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-10 09:03:56 (federatedscope.core.fed_runner:275) INFO: Client 46 has been set up ... 
2025-09-10 09:03:56 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-10 09:03:58 (federatedscope.core.fed_runner:275) INFO: Client 47 has been set up ... 
2025-09-10 09:03:59 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-10 09:04:01 (federatedscope.core.fed_runner:275) INFO: Client 48 has been set up ... 
2025-09-10 09:04:01 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-10 09:04:04 (federatedscope.core.fed_runner:275) INFO: Client 49 has been set up ... 
2025-09-10 09:04:04 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-10 09:04:06 (federatedscope.core.fed_runner:275) INFO: Client 50 has been set up ... 
2025-09-10 09:04:06 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-10 09:04:09 (federatedscope.core.fed_runner:275) INFO: Client 51 has been set up ... 
2025-09-10 09:04:09 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-10 09:04:11 (federatedscope.core.fed_runner:275) INFO: Client 52 has been set up ... 
2025-09-10 09:04:12 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-10 09:04:14 (federatedscope.core.fed_runner:275) INFO: Client 53 has been set up ... 
2025-09-10 09:04:14 (federatedscope.core.trainers.trainer:569) INFO: Model meta-info: <class 'federatedscope.llm.model.adapter_builder.AdapterModel'>.
2025-09-10 09:04:14 (federatedscope.core.trainers.trainer:584) INFO: Num of original para names: 1344.
2025-09-10 09:04:14 (federatedscope.core.trainers.trainer:585) INFO: Num of original trainable para names: 1634.
2025-09-10 09:04:14 (federatedscope.core.trainers.trainer:587) INFO: Num of preserved para names in local update: 1344. 
Preserved para names in local update: {'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_1.weight'}.
2025-09-10 09:04:14 (federatedscope.core.trainers.trainer:591) INFO: Num of filtered para names in local update: 0. 
Filtered para names in local update: set().
2025-09-10 09:04:14 (federatedscope.core.trainers.trainer:599) INFO: After register default hooks,
	the hooks_in_train is:
	{
	  "on_fit_start": [
	    "_hook_on_fit_start_numerical_precision",
	    "_hook_on_data_parallel_init",
	    "_hook_on_fit_start_init",
	    "_hook_on_fit_start_calculate_model_size"
	  ],
	  "on_batch_start": [
	    "_hook_on_batch_start_init"
	  ],
	  "on_batch_forward": [
	    "_hook_on_batch_forward",
	    "_hook_on_batch_forward_regularizer",
	    "_hook_on_batch_forward_flop_count"
	  ],
	  "on_batch_backward": [
	    "_hook_on_batch_backward"
	  ],
	  "on_batch_end": [
	    "_hook_on_batch_end"
	  ],
	  "on_fit_end": [
	    "_hook_on_fit_end",
	    "_hook_on_fit_end_free_space"
	  ]
	};
	the hooks_in_eval is:
            t{
	  "on_fit_start": [
	    "_hook_on_fit_start_numerical_precision",
	    "_hook_on_data_parallel_init",
	    "_hook_on_fit_start_init"
	  ],
	  "on_batch_start": [
	    "_hook_on_batch_start_init"
	  ],
	  "on_batch_forward": [
	    "_hook_on_batch_forward"
	  ],
	  "on_batch_end": [
	    "_hook_on_batch_end"
	  ],
	  "on_fit_end": [
	    "_hook_on_fit_end",
	    "_hook_on_fit_end_free_space"
	  ]
	}
2025-09-10 09:04:14 (federatedscope.llm.llm_local.server:103) INFO: Waited all clients join, start now...
2025-09-10 09:04:14 (federatedscope.llm.llm_local.server:111) INFO: ----------- Starting training (Round #0) -------------
2025-09-10 09:04:14 (federatedscope.llm.llm_local.server:114) INFO: Server: Performing a grouping step...
2025-09-10 09:04:26 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:04:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-09-10 09:04:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140169377046592 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:04:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=73, num_train_batch_last_epoch=27, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:04:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-10 09:04:36 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=146, loss_sum=96.828064, avg_loss=0.663206, seen=146, correct=88, accuracy=0.602740
2025-09-10 09:04:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:04:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:04:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:04:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1786MB allocated=1759MB
2025-09-10 09:04:38 (federatedscope.llm.llm_local.client:480) INFO: Client 1 Adapter 0 with val results: {'val_total': 146, 'val_loss': 96.82806396484375, 'val_avg_loss': 0.663205917567423, 'val_seen': 146, 'val_correct': 88, 'val_acc': 0.6027397260273972}
2025-09-10 09:04:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:04:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:04:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:04:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:04:39 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.190872, avg_loss=0.579772, seen=40, correct=26, accuracy=0.650000
2025-09-10 09:04:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:04:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:04:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:04:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1786MB allocated=1759MB
2025-09-10 09:04:40 (federatedscope.llm.llm_local.client:501) INFO: Client 1 Adapter 0 with test results: {'test_total': 40, 'test_loss': 23.190872192382812, 'test_avg_loss': 0.5797718048095704, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-10 09:04:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-09-10 09:04:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:04:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=73, num_train_batch_last_epoch=27, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:04:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-10 09:04:43 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=146, loss_sum=98.336464, avg_loss=0.673537, seen=146, correct=82, accuracy=0.561644
2025-09-10 09:04:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:04:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:04:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:04:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1786MB allocated=1734MB
2025-09-10 09:04:45 (federatedscope.llm.llm_local.client:480) INFO: Client 1 Adapter 1 with val results: {'val_total': 146, 'val_loss': 98.33646392822266, 'val_avg_loss': 0.6735374241659086, 'val_seen': 146, 'val_correct': 82, 'val_acc': 0.5616438356164384}
2025-09-10 09:04:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:04:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:04:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:04:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:04:46 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.320623, avg_loss=0.633016, seen=40, correct=27, accuracy=0.675000
2025-09-10 09:04:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:04:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:04:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:04:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1786MB allocated=1734MB
2025-09-10 09:04:47 (federatedscope.llm.llm_local.client:501) INFO: Client 1 Adapter 1 with test results: {'test_total': 40, 'test_loss': 25.32062339782715, 'test_avg_loss': 0.6330155849456787, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-10 09:04:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-09-10 09:04:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:04:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=73, num_train_batch_last_epoch=27, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:04:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-10 09:04:50 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=146, loss_sum=99.452560, avg_loss=0.681182, seen=146, correct=83, accuracy=0.568493
2025-09-10 09:04:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:04:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:04:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:04:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1786MB allocated=1734MB
2025-09-10 09:04:52 (federatedscope.llm.llm_local.client:480) INFO: Client 1 Adapter 2 with val results: {'val_total': 146, 'val_loss': 99.45256042480469, 'val_avg_loss': 0.6811819207178403, 'val_seen': 146, 'val_correct': 83, 'val_acc': 0.5684931506849316}
2025-09-10 09:04:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:04:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:04:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:04:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:04:53 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.903856, avg_loss=0.647596, seen=40, correct=25, accuracy=0.625000
2025-09-10 09:04:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:04:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:04:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:04:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1786MB allocated=1734MB
2025-09-10 09:04:55 (federatedscope.llm.llm_local.client:501) INFO: Client 1 Adapter 2 with test results: {'test_total': 40, 'test_loss': 25.90385627746582, 'test_avg_loss': 0.6475964069366456, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-10 09:04:55 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:04:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-10 09:04:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:04:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=4, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:04:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-10 09:04:57 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=8.809361, avg_loss=0.800851, seen=11, correct=5, accuracy=0.454545
2025-09-10 09:04:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:04:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:04:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:04:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1786MB allocated=1768MB
2025-09-10 09:04:58 (federatedscope.llm.llm_local.client:480) INFO: Client 2 Adapter 0 with val results: {'val_total': 11, 'val_loss': 8.80936050415039, 'val_avg_loss': 0.8008509549227628, 'val_seen': 11, 'val_correct': 5, 'val_acc': 0.45454545454545453}
2025-09-10 09:04:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:04:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:04:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:05:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:05:01 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.737072, avg_loss=0.718427, seen=40, correct=24, accuracy=0.600000
2025-09-10 09:05:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:05:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:05:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:05:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1786MB allocated=1768MB
2025-09-10 09:05:02 (federatedscope.llm.llm_local.client:501) INFO: Client 2 Adapter 0 with test results: {'test_total': 40, 'test_loss': 28.737071990966797, 'test_avg_loss': 0.7184267997741699, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-10 09:05:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-10 09:05:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:05:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=4, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:05:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-10 09:05:03 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.854023, avg_loss=0.714002, seen=11, correct=7, accuracy=0.636364
2025-09-10 09:05:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:05:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:05:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:05:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1786MB allocated=1743MB
2025-09-10 09:05:05 (federatedscope.llm.llm_local.client:480) INFO: Client 2 Adapter 1 with val results: {'val_total': 11, 'val_loss': 7.854022979736328, 'val_avg_loss': 0.714002089066939, 'val_seen': 11, 'val_correct': 7, 'val_acc': 0.6363636363636364}
2025-09-10 09:05:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:05:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:05:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:05:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:05:07 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.402802, avg_loss=0.635070, seen=40, correct=27, accuracy=0.675000
2025-09-10 09:05:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:05:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:05:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:05:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1786MB allocated=1743MB
2025-09-10 09:05:08 (federatedscope.llm.llm_local.client:501) INFO: Client 2 Adapter 1 with test results: {'test_total': 40, 'test_loss': 25.402801513671875, 'test_avg_loss': 0.6350700378417968, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-10 09:05:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-10 09:05:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:05:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=4, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:05:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-10 09:05:09 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=9.111066, avg_loss=0.828279, seen=11, correct=3, accuracy=0.272727
2025-09-10 09:05:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:05:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:05:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:05:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1786MB allocated=1743MB
2025-09-10 09:05:10 (federatedscope.llm.llm_local.client:480) INFO: Client 2 Adapter 2 with val results: {'val_total': 11, 'val_loss': 9.111065864562988, 'val_avg_loss': 0.8282787149602716, 'val_seen': 11, 'val_correct': 3, 'val_acc': 0.2727272727272727}
2025-09-10 09:05:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:05:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:05:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:05:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:05:12 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.726257, avg_loss=0.793156, seen=40, correct=17, accuracy=0.425000
2025-09-10 09:05:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:05:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:05:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:05:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1786MB allocated=1743MB
2025-09-10 09:05:13 (federatedscope.llm.llm_local.client:501) INFO: Client 2 Adapter 2 with test results: {'test_total': 40, 'test_loss': 31.72625732421875, 'test_avg_loss': 0.7931564331054688, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-09-10 09:05:13 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:05:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-09-10 09:05:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:05:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=18, num_train_batch_last_epoch=10, num_train_epoch=6, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:05:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-09-10 09:05:15 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=36, loss_sum=28.697495, avg_loss=0.797153, seen=36, correct=17, accuracy=0.472222
2025-09-10 09:05:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:05:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:05:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:05:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1786MB allocated=1776MB
2025-09-10 09:05:17 (federatedscope.llm.llm_local.client:480) INFO: Client 3 Adapter 0 with val results: {'val_total': 36, 'val_loss': 28.697494506835938, 'val_avg_loss': 0.7971526251898872, 'val_seen': 36, 'val_correct': 17, 'val_acc': 0.4722222222222222}
2025-09-10 09:05:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:05:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:05:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:05:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:05:19 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.530434, avg_loss=0.688261, seen=40, correct=21, accuracy=0.525000
2025-09-10 09:05:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:05:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:05:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:05:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1786MB allocated=1776MB
2025-09-10 09:05:21 (federatedscope.llm.llm_local.client:501) INFO: Client 3 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.530433654785156, 'test_avg_loss': 0.688260841369629, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-10 09:05:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-09-10 09:05:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:05:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=18, num_train_batch_last_epoch=10, num_train_epoch=6, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:05:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-09-10 09:05:23 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=36, loss_sum=28.066769, avg_loss=0.779632, seen=36, correct=17, accuracy=0.472222
2025-09-10 09:05:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:05:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:05:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:05:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1786MB allocated=1751MB
2025-09-10 09:05:24 (federatedscope.llm.llm_local.client:480) INFO: Client 3 Adapter 1 with val results: {'val_total': 36, 'val_loss': 28.066768646240234, 'val_avg_loss': 0.779632462395562, 'val_seen': 36, 'val_correct': 17, 'val_acc': 0.4722222222222222}
2025-09-10 09:05:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:05:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:05:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:05:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:05:25 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.269459, avg_loss=0.706736, seen=40, correct=22, accuracy=0.550000
2025-09-10 09:05:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:05:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:05:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:05:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1786MB allocated=1751MB
2025-09-10 09:05:27 (federatedscope.llm.llm_local.client:501) INFO: Client 3 Adapter 1 with test results: {'test_total': 40, 'test_loss': 28.269458770751953, 'test_avg_loss': 0.7067364692687989, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-10 09:05:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-09-10 09:05:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:05:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=18, num_train_batch_last_epoch=10, num_train_epoch=6, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:05:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-09-10 09:05:29 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=36, loss_sum=26.420765, avg_loss=0.733910, seen=36, correct=18, accuracy=0.500000
2025-09-10 09:05:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:05:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:05:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:05:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1786MB allocated=1751MB
2025-09-10 09:05:30 (federatedscope.llm.llm_local.client:480) INFO: Client 3 Adapter 2 with val results: {'val_total': 36, 'val_loss': 26.420764923095703, 'val_avg_loss': 0.7339101367526584, 'val_seen': 36, 'val_correct': 18, 'val_acc': 0.5}
2025-09-10 09:05:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:05:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:05:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:05:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:05:31 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.402245, avg_loss=0.710056, seen=40, correct=23, accuracy=0.575000
2025-09-10 09:05:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:05:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:05:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:05:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1786MB allocated=1751MB
2025-09-10 09:05:32 (federatedscope.llm.llm_local.client:501) INFO: Client 3 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.402244567871094, 'test_avg_loss': 0.7100561141967774, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-10 09:05:33 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:05:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-10 09:05:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:05:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=4, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:05:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-10 09:05:34 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=8.009118, avg_loss=0.728102, seen=11, correct=6, accuracy=0.545455
2025-09-10 09:05:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:05:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:05:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:05:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1806MB allocated=1785MB
2025-09-10 09:05:36 (federatedscope.llm.llm_local.client:480) INFO: Client 4 Adapter 0 with val results: {'val_total': 11, 'val_loss': 8.00911808013916, 'val_avg_loss': 0.7281016436490145, 'val_seen': 11, 'val_correct': 6, 'val_acc': 0.5454545454545454}
2025-09-10 09:05:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:05:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:05:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:05:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:05:38 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.872456, avg_loss=0.696811, seen=40, correct=20, accuracy=0.500000
2025-09-10 09:05:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:05:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:05:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:05:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1806MB allocated=1785MB
2025-09-10 09:05:40 (federatedscope.llm.llm_local.client:501) INFO: Client 4 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.872455596923828, 'test_avg_loss': 0.6968113899230957, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-10 09:05:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-10 09:05:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:05:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=4, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:05:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-10 09:05:41 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.910979, avg_loss=0.719180, seen=11, correct=5, accuracy=0.454545
2025-09-10 09:05:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:05:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:05:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:05:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1806MB allocated=1759MB
2025-09-10 09:05:42 (federatedscope.llm.llm_local.client:480) INFO: Client 4 Adapter 1 with val results: {'val_total': 11, 'val_loss': 7.9109787940979, 'val_avg_loss': 0.7191798903725364, 'val_seen': 11, 'val_correct': 5, 'val_acc': 0.45454545454545453}
2025-09-10 09:05:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:05:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:05:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:05:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:05:45 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.673655, avg_loss=0.616841, seen=40, correct=25, accuracy=0.625000
2025-09-10 09:05:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:05:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:05:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:05:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1806MB allocated=1759MB
2025-09-10 09:05:46 (federatedscope.llm.llm_local.client:501) INFO: Client 4 Adapter 1 with test results: {'test_total': 40, 'test_loss': 24.673654556274414, 'test_avg_loss': 0.6168413639068604, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-10 09:05:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-10 09:05:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:05:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=4, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:05:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-10 09:05:47 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.073946, avg_loss=0.643086, seen=11, correct=7, accuracy=0.636364
2025-09-10 09:05:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:05:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:05:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:05:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1806MB allocated=1759MB
2025-09-10 09:05:48 (federatedscope.llm.llm_local.client:480) INFO: Client 4 Adapter 2 with val results: {'val_total': 11, 'val_loss': 7.07394552230835, 'val_avg_loss': 0.6430859565734863, 'val_seen': 11, 'val_correct': 7, 'val_acc': 0.6363636363636364}
2025-09-10 09:05:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:05:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:05:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:05:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:05:49 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.096897, avg_loss=0.777422, seen=40, correct=18, accuracy=0.450000
2025-09-10 09:05:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:05:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:05:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:05:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1806MB allocated=1759MB
2025-09-10 09:05:51 (federatedscope.llm.llm_local.client:501) INFO: Client 4 Adapter 2 with test results: {'test_total': 40, 'test_loss': 31.09689712524414, 'test_avg_loss': 0.7774224281311035, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-10 09:05:51 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:05:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-09-10 09:05:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:05:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=7, num_train_batch_last_epoch=2, num_train_epoch=15, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:05:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-09-10 09:05:52 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=14, loss_sum=9.699093, avg_loss=0.692792, seen=14, correct=8, accuracy=0.571429
2025-09-10 09:05:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:05:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:05:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:05:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1806MB allocated=1793MB
2025-09-10 09:05:54 (federatedscope.llm.llm_local.client:480) INFO: Client 5 Adapter 0 with val results: {'val_total': 14, 'val_loss': 9.699092864990234, 'val_avg_loss': 0.6927923474993024, 'val_seen': 14, 'val_correct': 8, 'val_acc': 0.5714285714285714}
2025-09-10 09:05:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:05:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:05:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:05:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:05:55 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.447971, avg_loss=0.686199, seen=40, correct=22, accuracy=0.550000
2025-09-10 09:05:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:05:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:05:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:05:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1806MB allocated=1793MB
2025-09-10 09:05:56 (federatedscope.llm.llm_local.client:501) INFO: Client 5 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.44797134399414, 'test_avg_loss': 0.6861992835998535, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-10 09:05:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-09-10 09:05:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:05:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=7, num_train_batch_last_epoch=2, num_train_epoch=15, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:05:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-09-10 09:05:58 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=14, loss_sum=10.124768, avg_loss=0.723198, seen=14, correct=10, accuracy=0.714286
2025-09-10 09:05:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:05:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:05:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:05:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1806MB allocated=1768MB
2025-09-10 09:05:59 (federatedscope.llm.llm_local.client:480) INFO: Client 5 Adapter 1 with val results: {'val_total': 14, 'val_loss': 10.124768257141113, 'val_avg_loss': 0.7231977326529366, 'val_seen': 14, 'val_correct': 10, 'val_acc': 0.7142857142857143}
2025-09-10 09:05:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:05:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:05:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:06:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:06:01 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.212856, avg_loss=0.680321, seen=40, correct=23, accuracy=0.575000
2025-09-10 09:06:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:06:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:06:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:06:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1806MB allocated=1768MB
2025-09-10 09:06:03 (federatedscope.llm.llm_local.client:501) INFO: Client 5 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.21285629272461, 'test_avg_loss': 0.6803214073181152, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-10 09:06:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-09-10 09:06:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:06:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=7, num_train_batch_last_epoch=2, num_train_epoch=15, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:06:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-09-10 09:06:05 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=14, loss_sum=9.031166, avg_loss=0.645083, seen=14, correct=9, accuracy=0.642857
2025-09-10 09:06:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:06:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:06:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:06:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1806MB allocated=1768MB
2025-09-10 09:06:07 (federatedscope.llm.llm_local.client:480) INFO: Client 5 Adapter 2 with val results: {'val_total': 14, 'val_loss': 9.031166076660156, 'val_avg_loss': 0.6450832911900112, 'val_seen': 14, 'val_correct': 9, 'val_acc': 0.6428571428571429}
2025-09-10 09:06:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:06:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:06:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:06:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:06:08 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.361305, avg_loss=0.609033, seen=40, correct=27, accuracy=0.675000
2025-09-10 09:06:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:06:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:06:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:06:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1806MB allocated=1768MB
2025-09-10 09:06:09 (federatedscope.llm.llm_local.client:501) INFO: Client 5 Adapter 2 with test results: {'test_total': 40, 'test_loss': 24.361305236816406, 'test_avg_loss': 0.6090326309204102, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-10 09:06:09 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:06:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-09-10 09:06:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:06:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=33, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:06:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 09:06:12 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=134, loss_sum=91.635117, avg_loss=0.683844, seen=134, correct=82, accuracy=0.611940
2025-09-10 09:06:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:06:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:06:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:06:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1806MB allocated=1801MB
2025-09-10 09:06:13 (federatedscope.llm.llm_local.client:480) INFO: Client 6 Adapter 0 with val results: {'val_total': 134, 'val_loss': 91.63511657714844, 'val_avg_loss': 0.6838441535608092, 'val_seen': 134, 'val_correct': 82, 'val_acc': 0.6119402985074627}
2025-09-10 09:06:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:06:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:06:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:06:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:06:15 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=32.420906, avg_loss=0.810523, seen=40, correct=20, accuracy=0.500000
2025-09-10 09:06:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:06:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:06:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:06:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1806MB allocated=1801MB
2025-09-10 09:06:16 (federatedscope.llm.llm_local.client:501) INFO: Client 6 Adapter 0 with test results: {'test_total': 40, 'test_loss': 32.42090606689453, 'test_avg_loss': 0.8105226516723633, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-10 09:06:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-09-10 09:06:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:06:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=33, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:06:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 09:06:19 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=134, loss_sum=86.489471, avg_loss=0.645444, seen=134, correct=85, accuracy=0.634328
2025-09-10 09:06:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:06:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:06:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:06:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1806MB allocated=1776MB
2025-09-10 09:06:21 (federatedscope.llm.llm_local.client:480) INFO: Client 6 Adapter 1 with val results: {'val_total': 134, 'val_loss': 86.48947143554688, 'val_avg_loss': 0.6454438166831856, 'val_seen': 134, 'val_correct': 85, 'val_acc': 0.6343283582089553}
2025-09-10 09:06:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:06:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:06:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:06:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:06:23 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.746651, avg_loss=0.768666, seen=40, correct=18, accuracy=0.450000
2025-09-10 09:06:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:06:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:06:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:06:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1806MB allocated=1776MB
2025-09-10 09:06:25 (federatedscope.llm.llm_local.client:501) INFO: Client 6 Adapter 1 with test results: {'test_total': 40, 'test_loss': 30.74665069580078, 'test_avg_loss': 0.7686662673950195, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-10 09:06:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-09-10 09:06:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:06:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=33, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:06:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 09:06:29 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=134, loss_sum=89.727005, avg_loss=0.669605, seen=134, correct=78, accuracy=0.582090
2025-09-10 09:06:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:06:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:06:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:06:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1806MB allocated=1776MB
2025-09-10 09:06:30 (federatedscope.llm.llm_local.client:480) INFO: Client 6 Adapter 2 with val results: {'val_total': 134, 'val_loss': 89.72700500488281, 'val_avg_loss': 0.6696045149618121, 'val_seen': 134, 'val_correct': 78, 'val_acc': 0.582089552238806}
2025-09-10 09:06:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:06:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:06:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:06:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:06:32 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.047482, avg_loss=0.776187, seen=40, correct=21, accuracy=0.525000
2025-09-10 09:06:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:06:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:06:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:06:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1806MB allocated=1776MB
2025-09-10 09:06:34 (federatedscope.llm.llm_local.client:501) INFO: Client 6 Adapter 2 with test results: {'test_total': 40, 'test_loss': 31.047481536865234, 'test_avg_loss': 0.7761870384216308, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-10 09:06:34 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:06:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-09-10 09:06:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:06:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=29, num_train_batch_last_epoch=13, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:06:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-09-10 09:06:37 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=57, loss_sum=37.267483, avg_loss=0.653815, seen=57, correct=34, accuracy=0.596491
2025-09-10 09:06:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:06:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:06:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:06:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1826MB allocated=1810MB
2025-09-10 09:06:39 (federatedscope.llm.llm_local.client:480) INFO: Client 7 Adapter 0 with val results: {'val_total': 57, 'val_loss': 37.26748275756836, 'val_avg_loss': 0.6538154869748835, 'val_seen': 57, 'val_correct': 34, 'val_acc': 0.5964912280701754}
2025-09-10 09:06:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:06:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:06:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:06:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:06:40 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.277843, avg_loss=0.606946, seen=40, correct=27, accuracy=0.675000
2025-09-10 09:06:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:06:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:06:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:06:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1826MB allocated=1810MB
2025-09-10 09:06:41 (federatedscope.llm.llm_local.client:501) INFO: Client 7 Adapter 0 with test results: {'test_total': 40, 'test_loss': 24.277843475341797, 'test_avg_loss': 0.6069460868835449, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-10 09:06:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-09-10 09:06:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:06:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=29, num_train_batch_last_epoch=13, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:06:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-09-10 09:06:44 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=57, loss_sum=37.971626, avg_loss=0.666169, seen=57, correct=37, accuracy=0.649123
2025-09-10 09:06:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:06:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:06:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:06:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1826MB allocated=1785MB
2025-09-10 09:06:45 (federatedscope.llm.llm_local.client:480) INFO: Client 7 Adapter 1 with val results: {'val_total': 57, 'val_loss': 37.97162628173828, 'val_avg_loss': 0.6661688821357593, 'val_seen': 57, 'val_correct': 37, 'val_acc': 0.6491228070175439}
2025-09-10 09:06:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:06:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:06:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:06:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:06:47 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.090021, avg_loss=0.602251, seen=40, correct=29, accuracy=0.725000
2025-09-10 09:06:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:06:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:06:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:06:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1826MB allocated=1785MB
2025-09-10 09:06:49 (federatedscope.llm.llm_local.client:501) INFO: Client 7 Adapter 1 with test results: {'test_total': 40, 'test_loss': 24.09002113342285, 'test_avg_loss': 0.6022505283355712, 'test_seen': 40, 'test_correct': 29, 'test_acc': 0.725}
2025-09-10 09:06:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-09-10 09:06:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:06:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=29, num_train_batch_last_epoch=13, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:06:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-09-10 09:06:51 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=57, loss_sum=37.833748, avg_loss=0.663750, seen=57, correct=30, accuracy=0.526316
2025-09-10 09:06:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:06:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:06:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:06:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1826MB allocated=1785MB
2025-09-10 09:06:53 (federatedscope.llm.llm_local.client:480) INFO: Client 7 Adapter 2 with val results: {'val_total': 57, 'val_loss': 37.83374786376953, 'val_avg_loss': 0.6637499625222725, 'val_seen': 57, 'val_correct': 30, 'val_acc': 0.5263157894736842}
2025-09-10 09:06:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:06:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:06:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:06:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:06:55 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.869793, avg_loss=0.621745, seen=40, correct=27, accuracy=0.675000
2025-09-10 09:06:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:06:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:06:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:06:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1826MB allocated=1785MB
2025-09-10 09:06:56 (federatedscope.llm.llm_local.client:501) INFO: Client 7 Adapter 2 with test results: {'test_total': 40, 'test_loss': 24.869792938232422, 'test_avg_loss': 0.6217448234558105, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-10 09:06:56 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:06:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-09-10 09:06:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:06:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=35, num_train_batch_last_epoch=30, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:06:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-10 09:06:59 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=69, loss_sum=45.597900, avg_loss=0.660839, seen=69, correct=42, accuracy=0.608696
2025-09-10 09:06:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:06:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:06:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:07:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1826MB allocated=1818MB
2025-09-10 09:07:00 (federatedscope.llm.llm_local.client:480) INFO: Client 8 Adapter 0 with val results: {'val_total': 69, 'val_loss': 45.597900390625, 'val_avg_loss': 0.6608391360960145, 'val_seen': 69, 'val_correct': 42, 'val_acc': 0.6086956521739131}
2025-09-10 09:07:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:07:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:07:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:07:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:07:02 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.644970, avg_loss=0.766124, seen=40, correct=19, accuracy=0.475000
2025-09-10 09:07:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:07:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:07:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:07:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1826MB allocated=1818MB
2025-09-10 09:07:04 (federatedscope.llm.llm_local.client:501) INFO: Client 8 Adapter 0 with test results: {'test_total': 40, 'test_loss': 30.644969940185547, 'test_avg_loss': 0.7661242485046387, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-10 09:07:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-09-10 09:07:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:07:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=35, num_train_batch_last_epoch=30, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:07:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-10 09:07:07 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=69, loss_sum=46.861423, avg_loss=0.679151, seen=69, correct=38, accuracy=0.550725
2025-09-10 09:07:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:07:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:07:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:07:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1826MB allocated=1793MB
2025-09-10 09:07:09 (federatedscope.llm.llm_local.client:480) INFO: Client 8 Adapter 1 with val results: {'val_total': 69, 'val_loss': 46.86142349243164, 'val_avg_loss': 0.679151065107705, 'val_seen': 69, 'val_correct': 38, 'val_acc': 0.5507246376811594}
2025-09-10 09:07:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:07:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:07:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:07:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:07:11 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.296043, avg_loss=0.757401, seen=40, correct=22, accuracy=0.550000
2025-09-10 09:07:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:07:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:07:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:07:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1826MB allocated=1793MB
2025-09-10 09:07:12 (federatedscope.llm.llm_local.client:501) INFO: Client 8 Adapter 1 with test results: {'test_total': 40, 'test_loss': 30.296043395996094, 'test_avg_loss': 0.7574010848999023, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-10 09:07:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-09-10 09:07:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:07:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=35, num_train_batch_last_epoch=30, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:07:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-10 09:07:14 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=69, loss_sum=47.014908, avg_loss=0.681375, seen=69, correct=39, accuracy=0.565217
2025-09-10 09:07:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:07:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:07:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:07:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1826MB allocated=1793MB
2025-09-10 09:07:16 (federatedscope.llm.llm_local.client:480) INFO: Client 8 Adapter 2 with val results: {'val_total': 69, 'val_loss': 47.01490783691406, 'val_avg_loss': 0.6813754758973053, 'val_seen': 69, 'val_correct': 39, 'val_acc': 0.5652173913043478}
2025-09-10 09:07:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:07:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:07:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:07:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:07:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.044559, avg_loss=0.726114, seen=40, correct=23, accuracy=0.575000
2025-09-10 09:07:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:07:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:07:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:07:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1826MB allocated=1793MB
2025-09-10 09:07:19 (federatedscope.llm.llm_local.client:501) INFO: Client 8 Adapter 2 with test results: {'test_total': 40, 'test_loss': 29.044559478759766, 'test_avg_loss': 0.7261139869689941, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-10 09:07:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:07:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-10 09:07:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:07:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=6, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:07:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-10 09:07:24 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=126.516037, avg_loss=0.672958, seen=188, correct=110, accuracy=0.585106
2025-09-10 09:07:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:07:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:07:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:07:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1846MB allocated=1827MB
2025-09-10 09:07:27 (federatedscope.llm.llm_local.client:480) INFO: Client 9 Adapter 0 with val results: {'val_total': 188, 'val_loss': 126.51603698730469, 'val_avg_loss': 0.6729576435494931, 'val_seen': 188, 'val_correct': 110, 'val_acc': 0.5851063829787234}
2025-09-10 09:07:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:07:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:07:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:07:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:07:29 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.840139, avg_loss=0.671003, seen=40, correct=25, accuracy=0.625000
2025-09-10 09:07:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:07:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:07:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:07:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1846MB allocated=1827MB
2025-09-10 09:07:30 (federatedscope.llm.llm_local.client:501) INFO: Client 9 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.840139389038086, 'test_avg_loss': 0.6710034847259522, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-10 09:07:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-10 09:07:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:07:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=6, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:07:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-10 09:07:34 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=126.944931, avg_loss=0.675239, seen=188, correct=111, accuracy=0.590426
2025-09-10 09:07:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:07:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:07:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:07:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1846MB allocated=1801MB
2025-09-10 09:07:36 (federatedscope.llm.llm_local.client:480) INFO: Client 9 Adapter 1 with val results: {'val_total': 188, 'val_loss': 126.94493103027344, 'val_avg_loss': 0.67523899484188, 'val_seen': 188, 'val_correct': 111, 'val_acc': 0.5904255319148937}
2025-09-10 09:07:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:07:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:07:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:07:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:07:37 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.372282, avg_loss=0.634307, seen=40, correct=26, accuracy=0.650000
2025-09-10 09:07:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:07:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:07:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:07:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1846MB allocated=1801MB
2025-09-10 09:07:39 (federatedscope.llm.llm_local.client:501) INFO: Client 9 Adapter 1 with test results: {'test_total': 40, 'test_loss': 25.372282028198242, 'test_avg_loss': 0.6343070507049561, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-10 09:07:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-10 09:07:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:07:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=6, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:07:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-10 09:07:43 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=128.274918, avg_loss=0.682313, seen=188, correct=106, accuracy=0.563830
2025-09-10 09:07:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:07:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:07:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:07:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1846MB allocated=1801MB
2025-09-10 09:07:44 (federatedscope.llm.llm_local.client:480) INFO: Client 9 Adapter 2 with val results: {'val_total': 188, 'val_loss': 128.27491760253906, 'val_avg_loss': 0.6823133915028674, 'val_seen': 188, 'val_correct': 106, 'val_acc': 0.5638297872340425}
2025-09-10 09:07:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:07:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:07:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:07:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:07:47 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.726894, avg_loss=0.693172, seen=40, correct=24, accuracy=0.600000
2025-09-10 09:07:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:07:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:07:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:07:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1846MB allocated=1801MB
2025-09-10 09:07:48 (federatedscope.llm.llm_local.client:501) INFO: Client 9 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.72689437866211, 'test_avg_loss': 0.6931723594665528, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-10 09:07:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:07:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-09-10 09:07:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:07:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=32, num_train_batch_last_epoch=4, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:07:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-09-10 09:07:51 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=63, loss_sum=42.064457, avg_loss=0.667690, seen=63, correct=38, accuracy=0.603175
2025-09-10 09:07:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:07:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:07:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:07:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1846MB allocated=1835MB
2025-09-10 09:07:53 (federatedscope.llm.llm_local.client:480) INFO: Client 10 Adapter 0 with val results: {'val_total': 63, 'val_loss': 42.064456939697266, 'val_avg_loss': 0.6676897926936074, 'val_seen': 63, 'val_correct': 38, 'val_acc': 0.6031746031746031}
2025-09-10 09:07:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:07:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:07:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:07:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:07:54 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=22.612431, avg_loss=0.565311, seen=40, correct=30, accuracy=0.750000
2025-09-10 09:07:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:07:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:07:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:07:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1846MB allocated=1835MB
2025-09-10 09:07:55 (federatedscope.llm.llm_local.client:501) INFO: Client 10 Adapter 0 with test results: {'test_total': 40, 'test_loss': 22.612430572509766, 'test_avg_loss': 0.5653107643127442, 'test_seen': 40, 'test_correct': 30, 'test_acc': 0.75}
2025-09-10 09:07:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-09-10 09:07:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:07:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=32, num_train_batch_last_epoch=4, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:07:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-09-10 09:07:57 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=63, loss_sum=41.009930, avg_loss=0.650951, seen=63, correct=42, accuracy=0.666667
2025-09-10 09:07:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:07:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:07:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:07:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1846MB allocated=1810MB
2025-09-10 09:07:59 (federatedscope.llm.llm_local.client:480) INFO: Client 10 Adapter 1 with val results: {'val_total': 63, 'val_loss': 41.00992965698242, 'val_avg_loss': 0.6509512643965464, 'val_seen': 63, 'val_correct': 42, 'val_acc': 0.6666666666666666}
2025-09-10 09:07:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:07:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:07:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:08:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:08:00 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.689796, avg_loss=0.617245, seen=40, correct=28, accuracy=0.700000
2025-09-10 09:08:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:08:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:08:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:08:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1846MB allocated=1810MB
2025-09-10 09:08:02 (federatedscope.llm.llm_local.client:501) INFO: Client 10 Adapter 1 with test results: {'test_total': 40, 'test_loss': 24.689796447753906, 'test_avg_loss': 0.6172449111938476, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}
2025-09-10 09:08:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-09-10 09:08:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:08:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=32, num_train_batch_last_epoch=4, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:08:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-09-10 09:08:06 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=63, loss_sum=44.724602, avg_loss=0.709914, seen=63, correct=37, accuracy=0.587302
2025-09-10 09:08:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:08:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:08:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:08:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1846MB allocated=1810MB
2025-09-10 09:08:07 (federatedscope.llm.llm_local.client:480) INFO: Client 10 Adapter 2 with val results: {'val_total': 63, 'val_loss': 44.72460174560547, 'val_avg_loss': 0.709914313422309, 'val_seen': 63, 'val_correct': 37, 'val_acc': 0.5873015873015873}
2025-09-10 09:08:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:08:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:08:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:08:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:08:08 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.410593, avg_loss=0.635265, seen=40, correct=22, accuracy=0.550000
2025-09-10 09:08:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:08:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:08:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:08:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1846MB allocated=1810MB
2025-09-10 09:08:10 (federatedscope.llm.llm_local.client:501) INFO: Client 10 Adapter 2 with test results: {'test_total': 40, 'test_loss': 25.410593032836914, 'test_avg_loss': 0.6352648258209228, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-10 09:08:10 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:08:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-09-10 09:08:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:08:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=16, num_train_batch_last_epoch=4, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:08:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-10 09:08:11 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=32, loss_sum=20.067278, avg_loss=0.627102, seen=32, correct=19, accuracy=0.593750
2025-09-10 09:08:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:08:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:08:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:08:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1846MB allocated=1843MB
2025-09-10 09:08:13 (federatedscope.llm.llm_local.client:480) INFO: Client 11 Adapter 0 with val results: {'val_total': 32, 'val_loss': 20.067277908325195, 'val_avg_loss': 0.6271024346351624, 'val_seen': 32, 'val_correct': 19, 'val_acc': 0.59375}
2025-09-10 09:08:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:08:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:08:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:08:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:08:15 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.605240, avg_loss=0.640131, seen=40, correct=23, accuracy=0.575000
2025-09-10 09:08:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:08:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:08:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:08:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1846MB allocated=1843MB
2025-09-10 09:08:16 (federatedscope.llm.llm_local.client:501) INFO: Client 11 Adapter 0 with test results: {'test_total': 40, 'test_loss': 25.605239868164062, 'test_avg_loss': 0.6401309967041016, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-10 09:08:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-09-10 09:08:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:08:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=16, num_train_batch_last_epoch=4, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:08:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-10 09:08:17 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=32, loss_sum=20.914560, avg_loss=0.653580, seen=32, correct=19, accuracy=0.593750
2025-09-10 09:08:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:08:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:08:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:08:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1846MB allocated=1818MB
2025-09-10 09:08:19 (federatedscope.llm.llm_local.client:480) INFO: Client 11 Adapter 1 with val results: {'val_total': 32, 'val_loss': 20.914560317993164, 'val_avg_loss': 0.6535800099372864, 'val_seen': 32, 'val_correct': 19, 'val_acc': 0.59375}
2025-09-10 09:08:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:08:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:08:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:08:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:08:19 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.055752, avg_loss=0.651394, seen=40, correct=23, accuracy=0.575000
2025-09-10 09:08:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:08:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:08:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:08:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1846MB allocated=1818MB
2025-09-10 09:08:21 (federatedscope.llm.llm_local.client:501) INFO: Client 11 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.05575180053711, 'test_avg_loss': 0.6513937950134278, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-10 09:08:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-09-10 09:08:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:08:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=16, num_train_batch_last_epoch=4, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:08:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-10 09:08:22 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=32, loss_sum=19.565603, avg_loss=0.611425, seen=32, correct=20, accuracy=0.625000
2025-09-10 09:08:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:08:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:08:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:08:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1846MB allocated=1818MB
2025-09-10 09:08:24 (federatedscope.llm.llm_local.client:480) INFO: Client 11 Adapter 2 with val results: {'val_total': 32, 'val_loss': 19.565603256225586, 'val_avg_loss': 0.6114251017570496, 'val_seen': 32, 'val_correct': 20, 'val_acc': 0.625}
2025-09-10 09:08:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:08:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:08:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:08:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:08:25 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.473003, avg_loss=0.736825, seen=40, correct=19, accuracy=0.475000
2025-09-10 09:08:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:08:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:08:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:08:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1846MB allocated=1818MB
2025-09-10 09:08:26 (federatedscope.llm.llm_local.client:501) INFO: Client 11 Adapter 2 with test results: {'test_total': 40, 'test_loss': 29.473003387451172, 'test_avg_loss': 0.7368250846862793, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-10 09:08:26 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:08:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-09-10 09:08:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:08:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=69, num_train_batch_last_epoch=31, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:08:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-09-10 09:08:30 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=137, loss_sum=91.303375, avg_loss=0.666448, seen=137, correct=81, accuracy=0.591241
2025-09-10 09:08:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:08:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:08:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:08:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1866MB allocated=1852MB
2025-09-10 09:08:32 (federatedscope.llm.llm_local.client:480) INFO: Client 12 Adapter 0 with val results: {'val_total': 137, 'val_loss': 91.30337524414062, 'val_avg_loss': 0.6664479944827782, 'val_seen': 137, 'val_correct': 81, 'val_acc': 0.5912408759124088}
2025-09-10 09:08:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:08:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:08:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:08:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:08:33 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.508492, avg_loss=0.637712, seen=40, correct=27, accuracy=0.675000
2025-09-10 09:08:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:08:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:08:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:08:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1866MB allocated=1852MB
2025-09-10 09:08:35 (federatedscope.llm.llm_local.client:501) INFO: Client 12 Adapter 0 with test results: {'test_total': 40, 'test_loss': 25.50849151611328, 'test_avg_loss': 0.6377122879028321, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-10 09:08:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-09-10 09:08:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:08:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=69, num_train_batch_last_epoch=31, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:08:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-09-10 09:08:40 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=137, loss_sum=92.850014, avg_loss=0.677737, seen=137, correct=80, accuracy=0.583942
2025-09-10 09:08:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:08:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:08:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:08:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1866MB allocated=1827MB
2025-09-10 09:08:41 (federatedscope.llm.llm_local.client:480) INFO: Client 12 Adapter 1 with val results: {'val_total': 137, 'val_loss': 92.85001373291016, 'val_avg_loss': 0.6777373265175923, 'val_seen': 137, 'val_correct': 80, 'val_acc': 0.583941605839416}
2025-09-10 09:08:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:08:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:08:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:08:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:08:42 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.078238, avg_loss=0.626956, seen=40, correct=27, accuracy=0.675000
2025-09-10 09:08:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:08:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:08:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:08:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1866MB allocated=1827MB
2025-09-10 09:08:43 (federatedscope.llm.llm_local.client:501) INFO: Client 12 Adapter 1 with test results: {'test_total': 40, 'test_loss': 25.078237533569336, 'test_avg_loss': 0.6269559383392334, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-10 09:08:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-09-10 09:08:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:08:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=69, num_train_batch_last_epoch=31, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:08:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-09-10 09:08:46 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=137, loss_sum=91.319519, avg_loss=0.666566, seen=137, correct=83, accuracy=0.605839
2025-09-10 09:08:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:08:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:08:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:08:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1866MB allocated=1827MB
2025-09-10 09:08:48 (federatedscope.llm.llm_local.client:480) INFO: Client 12 Adapter 2 with val results: {'val_total': 137, 'val_loss': 91.31951904296875, 'val_avg_loss': 0.6665658324304289, 'val_seen': 137, 'val_correct': 83, 'val_acc': 0.6058394160583942}
2025-09-10 09:08:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:08:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:08:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:08:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:08:49 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.154463, avg_loss=0.653862, seen=40, correct=24, accuracy=0.600000
2025-09-10 09:08:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:08:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:08:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:08:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1866MB allocated=1827MB
2025-09-10 09:08:50 (federatedscope.llm.llm_local.client:501) INFO: Client 12 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.154462814331055, 'test_avg_loss': 0.6538615703582764, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-10 09:08:50 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:08:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-09-10 09:08:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:08:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=36, num_train_batch_last_epoch=28, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:08:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-10 09:08:53 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=72, loss_sum=48.540184, avg_loss=0.674169, seen=72, correct=42, accuracy=0.583333
2025-09-10 09:08:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:08:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:08:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:08:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1866MB allocated=1860MB
2025-09-10 09:08:55 (federatedscope.llm.llm_local.client:480) INFO: Client 13 Adapter 0 with val results: {'val_total': 72, 'val_loss': 48.540184020996094, 'val_avg_loss': 0.6741692225138346, 'val_seen': 72, 'val_correct': 42, 'val_acc': 0.5833333333333334}
2025-09-10 09:08:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:08:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:08:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:08:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:08:57 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.185530, avg_loss=0.704638, seen=40, correct=23, accuracy=0.575000
2025-09-10 09:08:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:08:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:08:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:08:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1866MB allocated=1860MB
2025-09-10 09:08:58 (federatedscope.llm.llm_local.client:501) INFO: Client 13 Adapter 0 with test results: {'test_total': 40, 'test_loss': 28.185529708862305, 'test_avg_loss': 0.7046382427215576, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-10 09:08:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-09-10 09:08:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:08:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=36, num_train_batch_last_epoch=28, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:09:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-10 09:09:00 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=72, loss_sum=47.917168, avg_loss=0.665516, seen=72, correct=43, accuracy=0.597222
2025-09-10 09:09:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:09:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:09:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:09:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1866MB allocated=1835MB
2025-09-10 09:09:01 (federatedscope.llm.llm_local.client:480) INFO: Client 13 Adapter 1 with val results: {'val_total': 72, 'val_loss': 47.91716766357422, 'val_avg_loss': 0.665516217549642, 'val_seen': 72, 'val_correct': 43, 'val_acc': 0.5972222222222222}
2025-09-10 09:09:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:09:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:09:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:09:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:09:03 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.426258, avg_loss=0.610656, seen=40, correct=24, accuracy=0.600000
2025-09-10 09:09:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:09:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:09:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:09:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1866MB allocated=1835MB
2025-09-10 09:09:04 (federatedscope.llm.llm_local.client:501) INFO: Client 13 Adapter 1 with test results: {'test_total': 40, 'test_loss': 24.426258087158203, 'test_avg_loss': 0.610656452178955, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-10 09:09:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-09-10 09:09:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:09:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=36, num_train_batch_last_epoch=28, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:09:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-10 09:09:06 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=72, loss_sum=49.509052, avg_loss=0.687626, seen=72, correct=41, accuracy=0.569444
2025-09-10 09:09:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:09:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:09:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:09:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1866MB allocated=1835MB
2025-09-10 09:09:08 (federatedscope.llm.llm_local.client:480) INFO: Client 13 Adapter 2 with val results: {'val_total': 72, 'val_loss': 49.50905227661133, 'val_avg_loss': 0.6876257260640463, 'val_seen': 72, 'val_correct': 41, 'val_acc': 0.5694444444444444}
2025-09-10 09:09:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:09:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:09:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:09:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:09:10 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.284298, avg_loss=0.732107, seen=40, correct=22, accuracy=0.550000
2025-09-10 09:09:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:09:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:09:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:09:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1866MB allocated=1835MB
2025-09-10 09:09:11 (federatedscope.llm.llm_local.client:501) INFO: Client 13 Adapter 2 with test results: {'test_total': 40, 'test_loss': 29.284297943115234, 'test_avg_loss': 0.7321074485778809, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-10 09:09:11 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:09:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-09-10 09:09:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:09:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=80, num_train_batch_last_epoch=20, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:09:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-09-10 09:09:15 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=160, loss_sum=105.378448, avg_loss=0.658615, seen=160, correct=98, accuracy=0.612500
2025-09-10 09:09:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:09:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:09:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:09:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1886MB allocated=1869MB
2025-09-10 09:09:16 (federatedscope.llm.llm_local.client:480) INFO: Client 14 Adapter 0 with val results: {'val_total': 160, 'val_loss': 105.37844848632812, 'val_avg_loss': 0.6586153030395507, 'val_seen': 160, 'val_correct': 98, 'val_acc': 0.6125}
2025-09-10 09:09:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:09:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:09:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:09:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:09:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.685223, avg_loss=0.592131, seen=40, correct=29, accuracy=0.725000
2025-09-10 09:09:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:09:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:09:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:09:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1886MB allocated=1869MB
2025-09-10 09:09:19 (federatedscope.llm.llm_local.client:501) INFO: Client 14 Adapter 0 with test results: {'test_total': 40, 'test_loss': 23.685222625732422, 'test_avg_loss': 0.5921305656433106, 'test_seen': 40, 'test_correct': 29, 'test_acc': 0.725}
2025-09-10 09:09:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-09-10 09:09:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:09:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=80, num_train_batch_last_epoch=20, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:09:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-09-10 09:09:22 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=160, loss_sum=108.741127, avg_loss=0.679632, seen=160, correct=93, accuracy=0.581250
2025-09-10 09:09:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:09:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:09:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:09:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1886MB allocated=1843MB
2025-09-10 09:09:24 (federatedscope.llm.llm_local.client:480) INFO: Client 14 Adapter 1 with val results: {'val_total': 160, 'val_loss': 108.74112701416016, 'val_avg_loss': 0.679632043838501, 'val_seen': 160, 'val_correct': 93, 'val_acc': 0.58125}
2025-09-10 09:09:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:09:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:09:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:09:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:09:26 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.378902, avg_loss=0.609473, seen=40, correct=27, accuracy=0.675000
2025-09-10 09:09:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:09:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:09:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:09:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1886MB allocated=1843MB
2025-09-10 09:09:27 (federatedscope.llm.llm_local.client:501) INFO: Client 14 Adapter 1 with test results: {'test_total': 40, 'test_loss': 24.378902435302734, 'test_avg_loss': 0.6094725608825684, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-10 09:09:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-09-10 09:09:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:09:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=80, num_train_batch_last_epoch=20, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:09:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-09-10 09:09:31 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=160, loss_sum=106.269020, avg_loss=0.664181, seen=160, correct=91, accuracy=0.568750
2025-09-10 09:09:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:09:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:09:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:09:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1886MB allocated=1843MB
2025-09-10 09:09:34 (federatedscope.llm.llm_local.client:480) INFO: Client 14 Adapter 2 with val results: {'val_total': 160, 'val_loss': 106.2690200805664, 'val_avg_loss': 0.6641813755035401, 'val_seen': 160, 'val_correct': 91, 'val_acc': 0.56875}
2025-09-10 09:09:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:09:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:09:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:09:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:09:36 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.413658, avg_loss=0.660341, seen=40, correct=24, accuracy=0.600000
2025-09-10 09:09:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:09:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:09:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:09:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1886MB allocated=1843MB
2025-09-10 09:09:37 (federatedscope.llm.llm_local.client:501) INFO: Client 14 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.413658142089844, 'test_avg_loss': 0.660341453552246, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-10 09:09:38 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:09:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:09:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:09:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:09:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:09:41 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=130.545517, avg_loss=0.652728, seen=200, correct=121, accuracy=0.605000
2025-09-10 09:09:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:09:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:09:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:09:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1886MB allocated=1877MB
2025-09-10 09:09:43 (federatedscope.llm.llm_local.client:480) INFO: Client 15 Adapter 0 with val results: {'val_total': 200, 'val_loss': 130.54551696777344, 'val_avg_loss': 0.6527275848388672, 'val_seen': 200, 'val_correct': 121, 'val_acc': 0.605}
2025-09-10 09:09:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:09:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:09:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:09:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:09:44 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.811968, avg_loss=0.670299, seen=40, correct=24, accuracy=0.600000
2025-09-10 09:09:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:09:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:09:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:09:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1886MB allocated=1877MB
2025-09-10 09:09:45 (federatedscope.llm.llm_local.client:501) INFO: Client 15 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.811967849731445, 'test_avg_loss': 0.6702991962432862, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-10 09:09:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:09:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:09:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:09:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:09:49 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=127.835892, avg_loss=0.639179, seen=200, correct=126, accuracy=0.630000
2025-09-10 09:09:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:09:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:09:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:09:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1886MB allocated=1852MB
2025-09-10 09:09:51 (federatedscope.llm.llm_local.client:480) INFO: Client 15 Adapter 1 with val results: {'val_total': 200, 'val_loss': 127.83589172363281, 'val_avg_loss': 0.639179458618164, 'val_seen': 200, 'val_correct': 126, 'val_acc': 0.63}
2025-09-10 09:09:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:09:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:09:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:09:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:09:52 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.584122, avg_loss=0.689603, seen=40, correct=26, accuracy=0.650000
2025-09-10 09:09:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:09:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:09:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:09:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1886MB allocated=1852MB
2025-09-10 09:09:54 (federatedscope.llm.llm_local.client:501) INFO: Client 15 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.584121704101562, 'test_avg_loss': 0.689603042602539, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-10 09:09:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:09:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:09:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:09:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:09:58 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=143.283295, avg_loss=0.716416, seen=200, correct=109, accuracy=0.545000
2025-09-10 09:09:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:09:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:09:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:09:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1886MB allocated=1852MB
2025-09-10 09:09:59 (federatedscope.llm.llm_local.client:480) INFO: Client 15 Adapter 2 with val results: {'val_total': 200, 'val_loss': 143.28329467773438, 'val_avg_loss': 0.7164164733886719, 'val_seen': 200, 'val_correct': 109, 'val_acc': 0.545}
2025-09-10 09:09:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:09:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:09:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:10:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:10:00 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.660612, avg_loss=0.666515, seen=40, correct=22, accuracy=0.550000
2025-09-10 09:10:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:10:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:10:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:10:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1886MB allocated=1852MB
2025-09-10 09:10:02 (federatedscope.llm.llm_local.client:501) INFO: Client 15 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.660612106323242, 'test_avg_loss': 0.666515302658081, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-10 09:10:02 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:10:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-09-10 09:10:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:10:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=32, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:10:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 09:10:05 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=136, loss_sum=88.370728, avg_loss=0.649785, seen=136, correct=83, accuracy=0.610294
2025-09-10 09:10:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:10:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:10:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:10:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1906MB allocated=1885MB
2025-09-10 09:10:07 (federatedscope.llm.llm_local.client:480) INFO: Client 16 Adapter 0 with val results: {'val_total': 136, 'val_loss': 88.3707275390625, 'val_avg_loss': 0.649784761316636, 'val_seen': 136, 'val_correct': 83, 'val_acc': 0.6102941176470589}
2025-09-10 09:10:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:10:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:10:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:10:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:10:08 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.370096, avg_loss=0.584252, seen=40, correct=27, accuracy=0.675000
2025-09-10 09:10:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:10:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:10:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:10:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1906MB allocated=1885MB
2025-09-10 09:10:10 (federatedscope.llm.llm_local.client:501) INFO: Client 16 Adapter 0 with test results: {'test_total': 40, 'test_loss': 23.37009620666504, 'test_avg_loss': 0.584252405166626, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-10 09:10:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-09-10 09:10:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:10:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=32, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:10:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 09:10:14 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=136, loss_sum=86.587326, avg_loss=0.636672, seen=136, correct=86, accuracy=0.632353
2025-09-10 09:10:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:10:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:10:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:10:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1906MB allocated=1860MB
2025-09-10 09:10:15 (federatedscope.llm.llm_local.client:480) INFO: Client 16 Adapter 1 with val results: {'val_total': 136, 'val_loss': 86.58732604980469, 'val_avg_loss': 0.6366715150720933, 'val_seen': 136, 'val_correct': 86, 'val_acc': 0.6323529411764706}
2025-09-10 09:10:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:10:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:10:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:10:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:10:17 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.224030, avg_loss=0.680601, seen=40, correct=22, accuracy=0.550000
2025-09-10 09:10:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:10:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:10:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:10:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1906MB allocated=1860MB
2025-09-10 09:10:18 (federatedscope.llm.llm_local.client:501) INFO: Client 16 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.224029541015625, 'test_avg_loss': 0.6806007385253906, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-10 09:10:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-09-10 09:10:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:10:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=32, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:10:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 09:10:21 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=136, loss_sum=89.555176, avg_loss=0.658494, seen=136, correct=86, accuracy=0.632353
2025-09-10 09:10:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:10:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:10:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:10:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1906MB allocated=1860MB
2025-09-10 09:10:23 (federatedscope.llm.llm_local.client:480) INFO: Client 16 Adapter 2 with val results: {'val_total': 136, 'val_loss': 89.55517578125, 'val_avg_loss': 0.6584939395680147, 'val_seen': 136, 'val_correct': 86, 'val_acc': 0.6323529411764706}
2025-09-10 09:10:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:10:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:10:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:10:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:10:25 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.266668, avg_loss=0.631667, seen=40, correct=28, accuracy=0.700000
2025-09-10 09:10:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:10:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:10:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:10:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1906MB allocated=1860MB
2025-09-10 09:10:26 (federatedscope.llm.llm_local.client:501) INFO: Client 16 Adapter 2 with test results: {'test_total': 40, 'test_loss': 25.26666831970215, 'test_avg_loss': 0.6316667079925538, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}
2025-09-10 09:10:26 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:10:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:10:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:10:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:10:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:10:30 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=128.078110, avg_loss=0.640391, seen=200, correct=128, accuracy=0.640000
2025-09-10 09:10:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:10:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:10:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:10:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1906MB allocated=1894MB
2025-09-10 09:10:33 (federatedscope.llm.llm_local.client:480) INFO: Client 17 Adapter 0 with val results: {'val_total': 200, 'val_loss': 128.07810974121094, 'val_avg_loss': 0.6403905487060547, 'val_seen': 200, 'val_correct': 128, 'val_acc': 0.64}
2025-09-10 09:10:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:10:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:10:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:10:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:10:35 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.435249, avg_loss=0.685881, seen=40, correct=23, accuracy=0.575000
2025-09-10 09:10:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:10:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:10:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:10:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1906MB allocated=1894MB
2025-09-10 09:10:37 (federatedscope.llm.llm_local.client:501) INFO: Client 17 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.43524932861328, 'test_avg_loss': 0.685881233215332, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-10 09:10:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:10:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:10:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:10:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:10:42 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=128.728806, avg_loss=0.643644, seen=200, correct=126, accuracy=0.630000
2025-09-10 09:10:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:10:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:10:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:10:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1906MB allocated=1869MB
2025-09-10 09:10:44 (federatedscope.llm.llm_local.client:480) INFO: Client 17 Adapter 1 with val results: {'val_total': 200, 'val_loss': 128.7288055419922, 'val_avg_loss': 0.643644027709961, 'val_seen': 200, 'val_correct': 126, 'val_acc': 0.63}
2025-09-10 09:10:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:10:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:10:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:10:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:10:45 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.747339, avg_loss=0.693683, seen=40, correct=23, accuracy=0.575000
2025-09-10 09:10:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:10:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:10:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:10:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1906MB allocated=1869MB
2025-09-10 09:10:46 (federatedscope.llm.llm_local.client:501) INFO: Client 17 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.747339248657227, 'test_avg_loss': 0.6936834812164306, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-10 09:10:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:10:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:10:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:10:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:10:50 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=131.178787, avg_loss=0.655894, seen=200, correct=125, accuracy=0.625000
2025-09-10 09:10:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:10:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:10:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:10:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1906MB allocated=1869MB
2025-09-10 09:10:52 (federatedscope.llm.llm_local.client:480) INFO: Client 17 Adapter 2 with val results: {'val_total': 200, 'val_loss': 131.1787872314453, 'val_avg_loss': 0.6558939361572266, 'val_seen': 200, 'val_correct': 125, 'val_acc': 0.625}
2025-09-10 09:10:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:10:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:10:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:10:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:10:54 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.533463, avg_loss=0.713337, seen=40, correct=21, accuracy=0.525000
2025-09-10 09:10:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:10:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:10:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:10:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1906MB allocated=1869MB
2025-09-10 09:10:56 (federatedscope.llm.llm_local.client:501) INFO: Client 17 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.533462524414062, 'test_avg_loss': 0.7133365631103515, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-10 09:10:57 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:10:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-09-10 09:10:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:10:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=32, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:11:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 09:11:02 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=135, loss_sum=93.260048, avg_loss=0.690815, seen=135, correct=84, accuracy=0.622222
2025-09-10 09:11:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:11:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:11:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:11:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1906MB allocated=1902MB
2025-09-10 09:11:04 (federatedscope.llm.llm_local.client:480) INFO: Client 18 Adapter 0 with val results: {'val_total': 135, 'val_loss': 93.26004791259766, 'val_avg_loss': 0.6908151697229457, 'val_seen': 135, 'val_correct': 84, 'val_acc': 0.6222222222222222}
2025-09-10 09:11:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:11:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:11:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:11:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:11:06 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.197376, avg_loss=0.604934, seen=40, correct=26, accuracy=0.650000
2025-09-10 09:11:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:11:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:11:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:11:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1906MB allocated=1902MB
2025-09-10 09:11:07 (federatedscope.llm.llm_local.client:501) INFO: Client 18 Adapter 0 with test results: {'test_total': 40, 'test_loss': 24.197376251220703, 'test_avg_loss': 0.6049344062805175, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-10 09:11:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-09-10 09:11:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:11:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=32, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:11:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 09:11:11 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=135, loss_sum=95.326164, avg_loss=0.706120, seen=135, correct=77, accuracy=0.570370
2025-09-10 09:11:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:11:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:11:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:11:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1906MB allocated=1877MB
2025-09-10 09:11:13 (federatedscope.llm.llm_local.client:480) INFO: Client 18 Adapter 1 with val results: {'val_total': 135, 'val_loss': 95.32616424560547, 'val_avg_loss': 0.7061197351526332, 'val_seen': 135, 'val_correct': 77, 'val_acc': 0.5703703703703704}
2025-09-10 09:11:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:11:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:11:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:11:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:11:15 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.612160, avg_loss=0.615304, seen=40, correct=26, accuracy=0.650000
2025-09-10 09:11:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:11:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:11:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:11:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1906MB allocated=1877MB
2025-09-10 09:11:16 (federatedscope.llm.llm_local.client:501) INFO: Client 18 Adapter 1 with test results: {'test_total': 40, 'test_loss': 24.612159729003906, 'test_avg_loss': 0.6153039932250977, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-10 09:11:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-09-10 09:11:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:11:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=32, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:11:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 09:11:19 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=135, loss_sum=97.082436, avg_loss=0.719129, seen=135, correct=65, accuracy=0.481481
2025-09-10 09:11:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:11:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:11:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:11:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1906MB allocated=1877MB
2025-09-10 09:11:22 (federatedscope.llm.llm_local.client:480) INFO: Client 18 Adapter 2 with val results: {'val_total': 135, 'val_loss': 97.08243560791016, 'val_avg_loss': 0.7191291526511864, 'val_seen': 135, 'val_correct': 65, 'val_acc': 0.48148148148148145}
2025-09-10 09:11:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:11:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:11:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:11:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:11:23 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.772472, avg_loss=0.719312, seen=40, correct=18, accuracy=0.450000
2025-09-10 09:11:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:11:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:11:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:11:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1906MB allocated=1877MB
2025-09-10 09:11:25 (federatedscope.llm.llm_local.client:501) INFO: Client 18 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.772472381591797, 'test_avg_loss': 0.7193118095397949, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-10 09:11:25 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:11:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-10 09:11:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:11:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=45, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:11:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-10 09:11:28 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=74.888779, avg_loss=0.680807, seen=110, correct=65, accuracy=0.590909
2025-09-10 09:11:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:11:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:11:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:11:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1926MB allocated=1910MB
2025-09-10 09:11:29 (federatedscope.llm.llm_local.client:480) INFO: Client 19 Adapter 0 with val results: {'val_total': 110, 'val_loss': 74.88877868652344, 'val_avg_loss': 0.6808070789683949, 'val_seen': 110, 'val_correct': 65, 'val_acc': 0.5909090909090909}
2025-09-10 09:11:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:11:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:11:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:11:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:11:30 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.449982, avg_loss=0.786250, seen=40, correct=20, accuracy=0.500000
2025-09-10 09:11:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:11:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:11:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:11:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1926MB allocated=1910MB
2025-09-10 09:11:31 (federatedscope.llm.llm_local.client:501) INFO: Client 19 Adapter 0 with test results: {'test_total': 40, 'test_loss': 31.449981689453125, 'test_avg_loss': 0.7862495422363281, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-10 09:11:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-10 09:11:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:11:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=45, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:11:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-10 09:11:34 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=76.878357, avg_loss=0.698894, seen=110, correct=54, accuracy=0.490909
2025-09-10 09:11:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:11:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:11:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:11:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1926MB allocated=1885MB
2025-09-10 09:11:36 (federatedscope.llm.llm_local.client:480) INFO: Client 19 Adapter 1 with val results: {'val_total': 110, 'val_loss': 76.87835693359375, 'val_avg_loss': 0.6988941539417614, 'val_seen': 110, 'val_correct': 54, 'val_acc': 0.4909090909090909}
2025-09-10 09:11:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:11:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:11:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:11:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:11:37 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.234592, avg_loss=0.780865, seen=40, correct=19, accuracy=0.475000
2025-09-10 09:11:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:11:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:11:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:11:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1926MB allocated=1885MB
2025-09-10 09:11:40 (federatedscope.llm.llm_local.client:501) INFO: Client 19 Adapter 1 with test results: {'test_total': 40, 'test_loss': 31.23459243774414, 'test_avg_loss': 0.7808648109436035, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-10 09:11:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-10 09:11:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:11:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=45, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:11:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-10 09:11:42 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=76.130226, avg_loss=0.692093, seen=110, correct=59, accuracy=0.536364
2025-09-10 09:11:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:11:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:11:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:11:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1926MB allocated=1885MB
2025-09-10 09:11:44 (federatedscope.llm.llm_local.client:480) INFO: Client 19 Adapter 2 with val results: {'val_total': 110, 'val_loss': 76.1302261352539, 'val_avg_loss': 0.6920929648659446, 'val_seen': 110, 'val_correct': 59, 'val_acc': 0.5363636363636364}
2025-09-10 09:11:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:11:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:11:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:11:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:11:45 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.098549, avg_loss=0.702464, seen=40, correct=26, accuracy=0.650000
2025-09-10 09:11:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:11:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:11:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:11:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1926MB allocated=1885MB
2025-09-10 09:11:46 (federatedscope.llm.llm_local.client:501) INFO: Client 19 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.098548889160156, 'test_avg_loss': 0.7024637222290039, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-10 09:11:46 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:11:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-09-10 09:11:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:11:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=63, num_train_batch_last_epoch=37, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:11:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-09-10 09:11:50 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=126, loss_sum=90.229691, avg_loss=0.716109, seen=126, correct=72, accuracy=0.571429
2025-09-10 09:11:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:11:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:11:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:11:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1926MB allocated=1919MB
2025-09-10 09:11:52 (federatedscope.llm.llm_local.client:480) INFO: Client 20 Adapter 0 with val results: {'val_total': 126, 'val_loss': 90.22969055175781, 'val_avg_loss': 0.7161086551726811, 'val_seen': 126, 'val_correct': 72, 'val_acc': 0.5714285714285714}
2025-09-10 09:11:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:11:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:11:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:11:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:11:54 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.268192, avg_loss=0.631705, seen=40, correct=26, accuracy=0.650000
2025-09-10 09:11:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:11:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:11:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:11:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1926MB allocated=1919MB
2025-09-10 09:11:56 (federatedscope.llm.llm_local.client:501) INFO: Client 20 Adapter 0 with test results: {'test_total': 40, 'test_loss': 25.268192291259766, 'test_avg_loss': 0.6317048072814941, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-10 09:11:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-09-10 09:11:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:11:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=63, num_train_batch_last_epoch=37, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:11:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-09-10 09:11:59 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=126, loss_sum=86.976700, avg_loss=0.690291, seen=126, correct=65, accuracy=0.515873
2025-09-10 09:11:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:11:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:12:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:12:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1926MB allocated=1894MB
2025-09-10 09:12:00 (federatedscope.llm.llm_local.client:480) INFO: Client 20 Adapter 1 with val results: {'val_total': 126, 'val_loss': 86.97669982910156, 'val_avg_loss': 0.690291268484933, 'val_seen': 126, 'val_correct': 65, 'val_acc': 0.5158730158730159}
2025-09-10 09:12:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:12:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:12:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:12:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:12:02 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.053574, avg_loss=0.651339, seen=40, correct=23, accuracy=0.575000
2025-09-10 09:12:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:12:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:12:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:12:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1926MB allocated=1894MB
2025-09-10 09:12:04 (federatedscope.llm.llm_local.client:501) INFO: Client 20 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.053573608398438, 'test_avg_loss': 0.651339340209961, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-10 09:12:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-09-10 09:12:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:12:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=63, num_train_batch_last_epoch=37, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:12:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-09-10 09:12:08 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=126, loss_sum=89.035484, avg_loss=0.706631, seen=126, correct=74, accuracy=0.587302
2025-09-10 09:12:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:12:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:12:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:12:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1926MB allocated=1894MB
2025-09-10 09:12:09 (federatedscope.llm.llm_local.client:480) INFO: Client 20 Adapter 2 with val results: {'val_total': 126, 'val_loss': 89.03548431396484, 'val_avg_loss': 0.7066308278886099, 'val_seen': 126, 'val_correct': 74, 'val_acc': 0.5873015873015873}
2025-09-10 09:12:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:12:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:12:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:12:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:12:11 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.729874, avg_loss=0.618247, seen=40, correct=24, accuracy=0.600000
2025-09-10 09:12:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:12:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:12:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:12:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1926MB allocated=1894MB
2025-09-10 09:12:12 (federatedscope.llm.llm_local.client:501) INFO: Client 20 Adapter 2 with test results: {'test_total': 40, 'test_loss': 24.729873657226562, 'test_avg_loss': 0.6182468414306641, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-10 09:12:12 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:12:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-09-10 09:12:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:12:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=77, num_train_batch_last_epoch=23, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:12:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-09-10 09:12:18 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=153, loss_sum=101.798248, avg_loss=0.665348, seen=153, correct=94, accuracy=0.614379
2025-09-10 09:12:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:12:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:12:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:12:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1946MB allocated=1927MB
2025-09-10 09:12:20 (federatedscope.llm.llm_local.client:480) INFO: Client 21 Adapter 0 with val results: {'val_total': 153, 'val_loss': 101.79824829101562, 'val_avg_loss': 0.6653480280458538, 'val_seen': 153, 'val_correct': 94, 'val_acc': 0.6143790849673203}
2025-09-10 09:12:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:12:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:12:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:12:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:12:21 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.286346, avg_loss=0.757159, seen=40, correct=18, accuracy=0.450000
2025-09-10 09:12:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:12:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:12:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:12:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1946MB allocated=1927MB
2025-09-10 09:12:23 (federatedscope.llm.llm_local.client:501) INFO: Client 21 Adapter 0 with test results: {'test_total': 40, 'test_loss': 30.286346435546875, 'test_avg_loss': 0.7571586608886719, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-10 09:12:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-09-10 09:12:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:12:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=77, num_train_batch_last_epoch=23, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:12:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-09-10 09:12:27 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=153, loss_sum=102.090965, avg_loss=0.667261, seen=153, correct=94, accuracy=0.614379
2025-09-10 09:12:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:12:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:12:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:12:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1946MB allocated=1902MB
2025-09-10 09:12:28 (federatedscope.llm.llm_local.client:480) INFO: Client 21 Adapter 1 with val results: {'val_total': 153, 'val_loss': 102.0909652709961, 'val_avg_loss': 0.6672612109215431, 'val_seen': 153, 'val_correct': 94, 'val_acc': 0.6143790849673203}
2025-09-10 09:12:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:12:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:12:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:12:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:12:30 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.691372, avg_loss=0.742284, seen=40, correct=22, accuracy=0.550000
2025-09-10 09:12:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:12:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:12:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:12:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1946MB allocated=1902MB
2025-09-10 09:12:32 (federatedscope.llm.llm_local.client:501) INFO: Client 21 Adapter 1 with test results: {'test_total': 40, 'test_loss': 29.69137191772461, 'test_avg_loss': 0.7422842979431152, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-10 09:12:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-09-10 09:12:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:12:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=77, num_train_batch_last_epoch=23, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:12:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-09-10 09:12:37 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=153, loss_sum=106.636383, avg_loss=0.696970, seen=153, correct=82, accuracy=0.535948
2025-09-10 09:12:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:12:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:12:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:12:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1946MB allocated=1902MB
2025-09-10 09:12:38 (federatedscope.llm.llm_local.client:480) INFO: Client 21 Adapter 2 with val results: {'val_total': 153, 'val_loss': 106.63638305664062, 'val_avg_loss': 0.6969698238996119, 'val_seen': 153, 'val_correct': 82, 'val_acc': 0.5359477124183006}
2025-09-10 09:12:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:12:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:12:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:12:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:12:40 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.786621, avg_loss=0.644666, seen=40, correct=25, accuracy=0.625000
2025-09-10 09:12:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:12:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:12:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:12:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1946MB allocated=1902MB
2025-09-10 09:12:42 (federatedscope.llm.llm_local.client:501) INFO: Client 21 Adapter 2 with test results: {'test_total': 40, 'test_loss': 25.78662109375, 'test_avg_loss': 0.64466552734375, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-10 09:12:42 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:12:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-10 09:12:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:12:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=4, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:12:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-10 09:12:44 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=9.325961, avg_loss=0.847815, seen=11, correct=4, accuracy=0.363636
2025-09-10 09:12:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:12:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:12:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:12:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1946MB allocated=1936MB
2025-09-10 09:12:46 (federatedscope.llm.llm_local.client:480) INFO: Client 22 Adapter 0 with val results: {'val_total': 11, 'val_loss': 9.325961112976074, 'val_avg_loss': 0.8478146466341886, 'val_seen': 11, 'val_correct': 4, 'val_acc': 0.36363636363636365}
2025-09-10 09:12:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:12:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:12:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:12:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:12:47 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.625280, avg_loss=0.690632, seen=40, correct=23, accuracy=0.575000
2025-09-10 09:12:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:12:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:12:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:12:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1946MB allocated=1936MB
2025-09-10 09:12:49 (federatedscope.llm.llm_local.client:501) INFO: Client 22 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.625280380249023, 'test_avg_loss': 0.6906320095062256, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-10 09:12:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-10 09:12:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:12:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=4, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:12:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-10 09:12:50 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=8.914474, avg_loss=0.810407, seen=11, correct=4, accuracy=0.363636
2025-09-10 09:12:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:12:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:12:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:12:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1946MB allocated=1910MB
2025-09-10 09:12:51 (federatedscope.llm.llm_local.client:480) INFO: Client 22 Adapter 1 with val results: {'val_total': 11, 'val_loss': 8.914473533630371, 'val_avg_loss': 0.8104066848754883, 'val_seen': 11, 'val_correct': 4, 'val_acc': 0.36363636363636365}
2025-09-10 09:12:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:12:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:12:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:12:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:12:52 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.980743, avg_loss=0.699519, seen=40, correct=23, accuracy=0.575000
2025-09-10 09:12:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:12:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:12:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:12:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1946MB allocated=1910MB
2025-09-10 09:12:53 (federatedscope.llm.llm_local.client:501) INFO: Client 22 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.980743408203125, 'test_avg_loss': 0.6995185852050781, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-10 09:12:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-10 09:12:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:12:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=4, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:12:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-10 09:12:54 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.633342, avg_loss=0.693940, seen=11, correct=9, accuracy=0.818182
2025-09-10 09:12:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:12:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:12:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:12:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1946MB allocated=1910MB
2025-09-10 09:12:57 (federatedscope.llm.llm_local.client:480) INFO: Client 22 Adapter 2 with val results: {'val_total': 11, 'val_loss': 7.6333417892456055, 'val_avg_loss': 0.6939401626586914, 'val_seen': 11, 'val_correct': 9, 'val_acc': 0.8181818181818182}
2025-09-10 09:12:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:12:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:12:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:12:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:12:58 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.549261, avg_loss=0.738732, seen=40, correct=22, accuracy=0.550000
2025-09-10 09:12:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:12:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:12:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:13:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1946MB allocated=1910MB
2025-09-10 09:13:00 (federatedscope.llm.llm_local.client:501) INFO: Client 22 Adapter 2 with test results: {'test_total': 40, 'test_loss': 29.54926109313965, 'test_avg_loss': 0.7387315273284912, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-10 09:13:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:13:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-09-10 09:13:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:13:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=15, num_train_batch_last_epoch=10, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:13:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-10 09:13:03 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=30, loss_sum=18.069229, avg_loss=0.602308, seen=30, correct=20, accuracy=0.666667
2025-09-10 09:13:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:13:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:13:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:13:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1966MB allocated=1944MB
2025-09-10 09:13:05 (federatedscope.llm.llm_local.client:480) INFO: Client 23 Adapter 0 with val results: {'val_total': 30, 'val_loss': 18.069229125976562, 'val_avg_loss': 0.6023076375325521, 'val_seen': 30, 'val_correct': 20, 'val_acc': 0.6666666666666666}
2025-09-10 09:13:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:13:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:13:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:13:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:13:07 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.220098, avg_loss=0.605502, seen=40, correct=24, accuracy=0.600000
2025-09-10 09:13:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:13:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:13:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:13:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1966MB allocated=1944MB
2025-09-10 09:13:09 (federatedscope.llm.llm_local.client:501) INFO: Client 23 Adapter 0 with test results: {'test_total': 40, 'test_loss': 24.2200984954834, 'test_avg_loss': 0.6055024623870849, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-10 09:13:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-09-10 09:13:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:13:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=15, num_train_batch_last_epoch=10, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:13:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-10 09:13:11 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=30, loss_sum=18.250710, avg_loss=0.608357, seen=30, correct=18, accuracy=0.600000
2025-09-10 09:13:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:13:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:13:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:13:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1966MB allocated=1919MB
2025-09-10 09:13:12 (federatedscope.llm.llm_local.client:480) INFO: Client 23 Adapter 1 with val results: {'val_total': 30, 'val_loss': 18.250709533691406, 'val_avg_loss': 0.6083569844563802, 'val_seen': 30, 'val_correct': 18, 'val_acc': 0.6}
2025-09-10 09:13:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:13:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:13:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:13:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:13:13 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.641811, avg_loss=0.641045, seen=40, correct=26, accuracy=0.650000
2025-09-10 09:13:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:13:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:13:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:13:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1966MB allocated=1919MB
2025-09-10 09:13:14 (federatedscope.llm.llm_local.client:501) INFO: Client 23 Adapter 1 with test results: {'test_total': 40, 'test_loss': 25.64181137084961, 'test_avg_loss': 0.6410452842712402, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-10 09:13:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-09-10 09:13:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:13:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=15, num_train_batch_last_epoch=10, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:13:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-10 09:13:16 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=30, loss_sum=19.484493, avg_loss=0.649483, seen=30, correct=21, accuracy=0.700000
2025-09-10 09:13:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:13:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:13:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:13:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1966MB allocated=1919MB
2025-09-10 09:13:17 (federatedscope.llm.llm_local.client:480) INFO: Client 23 Adapter 2 with val results: {'val_total': 30, 'val_loss': 19.484493255615234, 'val_avg_loss': 0.6494831085205078, 'val_seen': 30, 'val_correct': 21, 'val_acc': 0.7}
2025-09-10 09:13:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:13:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:13:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:13:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:13:19 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.012926, avg_loss=0.650323, seen=40, correct=23, accuracy=0.575000
2025-09-10 09:13:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:13:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:13:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:13:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1966MB allocated=1919MB
2025-09-10 09:13:20 (federatedscope.llm.llm_local.client:501) INFO: Client 23 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.01292610168457, 'test_avg_loss': 0.6503231525421143, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-10 09:13:20 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:13:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:13:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:13:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:13:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:13:25 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.760712, avg_loss=0.678804, seen=200, correct=112, accuracy=0.560000
2025-09-10 09:13:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:13:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:13:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:13:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1966MB allocated=1952MB
2025-09-10 09:13:27 (federatedscope.llm.llm_local.client:480) INFO: Client 24 Adapter 0 with val results: {'val_total': 200, 'val_loss': 135.76071166992188, 'val_avg_loss': 0.6788035583496094, 'val_seen': 200, 'val_correct': 112, 'val_acc': 0.56}
2025-09-10 09:13:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:13:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:13:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:13:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:13:29 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.177319, avg_loss=0.779433, seen=40, correct=18, accuracy=0.450000
2025-09-10 09:13:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:13:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:13:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:13:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1966MB allocated=1952MB
2025-09-10 09:13:31 (federatedscope.llm.llm_local.client:501) INFO: Client 24 Adapter 0 with test results: {'test_total': 40, 'test_loss': 31.177318572998047, 'test_avg_loss': 0.7794329643249511, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-10 09:13:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:13:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:13:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:13:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:13:35 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=134.620392, avg_loss=0.673102, seen=200, correct=120, accuracy=0.600000
2025-09-10 09:13:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:13:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:13:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:13:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1966MB allocated=1927MB
2025-09-10 09:13:36 (federatedscope.llm.llm_local.client:480) INFO: Client 24 Adapter 1 with val results: {'val_total': 200, 'val_loss': 134.62039184570312, 'val_avg_loss': 0.6731019592285157, 'val_seen': 200, 'val_correct': 120, 'val_acc': 0.6}
2025-09-10 09:13:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:13:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:13:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:13:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:13:38 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.749962, avg_loss=0.768749, seen=40, correct=21, accuracy=0.525000
2025-09-10 09:13:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:13:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:13:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:13:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1966MB allocated=1927MB
2025-09-10 09:13:39 (federatedscope.llm.llm_local.client:501) INFO: Client 24 Adapter 1 with test results: {'test_total': 40, 'test_loss': 30.749961853027344, 'test_avg_loss': 0.7687490463256836, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-10 09:13:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:13:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:13:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:13:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:13:43 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.719696, avg_loss=0.678598, seen=200, correct=114, accuracy=0.570000
2025-09-10 09:13:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:13:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:13:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:13:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1966MB allocated=1927MB
2025-09-10 09:13:45 (federatedscope.llm.llm_local.client:480) INFO: Client 24 Adapter 2 with val results: {'val_total': 200, 'val_loss': 135.71969604492188, 'val_avg_loss': 0.6785984802246093, 'val_seen': 200, 'val_correct': 114, 'val_acc': 0.57}
2025-09-10 09:13:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:13:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:13:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:13:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:13:47 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.361237, avg_loss=0.759031, seen=40, correct=19, accuracy=0.475000
2025-09-10 09:13:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:13:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:13:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:13:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1966MB allocated=1927MB
2025-09-10 09:13:49 (federatedscope.llm.llm_local.client:501) INFO: Client 24 Adapter 2 with test results: {'test_total': 40, 'test_loss': 30.361236572265625, 'test_avg_loss': 0.7590309143066406, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-10 09:13:49 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:13:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:13:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:13:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:13:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:13:53 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=146.362366, avg_loss=0.731812, seen=200, correct=105, accuracy=0.525000
2025-09-10 09:13:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:13:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:13:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:13:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1966MB allocated=1961MB
2025-09-10 09:13:55 (federatedscope.llm.llm_local.client:480) INFO: Client 25 Adapter 0 with val results: {'val_total': 200, 'val_loss': 146.36236572265625, 'val_avg_loss': 0.7318118286132812, 'val_seen': 200, 'val_correct': 105, 'val_acc': 0.525}
2025-09-10 09:13:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:13:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:13:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:13:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:13:58 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.499737, avg_loss=0.787493, seen=40, correct=19, accuracy=0.475000
2025-09-10 09:13:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:13:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:13:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:13:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1966MB allocated=1961MB
2025-09-10 09:13:59 (federatedscope.llm.llm_local.client:501) INFO: Client 25 Adapter 0 with test results: {'test_total': 40, 'test_loss': 31.499736785888672, 'test_avg_loss': 0.7874934196472168, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-10 09:14:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:14:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:14:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:14:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:14:04 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=145.899200, avg_loss=0.729496, seen=200, correct=104, accuracy=0.520000
2025-09-10 09:14:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:14:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:14:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:14:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1966MB allocated=1936MB
2025-09-10 09:14:05 (federatedscope.llm.llm_local.client:480) INFO: Client 25 Adapter 1 with val results: {'val_total': 200, 'val_loss': 145.89920043945312, 'val_avg_loss': 0.7294960021972656, 'val_seen': 200, 'val_correct': 104, 'val_acc': 0.52}
2025-09-10 09:14:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:14:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:14:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:14:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:14:07 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.043898, avg_loss=0.751097, seen=40, correct=18, accuracy=0.450000
2025-09-10 09:14:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:14:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:14:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:14:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1966MB allocated=1936MB
2025-09-10 09:14:08 (federatedscope.llm.llm_local.client:501) INFO: Client 25 Adapter 1 with test results: {'test_total': 40, 'test_loss': 30.04389762878418, 'test_avg_loss': 0.7510974407196045, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-10 09:14:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:14:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:14:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:14:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:14:13 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=142.585159, avg_loss=0.712926, seen=200, correct=107, accuracy=0.535000
2025-09-10 09:14:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:14:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:14:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:14:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1966MB allocated=1936MB
2025-09-10 09:14:15 (federatedscope.llm.llm_local.client:480) INFO: Client 25 Adapter 2 with val results: {'val_total': 200, 'val_loss': 142.5851593017578, 'val_avg_loss': 0.7129257965087891, 'val_seen': 200, 'val_correct': 107, 'val_acc': 0.535}
2025-09-10 09:14:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:14:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:14:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:14:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:14:17 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=32.601845, avg_loss=0.815046, seen=40, correct=21, accuracy=0.525000
2025-09-10 09:14:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:14:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:14:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:14:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1966MB allocated=1936MB
2025-09-10 09:14:20 (federatedscope.llm.llm_local.client:501) INFO: Client 25 Adapter 2 with test results: {'test_total': 40, 'test_loss': 32.601844787597656, 'test_avg_loss': 0.8150461196899415, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-10 09:14:20 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:14:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-09-10 09:14:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:14:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=81, num_train_batch_last_epoch=19, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:14:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-09-10 09:14:25 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=161, loss_sum=100.699905, avg_loss=0.625465, seen=161, correct=107, accuracy=0.664596
2025-09-10 09:14:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:14:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:14:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:14:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1986MB allocated=1969MB
2025-09-10 09:14:27 (federatedscope.llm.llm_local.client:480) INFO: Client 26 Adapter 0 with val results: {'val_total': 161, 'val_loss': 100.69990539550781, 'val_avg_loss': 0.6254652509037752, 'val_seen': 161, 'val_correct': 107, 'val_acc': 0.6645962732919255}
2025-09-10 09:14:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:14:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:14:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:14:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:14:28 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.701393, avg_loss=0.617535, seen=40, correct=27, accuracy=0.675000
2025-09-10 09:14:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:14:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:14:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:14:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1986MB allocated=1969MB
2025-09-10 09:14:29 (federatedscope.llm.llm_local.client:501) INFO: Client 26 Adapter 0 with test results: {'test_total': 40, 'test_loss': 24.701393127441406, 'test_avg_loss': 0.6175348281860351, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-10 09:14:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-09-10 09:14:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:14:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=81, num_train_batch_last_epoch=19, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:14:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-09-10 09:14:33 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=161, loss_sum=102.677933, avg_loss=0.637751, seen=161, correct=100, accuracy=0.621118
2025-09-10 09:14:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:14:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:14:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:14:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1986MB allocated=1944MB
2025-09-10 09:14:34 (federatedscope.llm.llm_local.client:480) INFO: Client 26 Adapter 1 with val results: {'val_total': 161, 'val_loss': 102.67793273925781, 'val_avg_loss': 0.6377511350264461, 'val_seen': 161, 'val_correct': 100, 'val_acc': 0.6211180124223602}
2025-09-10 09:14:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:14:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:14:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:14:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:14:36 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.843285, avg_loss=0.621082, seen=40, correct=23, accuracy=0.575000
2025-09-10 09:14:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:14:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:14:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:14:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1986MB allocated=1944MB
2025-09-10 09:14:37 (federatedscope.llm.llm_local.client:501) INFO: Client 26 Adapter 1 with test results: {'test_total': 40, 'test_loss': 24.843284606933594, 'test_avg_loss': 0.6210821151733399, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-10 09:14:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-09-10 09:14:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:14:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=81, num_train_batch_last_epoch=19, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:14:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-09-10 09:14:41 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=161, loss_sum=102.884232, avg_loss=0.639032, seen=161, correct=105, accuracy=0.652174
2025-09-10 09:14:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:14:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:14:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:14:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1986MB allocated=1944MB
2025-09-10 09:14:45 (federatedscope.llm.llm_local.client:480) INFO: Client 26 Adapter 2 with val results: {'val_total': 161, 'val_loss': 102.88423156738281, 'val_avg_loss': 0.6390324942073466, 'val_seen': 161, 'val_correct': 105, 'val_acc': 0.6521739130434783}
2025-09-10 09:14:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:14:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:14:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:14:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:14:48 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.289148, avg_loss=0.657229, seen=40, correct=25, accuracy=0.625000
2025-09-10 09:14:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:14:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:14:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:14:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1986MB allocated=1944MB
2025-09-10 09:14:49 (federatedscope.llm.llm_local.client:501) INFO: Client 26 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.289148330688477, 'test_avg_loss': 0.6572287082672119, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-10 09:14:49 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:14:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-09-10 09:14:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:14:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=62, num_train_batch_last_epoch=38, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:14:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-09-10 09:14:53 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=123, loss_sum=81.165985, avg_loss=0.659886, seen=123, correct=80, accuracy=0.650407
2025-09-10 09:14:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:14:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:14:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:14:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1986MB allocated=1978MB
2025-09-10 09:14:55 (federatedscope.llm.llm_local.client:480) INFO: Client 27 Adapter 0 with val results: {'val_total': 123, 'val_loss': 81.16598510742188, 'val_avg_loss': 0.6598860577839177, 'val_seen': 123, 'val_correct': 80, 'val_acc': 0.6504065040650406}
2025-09-10 09:14:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:14:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:14:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:14:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:14:56 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.890369, avg_loss=0.672259, seen=40, correct=23, accuracy=0.575000
2025-09-10 09:14:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:14:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:14:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:14:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1986MB allocated=1978MB
2025-09-10 09:14:57 (federatedscope.llm.llm_local.client:501) INFO: Client 27 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.890369415283203, 'test_avg_loss': 0.6722592353820801, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-10 09:14:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-09-10 09:14:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:14:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=62, num_train_batch_last_epoch=38, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:15:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-09-10 09:15:00 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=123, loss_sum=82.426277, avg_loss=0.670132, seen=123, correct=70, accuracy=0.569106
2025-09-10 09:15:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:15:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:15:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:15:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1986MB allocated=1952MB
2025-09-10 09:15:01 (federatedscope.llm.llm_local.client:480) INFO: Client 27 Adapter 1 with val results: {'val_total': 123, 'val_loss': 82.42627716064453, 'val_avg_loss': 0.6701323346393865, 'val_seen': 123, 'val_correct': 70, 'val_acc': 0.5691056910569106}
2025-09-10 09:15:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:15:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:15:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:15:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:15:02 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.497957, avg_loss=0.637449, seen=40, correct=25, accuracy=0.625000
2025-09-10 09:15:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:15:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:15:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:15:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1986MB allocated=1952MB
2025-09-10 09:15:04 (federatedscope.llm.llm_local.client:501) INFO: Client 27 Adapter 1 with test results: {'test_total': 40, 'test_loss': 25.497957229614258, 'test_avg_loss': 0.6374489307403565, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-10 09:15:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-09-10 09:15:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:15:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=62, num_train_batch_last_epoch=38, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:15:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-09-10 09:15:08 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=123, loss_sum=83.812637, avg_loss=0.681404, seen=123, correct=67, accuracy=0.544715
2025-09-10 09:15:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:15:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:15:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:15:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1986MB allocated=1952MB
2025-09-10 09:15:10 (federatedscope.llm.llm_local.client:480) INFO: Client 27 Adapter 2 with val results: {'val_total': 123, 'val_loss': 83.81263732910156, 'val_avg_loss': 0.681403555521151, 'val_seen': 123, 'val_correct': 67, 'val_acc': 0.5447154471544715}
2025-09-10 09:15:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:15:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:15:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:15:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:15:12 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.076317, avg_loss=0.726908, seen=40, correct=19, accuracy=0.475000
2025-09-10 09:15:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:15:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:15:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:15:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=1986MB allocated=1952MB
2025-09-10 09:15:14 (federatedscope.llm.llm_local.client:501) INFO: Client 27 Adapter 2 with test results: {'test_total': 40, 'test_loss': 29.076316833496094, 'test_avg_loss': 0.7269079208374023, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-10 09:15:14 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:15:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-09-10 09:15:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:15:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=38, num_train_batch_last_epoch=24, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:15:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-10 09:15:17 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=75, loss_sum=53.616940, avg_loss=0.714893, seen=75, correct=38, accuracy=0.506667
2025-09-10 09:15:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:15:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:15:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:15:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2006MB allocated=1986MB
2025-09-10 09:15:18 (federatedscope.llm.llm_local.client:480) INFO: Client 28 Adapter 0 with val results: {'val_total': 75, 'val_loss': 53.616939544677734, 'val_avg_loss': 0.7148925272623697, 'val_seen': 75, 'val_correct': 38, 'val_acc': 0.5066666666666667}
2025-09-10 09:15:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:15:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:15:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:15:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:15:19 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.087292, avg_loss=0.777182, seen=40, correct=18, accuracy=0.450000
2025-09-10 09:15:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:15:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:15:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:15:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2006MB allocated=1986MB
2025-09-10 09:15:21 (federatedscope.llm.llm_local.client:501) INFO: Client 28 Adapter 0 with test results: {'test_total': 40, 'test_loss': 31.087291717529297, 'test_avg_loss': 0.7771822929382324, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-10 09:15:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-09-10 09:15:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:15:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=38, num_train_batch_last_epoch=24, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:15:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-10 09:15:23 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=75, loss_sum=47.913254, avg_loss=0.638843, seen=75, correct=49, accuracy=0.653333
2025-09-10 09:15:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:15:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:15:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:15:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2006MB allocated=1961MB
2025-09-10 09:15:25 (federatedscope.llm.llm_local.client:480) INFO: Client 28 Adapter 1 with val results: {'val_total': 75, 'val_loss': 47.91325378417969, 'val_avg_loss': 0.6388433837890625, 'val_seen': 75, 'val_correct': 49, 'val_acc': 0.6533333333333333}
2025-09-10 09:15:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:15:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:15:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:15:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:15:26 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.905750, avg_loss=0.697644, seen=40, correct=22, accuracy=0.550000
2025-09-10 09:15:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:15:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:15:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:15:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2006MB allocated=1961MB
2025-09-10 09:15:28 (federatedscope.llm.llm_local.client:501) INFO: Client 28 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.905750274658203, 'test_avg_loss': 0.6976437568664551, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-10 09:15:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-09-10 09:15:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:15:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=38, num_train_batch_last_epoch=24, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:15:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-10 09:15:31 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=75, loss_sum=53.239349, avg_loss=0.709858, seen=75, correct=40, accuracy=0.533333
2025-09-10 09:15:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:15:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:15:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:15:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2006MB allocated=1961MB
2025-09-10 09:15:32 (federatedscope.llm.llm_local.client:480) INFO: Client 28 Adapter 2 with val results: {'val_total': 75, 'val_loss': 53.239349365234375, 'val_avg_loss': 0.7098579915364583, 'val_seen': 75, 'val_correct': 40, 'val_acc': 0.5333333333333333}
2025-09-10 09:15:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:15:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:15:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:15:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:15:34 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.290707, avg_loss=0.682268, seen=40, correct=25, accuracy=0.625000
2025-09-10 09:15:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:15:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:15:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:15:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2006MB allocated=1961MB
2025-09-10 09:15:36 (federatedscope.llm.llm_local.client:501) INFO: Client 28 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.290706634521484, 'test_avg_loss': 0.6822676658630371, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-10 09:15:36 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:15:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:15:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:15:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:15:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:15:40 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=142.140839, avg_loss=0.710704, seen=200, correct=114, accuracy=0.570000
2025-09-10 09:15:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:15:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:15:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:15:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2006MB allocated=1994MB
2025-09-10 09:15:42 (federatedscope.llm.llm_local.client:480) INFO: Client 29 Adapter 0 with val results: {'val_total': 200, 'val_loss': 142.14083862304688, 'val_avg_loss': 0.7107041931152344, 'val_seen': 200, 'val_correct': 114, 'val_acc': 0.57}
2025-09-10 09:15:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:15:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:15:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:15:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:15:42 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.902283, avg_loss=0.672557, seen=40, correct=23, accuracy=0.575000
2025-09-10 09:15:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:15:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:15:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:15:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2006MB allocated=1994MB
2025-09-10 09:15:44 (federatedscope.llm.llm_local.client:501) INFO: Client 29 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.90228271484375, 'test_avg_loss': 0.6725570678710937, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-10 09:15:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:15:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:15:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:15:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:15:48 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=139.477585, avg_loss=0.697388, seen=200, correct=121, accuracy=0.605000
2025-09-10 09:15:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:15:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:15:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:15:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2006MB allocated=1969MB
2025-09-10 09:15:49 (federatedscope.llm.llm_local.client:480) INFO: Client 29 Adapter 1 with val results: {'val_total': 200, 'val_loss': 139.4775848388672, 'val_avg_loss': 0.6973879241943359, 'val_seen': 200, 'val_correct': 121, 'val_acc': 0.605}
2025-09-10 09:15:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:15:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:15:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:15:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:15:51 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.011904, avg_loss=0.675298, seen=40, correct=23, accuracy=0.575000
2025-09-10 09:15:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:15:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:15:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:15:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2006MB allocated=1969MB
2025-09-10 09:15:52 (federatedscope.llm.llm_local.client:501) INFO: Client 29 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.011903762817383, 'test_avg_loss': 0.6752975940704345, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-10 09:15:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:15:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:15:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:15:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:15:57 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.849258, avg_loss=0.679246, seen=200, correct=107, accuracy=0.535000
2025-09-10 09:15:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:15:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:15:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:15:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2006MB allocated=1969MB
2025-09-10 09:15:59 (federatedscope.llm.llm_local.client:480) INFO: Client 29 Adapter 2 with val results: {'val_total': 200, 'val_loss': 135.84925842285156, 'val_avg_loss': 0.6792462921142578, 'val_seen': 200, 'val_correct': 107, 'val_acc': 0.535}
2025-09-10 09:15:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:15:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:15:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:16:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:16:01 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.110645, avg_loss=0.602766, seen=40, correct=27, accuracy=0.675000
2025-09-10 09:16:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:16:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:16:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:16:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2006MB allocated=1969MB
2025-09-10 09:16:03 (federatedscope.llm.llm_local.client:501) INFO: Client 29 Adapter 2 with test results: {'test_total': 40, 'test_loss': 24.110645294189453, 'test_avg_loss': 0.6027661323547363, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-10 09:16:03 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:16:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-09-10 09:16:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:16:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=85, num_train_batch_last_epoch=15, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:16:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-09-10 09:16:07 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=170, loss_sum=112.575615, avg_loss=0.662209, seen=170, correct=103, accuracy=0.605882
2025-09-10 09:16:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:16:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:16:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:16:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2006MB allocated=2003MB
2025-09-10 09:16:09 (federatedscope.llm.llm_local.client:480) INFO: Client 30 Adapter 0 with val results: {'val_total': 170, 'val_loss': 112.57561492919922, 'val_avg_loss': 0.6622094995835248, 'val_seen': 170, 'val_correct': 103, 'val_acc': 0.6058823529411764}
2025-09-10 09:16:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:16:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:16:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:16:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:16:11 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.904398, avg_loss=0.647610, seen=40, correct=24, accuracy=0.600000
2025-09-10 09:16:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:16:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:16:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:16:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2006MB allocated=2003MB
2025-09-10 09:16:12 (federatedscope.llm.llm_local.client:501) INFO: Client 30 Adapter 0 with test results: {'test_total': 40, 'test_loss': 25.90439796447754, 'test_avg_loss': 0.6476099491119385, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-10 09:16:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-09-10 09:16:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:16:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=85, num_train_batch_last_epoch=15, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:16:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-09-10 09:16:17 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=170, loss_sum=113.372231, avg_loss=0.666895, seen=170, correct=101, accuracy=0.594118
2025-09-10 09:16:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:16:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:16:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:16:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2006MB allocated=1978MB
2025-09-10 09:16:18 (federatedscope.llm.llm_local.client:480) INFO: Client 30 Adapter 1 with val results: {'val_total': 170, 'val_loss': 113.37223052978516, 'val_avg_loss': 0.6668954737046185, 'val_seen': 170, 'val_correct': 101, 'val_acc': 0.5941176470588235}
2025-09-10 09:16:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:16:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:16:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:16:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:16:20 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.474129, avg_loss=0.661853, seen=40, correct=18, accuracy=0.450000
2025-09-10 09:16:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:16:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:16:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:16:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2006MB allocated=1978MB
2025-09-10 09:16:21 (federatedscope.llm.llm_local.client:501) INFO: Client 30 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.47412872314453, 'test_avg_loss': 0.6618532180786133, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-10 09:16:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-09-10 09:16:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:16:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=85, num_train_batch_last_epoch=15, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:16:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-09-10 09:16:25 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=170, loss_sum=112.694702, avg_loss=0.662910, seen=170, correct=106, accuracy=0.623529
2025-09-10 09:16:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:16:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:16:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:16:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2006MB allocated=1978MB
2025-09-10 09:16:27 (federatedscope.llm.llm_local.client:480) INFO: Client 30 Adapter 2 with val results: {'val_total': 170, 'val_loss': 112.6947021484375, 'val_avg_loss': 0.6629100126378676, 'val_seen': 170, 'val_correct': 106, 'val_acc': 0.6235294117647059}
2025-09-10 09:16:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:16:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:16:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:16:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:16:29 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.881466, avg_loss=0.747037, seen=40, correct=21, accuracy=0.525000
2025-09-10 09:16:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:16:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:16:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:16:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2006MB allocated=1978MB
2025-09-10 09:16:31 (federatedscope.llm.llm_local.client:501) INFO: Client 30 Adapter 2 with test results: {'test_total': 40, 'test_loss': 29.881465911865234, 'test_avg_loss': 0.7470366477966308, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-10 09:16:31 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:16:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-09-10 09:16:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:16:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=97, num_train_batch_last_epoch=3, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:16:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-09-10 09:16:35 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=193, loss_sum=144.387604, avg_loss=0.748122, seen=193, correct=101, accuracy=0.523316
2025-09-10 09:16:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:16:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:16:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:16:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2026MB allocated=2011MB
2025-09-10 09:16:37 (federatedscope.llm.llm_local.client:480) INFO: Client 31 Adapter 0 with val results: {'val_total': 193, 'val_loss': 144.38760375976562, 'val_avg_loss': 0.748122299273397, 'val_seen': 193, 'val_correct': 101, 'val_acc': 0.5233160621761658}
2025-09-10 09:16:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:16:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:16:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:16:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:16:38 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.591446, avg_loss=0.614786, seen=40, correct=26, accuracy=0.650000
2025-09-10 09:16:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:16:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:16:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:16:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2026MB allocated=2011MB
2025-09-10 09:16:40 (federatedscope.llm.llm_local.client:501) INFO: Client 31 Adapter 0 with test results: {'test_total': 40, 'test_loss': 24.591445922851562, 'test_avg_loss': 0.6147861480712891, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-10 09:16:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-09-10 09:16:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:16:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=97, num_train_batch_last_epoch=3, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:16:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-09-10 09:16:44 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=193, loss_sum=142.597397, avg_loss=0.738847, seen=193, correct=91, accuracy=0.471503
2025-09-10 09:16:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:16:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:16:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:16:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2026MB allocated=1986MB
2025-09-10 09:16:45 (federatedscope.llm.llm_local.client:480) INFO: Client 31 Adapter 1 with val results: {'val_total': 193, 'val_loss': 142.59739685058594, 'val_avg_loss': 0.7388466158061447, 'val_seen': 193, 'val_correct': 91, 'val_acc': 0.47150259067357514}
2025-09-10 09:16:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:16:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:16:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:16:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:16:47 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.517021, avg_loss=0.662926, seen=40, correct=26, accuracy=0.650000
2025-09-10 09:16:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:16:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:16:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:16:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2026MB allocated=1986MB
2025-09-10 09:16:48 (federatedscope.llm.llm_local.client:501) INFO: Client 31 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.51702117919922, 'test_avg_loss': 0.6629255294799805, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-10 09:16:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-09-10 09:16:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:16:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=97, num_train_batch_last_epoch=3, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:16:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-09-10 09:16:52 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=193, loss_sum=133.792099, avg_loss=0.693223, seen=193, correct=110, accuracy=0.569948
2025-09-10 09:16:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:16:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:16:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:16:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2026MB allocated=1986MB
2025-09-10 09:16:54 (federatedscope.llm.llm_local.client:480) INFO: Client 31 Adapter 2 with val results: {'val_total': 193, 'val_loss': 133.79209899902344, 'val_avg_loss': 0.6932233108757692, 'val_seen': 193, 'val_correct': 110, 'val_acc': 0.5699481865284974}
2025-09-10 09:16:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:16:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:16:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:16:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:16:56 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.187391, avg_loss=0.654685, seen=40, correct=23, accuracy=0.575000
2025-09-10 09:16:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:16:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:16:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:16:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2026MB allocated=1986MB
2025-09-10 09:16:58 (federatedscope.llm.llm_local.client:501) INFO: Client 31 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.18739128112793, 'test_avg_loss': 0.6546847820281982, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-10 09:16:58 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:16:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-09-10 09:16:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:17:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=56, num_train_batch_last_epoch=44, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:17:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-10 09:17:02 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=112, loss_sum=66.521698, avg_loss=0.593944, seen=112, correct=76, accuracy=0.678571
2025-09-10 09:17:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:17:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:17:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:17:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2026MB allocated=2020MB
2025-09-10 09:17:04 (federatedscope.llm.llm_local.client:480) INFO: Client 32 Adapter 0 with val results: {'val_total': 112, 'val_loss': 66.52169799804688, 'val_avg_loss': 0.5939437321254185, 'val_seen': 112, 'val_correct': 76, 'val_acc': 0.6785714285714286}
2025-09-10 09:17:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:17:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:17:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:17:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:17:05 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.103971, avg_loss=0.702599, seen=40, correct=21, accuracy=0.525000
2025-09-10 09:17:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:17:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:17:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:17:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2026MB allocated=2020MB
2025-09-10 09:17:07 (federatedscope.llm.llm_local.client:501) INFO: Client 32 Adapter 0 with test results: {'test_total': 40, 'test_loss': 28.103971481323242, 'test_avg_loss': 0.702599287033081, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-10 09:17:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-09-10 09:17:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:17:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=56, num_train_batch_last_epoch=44, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:17:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-10 09:17:11 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=112, loss_sum=70.009636, avg_loss=0.625086, seen=112, correct=75, accuracy=0.669643
2025-09-10 09:17:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:17:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:17:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:17:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2026MB allocated=1994MB
2025-09-10 09:17:12 (federatedscope.llm.llm_local.client:480) INFO: Client 32 Adapter 1 with val results: {'val_total': 112, 'val_loss': 70.00963592529297, 'val_avg_loss': 0.6250860350472587, 'val_seen': 112, 'val_correct': 75, 'val_acc': 0.6696428571428571}
2025-09-10 09:17:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:17:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:17:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:17:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:17:14 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.739086, avg_loss=0.743477, seen=40, correct=20, accuracy=0.500000
2025-09-10 09:17:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:17:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:17:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:17:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2026MB allocated=1994MB
2025-09-10 09:17:16 (federatedscope.llm.llm_local.client:501) INFO: Client 32 Adapter 1 with test results: {'test_total': 40, 'test_loss': 29.739086151123047, 'test_avg_loss': 0.7434771537780762, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-10 09:17:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-09-10 09:17:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:17:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=56, num_train_batch_last_epoch=44, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:17:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-10 09:17:20 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=112, loss_sum=69.023460, avg_loss=0.616281, seen=112, correct=79, accuracy=0.705357
2025-09-10 09:17:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:17:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:17:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:17:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2026MB allocated=1994MB
2025-09-10 09:17:22 (federatedscope.llm.llm_local.client:480) INFO: Client 32 Adapter 2 with val results: {'val_total': 112, 'val_loss': 69.0234603881836, 'val_avg_loss': 0.6162808963230678, 'val_seen': 112, 'val_correct': 79, 'val_acc': 0.7053571428571429}
2025-09-10 09:17:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:17:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:17:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:17:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:17:23 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.809008, avg_loss=0.695225, seen=40, correct=23, accuracy=0.575000
2025-09-10 09:17:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:17:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:17:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:17:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2026MB allocated=1994MB
2025-09-10 09:17:25 (federatedscope.llm.llm_local.client:501) INFO: Client 32 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.80900764465332, 'test_avg_loss': 0.695225191116333, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-10 09:17:25 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:17:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-09-10 09:17:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:17:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=37, num_train_batch_last_epoch=26, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:17:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-10 09:17:27 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=74, loss_sum=57.039925, avg_loss=0.770810, seen=74, correct=34, accuracy=0.459459
2025-09-10 09:17:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:17:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:17:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:17:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2046MB allocated=2028MB
2025-09-10 09:17:29 (federatedscope.llm.llm_local.client:480) INFO: Client 33 Adapter 0 with val results: {'val_total': 74, 'val_loss': 57.03992462158203, 'val_avg_loss': 0.770809792183541, 'val_seen': 74, 'val_correct': 34, 'val_acc': 0.4594594594594595}
2025-09-10 09:17:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:17:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:17:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:17:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:17:31 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.574718, avg_loss=0.664368, seen=40, correct=22, accuracy=0.550000
2025-09-10 09:17:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:17:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:17:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:17:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2046MB allocated=2028MB
2025-09-10 09:17:32 (federatedscope.llm.llm_local.client:501) INFO: Client 33 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.574718475341797, 'test_avg_loss': 0.664367961883545, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-10 09:17:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-09-10 09:17:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:17:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=37, num_train_batch_last_epoch=26, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:17:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-10 09:17:34 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=74, loss_sum=52.820164, avg_loss=0.713786, seen=74, correct=42, accuracy=0.567568
2025-09-10 09:17:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:17:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:17:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:17:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2046MB allocated=2003MB
2025-09-10 09:17:36 (federatedscope.llm.llm_local.client:480) INFO: Client 33 Adapter 1 with val results: {'val_total': 74, 'val_loss': 52.82016372680664, 'val_avg_loss': 0.7137859963081978, 'val_seen': 74, 'val_correct': 42, 'val_acc': 0.5675675675675675}
2025-09-10 09:17:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:17:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:17:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:17:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:17:37 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.422247, avg_loss=0.660556, seen=40, correct=24, accuracy=0.600000
2025-09-10 09:17:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:17:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:17:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:17:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2046MB allocated=2003MB
2025-09-10 09:17:41 (federatedscope.llm.llm_local.client:501) INFO: Client 33 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.4222469329834, 'test_avg_loss': 0.6605561733245849, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-10 09:17:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-09-10 09:17:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:17:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=37, num_train_batch_last_epoch=26, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:17:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-10 09:17:43 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=74, loss_sum=52.216740, avg_loss=0.705632, seen=74, correct=44, accuracy=0.594595
2025-09-10 09:17:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:17:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:17:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:17:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2046MB allocated=2003MB
2025-09-10 09:17:44 (federatedscope.llm.llm_local.client:480) INFO: Client 33 Adapter 2 with val results: {'val_total': 74, 'val_loss': 52.216739654541016, 'val_avg_loss': 0.7056316169532569, 'val_seen': 74, 'val_correct': 44, 'val_acc': 0.5945945945945946}
2025-09-10 09:17:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:17:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:17:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:17:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:17:46 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.734932, avg_loss=0.668373, seen=40, correct=21, accuracy=0.525000
2025-09-10 09:17:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:17:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:17:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:17:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2046MB allocated=2003MB
2025-09-10 09:17:47 (federatedscope.llm.llm_local.client:501) INFO: Client 33 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.73493194580078, 'test_avg_loss': 0.6683732986450195, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-10 09:17:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:17:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:17:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:17:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:17:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:17:51 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=126.042274, avg_loss=0.630211, seen=200, correct=134, accuracy=0.670000
2025-09-10 09:17:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:17:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:17:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:17:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2046MB allocated=2036MB
2025-09-10 09:17:53 (federatedscope.llm.llm_local.client:480) INFO: Client 34 Adapter 0 with val results: {'val_total': 200, 'val_loss': 126.04227447509766, 'val_avg_loss': 0.6302113723754883, 'val_seen': 200, 'val_correct': 134, 'val_acc': 0.67}
2025-09-10 09:17:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:17:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:17:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:17:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:17:55 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.428854, avg_loss=0.585721, seen=40, correct=29, accuracy=0.725000
2025-09-10 09:17:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:17:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:17:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:17:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2046MB allocated=2036MB
2025-09-10 09:17:56 (federatedscope.llm.llm_local.client:501) INFO: Client 34 Adapter 0 with test results: {'test_total': 40, 'test_loss': 23.42885398864746, 'test_avg_loss': 0.5857213497161865, 'test_seen': 40, 'test_correct': 29, 'test_acc': 0.725}
2025-09-10 09:17:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:17:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:17:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:18:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:18:00 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=130.303192, avg_loss=0.651516, seen=200, correct=121, accuracy=0.605000
2025-09-10 09:18:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:18:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:18:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:18:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2046MB allocated=2011MB
2025-09-10 09:18:02 (federatedscope.llm.llm_local.client:480) INFO: Client 34 Adapter 1 with val results: {'val_total': 200, 'val_loss': 130.30319213867188, 'val_avg_loss': 0.6515159606933594, 'val_seen': 200, 'val_correct': 121, 'val_acc': 0.605}
2025-09-10 09:18:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:18:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:18:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:18:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:18:04 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.539463, avg_loss=0.663487, seen=40, correct=22, accuracy=0.550000
2025-09-10 09:18:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:18:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:18:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:18:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2046MB allocated=2011MB
2025-09-10 09:18:05 (federatedscope.llm.llm_local.client:501) INFO: Client 34 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.53946304321289, 'test_avg_loss': 0.6634865760803222, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-10 09:18:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:18:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:18:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:18:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:18:09 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=132.322311, avg_loss=0.661612, seen=200, correct=117, accuracy=0.585000
2025-09-10 09:18:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:18:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:18:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:18:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2046MB allocated=2011MB
2025-09-10 09:18:10 (federatedscope.llm.llm_local.client:480) INFO: Client 34 Adapter 2 with val results: {'val_total': 200, 'val_loss': 132.3223114013672, 'val_avg_loss': 0.6616115570068359, 'val_seen': 200, 'val_correct': 117, 'val_acc': 0.585}
2025-09-10 09:18:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:18:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:18:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:18:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:18:12 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=22.447956, avg_loss=0.561199, seen=40, correct=28, accuracy=0.700000
2025-09-10 09:18:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:18:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:18:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:18:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2046MB allocated=2011MB
2025-09-10 09:18:13 (federatedscope.llm.llm_local.client:501) INFO: Client 34 Adapter 2 with test results: {'test_total': 40, 'test_loss': 22.447956085205078, 'test_avg_loss': 0.5611989021301269, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}
2025-09-10 09:18:13 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:18:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:18:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:18:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:18:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:18:18 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=128.196396, avg_loss=0.640982, seen=200, correct=124, accuracy=0.620000
2025-09-10 09:18:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:18:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:18:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:18:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2066MB allocated=2045MB
2025-09-10 09:18:20 (federatedscope.llm.llm_local.client:480) INFO: Client 35 Adapter 0 with val results: {'val_total': 200, 'val_loss': 128.19639587402344, 'val_avg_loss': 0.6409819793701171, 'val_seen': 200, 'val_correct': 124, 'val_acc': 0.62}
2025-09-10 09:18:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:18:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:18:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:18:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:18:22 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.239861, avg_loss=0.655997, seen=40, correct=28, accuracy=0.700000
2025-09-10 09:18:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:18:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:18:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:18:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2066MB allocated=2045MB
2025-09-10 09:18:23 (federatedscope.llm.llm_local.client:501) INFO: Client 35 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.23986053466797, 'test_avg_loss': 0.6559965133666992, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}
2025-09-10 09:18:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:18:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:18:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:18:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:18:28 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=130.742081, avg_loss=0.653710, seen=200, correct=128, accuracy=0.640000
2025-09-10 09:18:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:18:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:18:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:18:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2066MB allocated=2020MB
2025-09-10 09:18:30 (federatedscope.llm.llm_local.client:480) INFO: Client 35 Adapter 1 with val results: {'val_total': 200, 'val_loss': 130.74208068847656, 'val_avg_loss': 0.6537104034423828, 'val_seen': 200, 'val_correct': 128, 'val_acc': 0.64}
2025-09-10 09:18:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:18:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:18:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:18:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:18:31 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.106293, avg_loss=0.677657, seen=40, correct=24, accuracy=0.600000
2025-09-10 09:18:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:18:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:18:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:18:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2066MB allocated=2020MB
2025-09-10 09:18:33 (federatedscope.llm.llm_local.client:501) INFO: Client 35 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.106292724609375, 'test_avg_loss': 0.6776573181152343, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-10 09:18:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:18:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:18:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:18:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:18:37 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=139.006485, avg_loss=0.695032, seen=200, correct=114, accuracy=0.570000
2025-09-10 09:18:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:18:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:18:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:18:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2066MB allocated=2020MB
2025-09-10 09:18:39 (federatedscope.llm.llm_local.client:480) INFO: Client 35 Adapter 2 with val results: {'val_total': 200, 'val_loss': 139.00648498535156, 'val_avg_loss': 0.6950324249267578, 'val_seen': 200, 'val_correct': 114, 'val_acc': 0.57}
2025-09-10 09:18:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:18:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:18:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:18:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:18:41 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.873409, avg_loss=0.721835, seen=40, correct=20, accuracy=0.500000
2025-09-10 09:18:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:18:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:18:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:18:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2066MB allocated=2020MB
2025-09-10 09:18:42 (federatedscope.llm.llm_local.client:501) INFO: Client 35 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.873409271240234, 'test_avg_loss': 0.7218352317810058, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-10 09:18:42 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:18:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-09-10 09:18:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:18:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=27, num_train_batch_last_epoch=19, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:18:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-09-10 09:18:45 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=54, loss_sum=38.111687, avg_loss=0.705772, seen=54, correct=24, accuracy=0.444444
2025-09-10 09:18:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:18:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:18:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:18:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2066MB allocated=2053MB
2025-09-10 09:18:47 (federatedscope.llm.llm_local.client:480) INFO: Client 36 Adapter 0 with val results: {'val_total': 54, 'val_loss': 38.11168670654297, 'val_avg_loss': 0.705771976047092, 'val_seen': 54, 'val_correct': 24, 'val_acc': 0.4444444444444444}
2025-09-10 09:18:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:18:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:18:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:18:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:18:49 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.805990, avg_loss=0.620150, seen=40, correct=24, accuracy=0.600000
2025-09-10 09:18:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:18:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:18:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:18:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2066MB allocated=2053MB
2025-09-10 09:18:51 (federatedscope.llm.llm_local.client:501) INFO: Client 36 Adapter 0 with test results: {'test_total': 40, 'test_loss': 24.80599021911621, 'test_avg_loss': 0.6201497554779053, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-10 09:18:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-09-10 09:18:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:18:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=27, num_train_batch_last_epoch=19, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:18:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-09-10 09:18:53 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=54, loss_sum=36.394073, avg_loss=0.673964, seen=54, correct=33, accuracy=0.611111
2025-09-10 09:18:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:18:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:18:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:18:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2066MB allocated=2028MB
2025-09-10 09:18:55 (federatedscope.llm.llm_local.client:480) INFO: Client 36 Adapter 1 with val results: {'val_total': 54, 'val_loss': 36.394073486328125, 'val_avg_loss': 0.6739643238208912, 'val_seen': 54, 'val_correct': 33, 'val_acc': 0.6111111111111112}
2025-09-10 09:18:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:18:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:18:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:18:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:18:56 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.162994, avg_loss=0.654075, seen=40, correct=22, accuracy=0.550000
2025-09-10 09:18:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:18:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:18:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:18:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2066MB allocated=2028MB
2025-09-10 09:18:58 (federatedscope.llm.llm_local.client:501) INFO: Client 36 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.162994384765625, 'test_avg_loss': 0.6540748596191406, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-10 09:18:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-09-10 09:18:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:18:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=27, num_train_batch_last_epoch=19, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:19:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-09-10 09:19:00 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=54, loss_sum=39.563728, avg_loss=0.732662, seen=54, correct=30, accuracy=0.555556
2025-09-10 09:19:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:19:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:19:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:19:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2066MB allocated=2028MB
2025-09-10 09:19:01 (federatedscope.llm.llm_local.client:480) INFO: Client 36 Adapter 2 with val results: {'val_total': 54, 'val_loss': 39.56372833251953, 'val_avg_loss': 0.7326616357873987, 'val_seen': 54, 'val_correct': 30, 'val_acc': 0.5555555555555556}
2025-09-10 09:19:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:19:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:19:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:19:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:19:02 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.355438, avg_loss=0.633886, seen=40, correct=26, accuracy=0.650000
2025-09-10 09:19:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:19:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:19:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:19:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2066MB allocated=2028MB
2025-09-10 09:19:03 (federatedscope.llm.llm_local.client:501) INFO: Client 36 Adapter 2 with test results: {'test_total': 40, 'test_loss': 25.355438232421875, 'test_avg_loss': 0.6338859558105469, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-10 09:19:03 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:19:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:19:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:19:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:19:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:19:08 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=137.101715, avg_loss=0.685509, seen=200, correct=110, accuracy=0.550000
2025-09-10 09:19:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:19:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:19:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:19:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2066MB allocated=2062MB
2025-09-10 09:19:11 (federatedscope.llm.llm_local.client:480) INFO: Client 37 Adapter 0 with val results: {'val_total': 200, 'val_loss': 137.10171508789062, 'val_avg_loss': 0.6855085754394531, 'val_seen': 200, 'val_correct': 110, 'val_acc': 0.55}
2025-09-10 09:19:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:19:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:19:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:19:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:19:12 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.103912, avg_loss=0.652598, seen=40, correct=25, accuracy=0.625000
2025-09-10 09:19:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:19:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:19:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:19:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2066MB allocated=2062MB
2025-09-10 09:19:13 (federatedscope.llm.llm_local.client:501) INFO: Client 37 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.103912353515625, 'test_avg_loss': 0.6525978088378906, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-10 09:19:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:19:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:19:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:19:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:19:17 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=138.714798, avg_loss=0.693574, seen=200, correct=114, accuracy=0.570000
2025-09-10 09:19:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:19:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:19:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:19:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2066MB allocated=2036MB
2025-09-10 09:19:18 (federatedscope.llm.llm_local.client:480) INFO: Client 37 Adapter 1 with val results: {'val_total': 200, 'val_loss': 138.7147979736328, 'val_avg_loss': 0.6935739898681641, 'val_seen': 200, 'val_correct': 114, 'val_acc': 0.57}
2025-09-10 09:19:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:19:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:19:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:19:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:19:20 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.149616, avg_loss=0.703740, seen=40, correct=22, accuracy=0.550000
2025-09-10 09:19:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:19:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:19:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:19:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2066MB allocated=2036MB
2025-09-10 09:19:22 (federatedscope.llm.llm_local.client:501) INFO: Client 37 Adapter 1 with test results: {'test_total': 40, 'test_loss': 28.149616241455078, 'test_avg_loss': 0.703740406036377, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-10 09:19:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:19:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:19:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:19:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:19:26 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=139.332520, avg_loss=0.696663, seen=200, correct=107, accuracy=0.535000
2025-09-10 09:19:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:19:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:19:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:19:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2066MB allocated=2036MB
2025-09-10 09:19:29 (federatedscope.llm.llm_local.client:480) INFO: Client 37 Adapter 2 with val results: {'val_total': 200, 'val_loss': 139.33251953125, 'val_avg_loss': 0.69666259765625, 'val_seen': 200, 'val_correct': 107, 'val_acc': 0.535}
2025-09-10 09:19:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:19:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:19:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:19:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:19:32 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.748636, avg_loss=0.668716, seen=40, correct=23, accuracy=0.575000
2025-09-10 09:19:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:19:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:19:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:19:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2066MB allocated=2036MB
2025-09-10 09:19:33 (federatedscope.llm.llm_local.client:501) INFO: Client 37 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.74863624572754, 'test_avg_loss': 0.6687159061431884, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-10 09:19:33 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:19:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:19:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:19:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:19:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:19:38 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=131.267303, avg_loss=0.656337, seen=200, correct=127, accuracy=0.635000
2025-09-10 09:19:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:19:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:19:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:19:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2086MB allocated=2070MB
2025-09-10 09:19:39 (federatedscope.llm.llm_local.client:480) INFO: Client 38 Adapter 0 with val results: {'val_total': 200, 'val_loss': 131.26730346679688, 'val_avg_loss': 0.6563365173339843, 'val_seen': 200, 'val_correct': 127, 'val_acc': 0.635}
2025-09-10 09:19:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:19:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:19:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:19:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:19:40 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.570950, avg_loss=0.639274, seen=40, correct=25, accuracy=0.625000
2025-09-10 09:19:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:19:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:19:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:19:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2086MB allocated=2070MB
2025-09-10 09:19:42 (federatedscope.llm.llm_local.client:501) INFO: Client 38 Adapter 0 with test results: {'test_total': 40, 'test_loss': 25.57094955444336, 'test_avg_loss': 0.639273738861084, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-10 09:19:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:19:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:19:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:19:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:19:46 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=130.153564, avg_loss=0.650768, seen=200, correct=120, accuracy=0.600000
2025-09-10 09:19:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:19:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:19:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:19:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2086MB allocated=2045MB
2025-09-10 09:19:48 (federatedscope.llm.llm_local.client:480) INFO: Client 38 Adapter 1 with val results: {'val_total': 200, 'val_loss': 130.153564453125, 'val_avg_loss': 0.650767822265625, 'val_seen': 200, 'val_correct': 120, 'val_acc': 0.6}
2025-09-10 09:19:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:19:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:19:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:19:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:19:50 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.033464, avg_loss=0.625837, seen=40, correct=26, accuracy=0.650000
2025-09-10 09:19:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:19:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:19:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:19:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2086MB allocated=2045MB
2025-09-10 09:19:52 (federatedscope.llm.llm_local.client:501) INFO: Client 38 Adapter 1 with test results: {'test_total': 40, 'test_loss': 25.033464431762695, 'test_avg_loss': 0.6258366107940674, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-10 09:19:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:19:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:19:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:19:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:19:57 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=138.265472, avg_loss=0.691327, seen=200, correct=119, accuracy=0.595000
2025-09-10 09:19:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:19:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:19:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:19:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2086MB allocated=2045MB
2025-09-10 09:19:58 (federatedscope.llm.llm_local.client:480) INFO: Client 38 Adapter 2 with val results: {'val_total': 200, 'val_loss': 138.26547241210938, 'val_avg_loss': 0.6913273620605469, 'val_seen': 200, 'val_correct': 119, 'val_acc': 0.595}
2025-09-10 09:19:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:19:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:19:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:20:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:20:00 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.640217, avg_loss=0.691005, seen=40, correct=22, accuracy=0.550000
2025-09-10 09:20:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:20:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:20:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:20:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2086MB allocated=2045MB
2025-09-10 09:20:01 (federatedscope.llm.llm_local.client:501) INFO: Client 38 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.640216827392578, 'test_avg_loss': 0.6910054206848144, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-10 09:20:01 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:20:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-10 09:20:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:20:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=16, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:20:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-10 09:20:04 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=60.614811, avg_loss=0.730299, seen=83, correct=47, accuracy=0.566265
2025-09-10 09:20:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:20:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:20:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:20:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2086MB allocated=2078MB
2025-09-10 09:20:05 (federatedscope.llm.llm_local.client:480) INFO: Client 39 Adapter 0 with val results: {'val_total': 83, 'val_loss': 60.614810943603516, 'val_avg_loss': 0.7302989270313677, 'val_seen': 83, 'val_correct': 47, 'val_acc': 0.5662650602409639}
2025-09-10 09:20:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:20:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:20:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:20:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:20:06 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.685783, avg_loss=0.767145, seen=40, correct=20, accuracy=0.500000
2025-09-10 09:20:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:20:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:20:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:20:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2086MB allocated=2078MB
2025-09-10 09:20:08 (federatedscope.llm.llm_local.client:501) INFO: Client 39 Adapter 0 with test results: {'test_total': 40, 'test_loss': 30.68578338623047, 'test_avg_loss': 0.7671445846557617, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-10 09:20:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-10 09:20:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:20:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=16, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:20:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-10 09:20:11 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=58.259201, avg_loss=0.701918, seen=83, correct=43, accuracy=0.518072
2025-09-10 09:20:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:20:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:20:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:20:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2086MB allocated=2053MB
2025-09-10 09:20:13 (federatedscope.llm.llm_local.client:480) INFO: Client 39 Adapter 1 with val results: {'val_total': 83, 'val_loss': 58.25920104980469, 'val_avg_loss': 0.7019180849374059, 'val_seen': 83, 'val_correct': 43, 'val_acc': 0.5180722891566265}
2025-09-10 09:20:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:20:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:20:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:20:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:20:14 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.887308, avg_loss=0.747183, seen=40, correct=19, accuracy=0.475000
2025-09-10 09:20:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:20:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:20:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:20:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2086MB allocated=2053MB
2025-09-10 09:20:16 (federatedscope.llm.llm_local.client:501) INFO: Client 39 Adapter 1 with test results: {'test_total': 40, 'test_loss': 29.88730812072754, 'test_avg_loss': 0.7471827030181885, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-10 09:20:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-10 09:20:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:20:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=16, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:20:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-10 09:20:18 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=62.712906, avg_loss=0.755577, seen=83, correct=48, accuracy=0.578313
2025-09-10 09:20:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:20:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:20:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:20:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2086MB allocated=2053MB
2025-09-10 09:20:20 (federatedscope.llm.llm_local.client:480) INFO: Client 39 Adapter 2 with val results: {'val_total': 83, 'val_loss': 62.71290588378906, 'val_avg_loss': 0.7555771793227598, 'val_seen': 83, 'val_correct': 48, 'val_acc': 0.5783132530120482}
2025-09-10 09:20:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:20:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:20:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:20:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:20:22 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.475464, avg_loss=0.761887, seen=40, correct=20, accuracy=0.500000
2025-09-10 09:20:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:20:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:20:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:20:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2086MB allocated=2053MB
2025-09-10 09:20:23 (federatedscope.llm.llm_local.client:501) INFO: Client 39 Adapter 2 with test results: {'test_total': 40, 'test_loss': 30.4754638671875, 'test_avg_loss': 0.7618865966796875, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-10 09:20:23 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:20:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:20:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:20:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:20:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:20:28 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=131.472290, avg_loss=0.657361, seen=200, correct=118, accuracy=0.590000
2025-09-10 09:20:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:20:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:20:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:20:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2106MB allocated=2087MB
2025-09-10 09:20:29 (federatedscope.llm.llm_local.client:480) INFO: Client 40 Adapter 0 with val results: {'val_total': 200, 'val_loss': 131.4722900390625, 'val_avg_loss': 0.6573614501953124, 'val_seen': 200, 'val_correct': 118, 'val_acc': 0.59}
2025-09-10 09:20:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:20:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:20:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:20:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:20:31 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.381912, avg_loss=0.684548, seen=40, correct=25, accuracy=0.625000
2025-09-10 09:20:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:20:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:20:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:20:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2106MB allocated=2087MB
2025-09-10 09:20:32 (federatedscope.llm.llm_local.client:501) INFO: Client 40 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.381912231445312, 'test_avg_loss': 0.6845478057861328, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-10 09:20:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:20:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:20:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:20:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:20:36 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=131.372147, avg_loss=0.656861, seen=200, correct=114, accuracy=0.570000
2025-09-10 09:20:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:20:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:20:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:20:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2106MB allocated=2062MB
2025-09-10 09:20:37 (federatedscope.llm.llm_local.client:480) INFO: Client 40 Adapter 1 with val results: {'val_total': 200, 'val_loss': 131.3721466064453, 'val_avg_loss': 0.6568607330322266, 'val_seen': 200, 'val_correct': 114, 'val_acc': 0.57}
2025-09-10 09:20:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:20:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:20:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:20:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:20:39 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.905125, avg_loss=0.672628, seen=40, correct=25, accuracy=0.625000
2025-09-10 09:20:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:20:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:20:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:20:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2106MB allocated=2062MB
2025-09-10 09:20:40 (federatedscope.llm.llm_local.client:501) INFO: Client 40 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.90512466430664, 'test_avg_loss': 0.672628116607666, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-10 09:20:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:20:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:20:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:20:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:20:44 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=129.173080, avg_loss=0.645865, seen=200, correct=124, accuracy=0.620000
2025-09-10 09:20:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:20:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:20:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:20:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2106MB allocated=2062MB
2025-09-10 09:20:45 (federatedscope.llm.llm_local.client:480) INFO: Client 40 Adapter 2 with val results: {'val_total': 200, 'val_loss': 129.17308044433594, 'val_avg_loss': 0.6458654022216797, 'val_seen': 200, 'val_correct': 124, 'val_acc': 0.62}
2025-09-10 09:20:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:20:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:20:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:20:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:20:47 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.898457, avg_loss=0.672461, seen=40, correct=26, accuracy=0.650000
2025-09-10 09:20:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:20:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:20:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:20:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2106MB allocated=2062MB
2025-09-10 09:20:48 (federatedscope.llm.llm_local.client:501) INFO: Client 40 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.898456573486328, 'test_avg_loss': 0.6724614143371582, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-10 09:20:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:20:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-09-10 09:20:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:20:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=60, num_train_batch_last_epoch=40, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:20:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-09-10 09:20:52 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=119, loss_sum=83.571861, avg_loss=0.702285, seen=119, correct=74, accuracy=0.621849
2025-09-10 09:20:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:20:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:20:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:20:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2106MB allocated=2095MB
2025-09-10 09:20:55 (federatedscope.llm.llm_local.client:480) INFO: Client 41 Adapter 0 with val results: {'val_total': 119, 'val_loss': 83.57186126708984, 'val_avg_loss': 0.7022845484629399, 'val_seen': 119, 'val_correct': 74, 'val_acc': 0.6218487394957983}
2025-09-10 09:20:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:20:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:20:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:20:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:20:57 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.946613, avg_loss=0.698665, seen=40, correct=24, accuracy=0.600000
2025-09-10 09:20:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:20:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:20:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:20:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2106MB allocated=2095MB
2025-09-10 09:20:58 (federatedscope.llm.llm_local.client:501) INFO: Client 41 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.946613311767578, 'test_avg_loss': 0.6986653327941894, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-10 09:20:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-09-10 09:20:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:20:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=60, num_train_batch_last_epoch=40, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:21:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-09-10 09:21:01 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=119, loss_sum=83.242493, avg_loss=0.699517, seen=119, correct=69, accuracy=0.579832
2025-09-10 09:21:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:21:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:21:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:21:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2106MB allocated=2070MB
2025-09-10 09:21:03 (federatedscope.llm.llm_local.client:480) INFO: Client 41 Adapter 1 with val results: {'val_total': 119, 'val_loss': 83.24249267578125, 'val_avg_loss': 0.6995167451746324, 'val_seen': 119, 'val_correct': 69, 'val_acc': 0.5798319327731093}
2025-09-10 09:21:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:21:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:21:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:21:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:21:05 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.807516, avg_loss=0.695188, seen=40, correct=24, accuracy=0.600000
2025-09-10 09:21:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:21:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:21:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:21:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2106MB allocated=2070MB
2025-09-10 09:21:06 (federatedscope.llm.llm_local.client:501) INFO: Client 41 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.80751609802246, 'test_avg_loss': 0.6951879024505615, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-10 09:21:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-09-10 09:21:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:21:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=60, num_train_batch_last_epoch=40, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:21:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-09-10 09:21:09 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=119, loss_sum=86.489250, avg_loss=0.726800, seen=119, correct=52, accuracy=0.436975
2025-09-10 09:21:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:21:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:21:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:21:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2106MB allocated=2070MB
2025-09-10 09:21:11 (federatedscope.llm.llm_local.client:480) INFO: Client 41 Adapter 2 with val results: {'val_total': 119, 'val_loss': 86.48925018310547, 'val_avg_loss': 0.7268004217067686, 'val_seen': 119, 'val_correct': 52, 'val_acc': 0.4369747899159664}
2025-09-10 09:21:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:21:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:21:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:21:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:21:12 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.278181, avg_loss=0.656955, seen=40, correct=25, accuracy=0.625000
2025-09-10 09:21:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:21:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:21:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:21:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2106MB allocated=2070MB
2025-09-10 09:21:14 (federatedscope.llm.llm_local.client:501) INFO: Client 41 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.278181076049805, 'test_avg_loss': 0.6569545269012451, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-10 09:21:15 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:21:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:21:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:21:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:21:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:21:20 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=129.265320, avg_loss=0.646327, seen=200, correct=123, accuracy=0.615000
2025-09-10 09:21:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:21:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:21:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:21:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2126MB allocated=2104MB
2025-09-10 09:21:21 (federatedscope.llm.llm_local.client:480) INFO: Client 42 Adapter 0 with val results: {'val_total': 200, 'val_loss': 129.26531982421875, 'val_avg_loss': 0.6463265991210938, 'val_seen': 200, 'val_correct': 123, 'val_acc': 0.615}
2025-09-10 09:21:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:21:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:21:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:21:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:21:23 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.370506, avg_loss=0.684263, seen=40, correct=26, accuracy=0.650000
2025-09-10 09:21:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:21:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:21:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:21:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2126MB allocated=2104MB
2025-09-10 09:21:24 (federatedscope.llm.llm_local.client:501) INFO: Client 42 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.370506286621094, 'test_avg_loss': 0.6842626571655274, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-10 09:21:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:21:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:21:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:21:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:21:28 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=133.959488, avg_loss=0.669797, seen=200, correct=123, accuracy=0.615000
2025-09-10 09:21:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:21:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:21:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:21:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2126MB allocated=2078MB
2025-09-10 09:21:31 (federatedscope.llm.llm_local.client:480) INFO: Client 42 Adapter 1 with val results: {'val_total': 200, 'val_loss': 133.95948791503906, 'val_avg_loss': 0.6697974395751953, 'val_seen': 200, 'val_correct': 123, 'val_acc': 0.615}
2025-09-10 09:21:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:21:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:21:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:21:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:21:32 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.726759, avg_loss=0.668169, seen=40, correct=23, accuracy=0.575000
2025-09-10 09:21:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:21:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:21:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:21:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2126MB allocated=2078MB
2025-09-10 09:21:34 (federatedscope.llm.llm_local.client:501) INFO: Client 42 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.72675895690918, 'test_avg_loss': 0.6681689739227294, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-10 09:21:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:21:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:21:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:21:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:21:38 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=131.873337, avg_loss=0.659367, seen=200, correct=113, accuracy=0.565000
2025-09-10 09:21:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:21:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:21:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:21:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2126MB allocated=2078MB
2025-09-10 09:21:40 (federatedscope.llm.llm_local.client:480) INFO: Client 42 Adapter 2 with val results: {'val_total': 200, 'val_loss': 131.8733367919922, 'val_avg_loss': 0.6593666839599609, 'val_seen': 200, 'val_correct': 113, 'val_acc': 0.565}
2025-09-10 09:21:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:21:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:21:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:21:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:21:41 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.525434, avg_loss=0.688136, seen=40, correct=22, accuracy=0.550000
2025-09-10 09:21:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:21:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:21:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:21:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2126MB allocated=2078MB
2025-09-10 09:21:43 (federatedscope.llm.llm_local.client:501) INFO: Client 42 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.525434494018555, 'test_avg_loss': 0.6881358623504639, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-10 09:21:43 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:21:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-09-10 09:21:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:21:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=45, num_train_batch_last_epoch=10, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:21:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-09-10 09:21:46 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=89, loss_sum=61.490013, avg_loss=0.690899, seen=89, correct=48, accuracy=0.539326
2025-09-10 09:21:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:21:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:21:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:21:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2126MB allocated=2112MB
2025-09-10 09:21:48 (federatedscope.llm.llm_local.client:480) INFO: Client 43 Adapter 0 with val results: {'val_total': 89, 'val_loss': 61.490013122558594, 'val_avg_loss': 0.690899023848973, 'val_seen': 89, 'val_correct': 48, 'val_acc': 0.5393258426966292}
2025-09-10 09:21:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:21:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:21:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:21:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:21:49 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.330305, avg_loss=0.633258, seen=40, correct=27, accuracy=0.675000
2025-09-10 09:21:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:21:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:21:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:21:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2126MB allocated=2112MB
2025-09-10 09:21:51 (federatedscope.llm.llm_local.client:501) INFO: Client 43 Adapter 0 with test results: {'test_total': 40, 'test_loss': 25.330305099487305, 'test_avg_loss': 0.6332576274871826, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-10 09:21:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-09-10 09:21:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:21:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=45, num_train_batch_last_epoch=10, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:21:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-09-10 09:21:54 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=89, loss_sum=58.462402, avg_loss=0.656881, seen=89, correct=56, accuracy=0.629213
2025-09-10 09:21:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:21:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:21:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:21:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2126MB allocated=2087MB
2025-09-10 09:21:56 (federatedscope.llm.llm_local.client:480) INFO: Client 43 Adapter 1 with val results: {'val_total': 89, 'val_loss': 58.46240234375, 'val_avg_loss': 0.6568809252106742, 'val_seen': 89, 'val_correct': 56, 'val_acc': 0.6292134831460674}
2025-09-10 09:21:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:21:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:21:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:21:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:21:57 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.518379, avg_loss=0.662959, seen=40, correct=25, accuracy=0.625000
2025-09-10 09:21:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:21:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:21:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:21:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2126MB allocated=2087MB
2025-09-10 09:21:59 (federatedscope.llm.llm_local.client:501) INFO: Client 43 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.51837921142578, 'test_avg_loss': 0.6629594802856446, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-10 09:21:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-09-10 09:21:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:21:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=45, num_train_batch_last_epoch=10, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:22:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-09-10 09:22:02 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=89, loss_sum=62.933968, avg_loss=0.707123, seen=89, correct=44, accuracy=0.494382
2025-09-10 09:22:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:22:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:22:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:22:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2126MB allocated=2087MB
2025-09-10 09:22:03 (federatedscope.llm.llm_local.client:480) INFO: Client 43 Adapter 2 with val results: {'val_total': 89, 'val_loss': 62.93396759033203, 'val_avg_loss': 0.7071232313520452, 'val_seen': 89, 'val_correct': 44, 'val_acc': 0.4943820224719101}
2025-09-10 09:22:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:22:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:22:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:22:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:22:04 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.759178, avg_loss=0.643979, seen=40, correct=29, accuracy=0.725000
2025-09-10 09:22:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:22:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:22:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:22:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2126MB allocated=2087MB
2025-09-10 09:22:05 (federatedscope.llm.llm_local.client:501) INFO: Client 43 Adapter 2 with test results: {'test_total': 40, 'test_loss': 25.759178161621094, 'test_avg_loss': 0.6439794540405274, 'test_seen': 40, 'test_correct': 29, 'test_acc': 0.725}
2025-09-10 09:22:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:22:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:22:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:22:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:22:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:22:10 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=141.186523, avg_loss=0.705933, seen=200, correct=110, accuracy=0.550000
2025-09-10 09:22:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:22:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:22:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:22:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2126MB allocated=2120MB
2025-09-10 09:22:11 (federatedscope.llm.llm_local.client:480) INFO: Client 44 Adapter 0 with val results: {'val_total': 200, 'val_loss': 141.1865234375, 'val_avg_loss': 0.7059326171875, 'val_seen': 200, 'val_correct': 110, 'val_acc': 0.55}
2025-09-10 09:22:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:22:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:22:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:22:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:22:13 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.158291, avg_loss=0.603957, seen=40, correct=25, accuracy=0.625000
2025-09-10 09:22:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:22:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:22:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:22:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2126MB allocated=2120MB
2025-09-10 09:22:15 (federatedscope.llm.llm_local.client:501) INFO: Client 44 Adapter 0 with test results: {'test_total': 40, 'test_loss': 24.15829086303711, 'test_avg_loss': 0.6039572715759277, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-10 09:22:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:22:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:22:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:22:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:22:20 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=143.400986, avg_loss=0.717005, seen=200, correct=104, accuracy=0.520000
2025-09-10 09:22:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:22:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:22:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:22:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2126MB allocated=2095MB
2025-09-10 09:22:22 (federatedscope.llm.llm_local.client:480) INFO: Client 44 Adapter 1 with val results: {'val_total': 200, 'val_loss': 143.40098571777344, 'val_avg_loss': 0.7170049285888672, 'val_seen': 200, 'val_correct': 104, 'val_acc': 0.52}
2025-09-10 09:22:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:22:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:22:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:22:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:22:24 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.214508, avg_loss=0.655363, seen=40, correct=26, accuracy=0.650000
2025-09-10 09:22:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:22:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:22:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:22:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2126MB allocated=2095MB
2025-09-10 09:22:25 (federatedscope.llm.llm_local.client:501) INFO: Client 44 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.214508056640625, 'test_avg_loss': 0.6553627014160156, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-10 09:22:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:22:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:22:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:22:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:22:30 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=145.811554, avg_loss=0.729058, seen=200, correct=104, accuracy=0.520000
2025-09-10 09:22:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:22:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:22:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:22:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2126MB allocated=2095MB
2025-09-10 09:22:32 (federatedscope.llm.llm_local.client:480) INFO: Client 44 Adapter 2 with val results: {'val_total': 200, 'val_loss': 145.81155395507812, 'val_avg_loss': 0.7290577697753906, 'val_seen': 200, 'val_correct': 104, 'val_acc': 0.52}
2025-09-10 09:22:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:22:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:22:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:22:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:22:33 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.871603, avg_loss=0.746790, seen=40, correct=19, accuracy=0.475000
2025-09-10 09:22:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:22:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:22:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:22:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2126MB allocated=2095MB
2025-09-10 09:22:35 (federatedscope.llm.llm_local.client:501) INFO: Client 44 Adapter 2 with test results: {'test_total': 40, 'test_loss': 29.87160301208496, 'test_avg_loss': 0.746790075302124, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-10 09:22:35 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:22:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-09-10 09:22:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:22:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=50, num_train_batch_last_epoch=50, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:22:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-09-10 09:22:38 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=100, loss_sum=65.396667, avg_loss=0.653967, seen=100, correct=60, accuracy=0.600000
2025-09-10 09:22:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:22:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:22:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:22:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2146MB allocated=2129MB
2025-09-10 09:22:39 (federatedscope.llm.llm_local.client:480) INFO: Client 45 Adapter 0 with val results: {'val_total': 100, 'val_loss': 65.39666748046875, 'val_avg_loss': 0.6539666748046875, 'val_seen': 100, 'val_correct': 60, 'val_acc': 0.6}
2025-09-10 09:22:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:22:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:22:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:22:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:22:41 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.673840, avg_loss=0.716846, seen=40, correct=24, accuracy=0.600000
2025-09-10 09:22:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:22:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:22:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:22:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2146MB allocated=2129MB
2025-09-10 09:22:42 (federatedscope.llm.llm_local.client:501) INFO: Client 45 Adapter 0 with test results: {'test_total': 40, 'test_loss': 28.673839569091797, 'test_avg_loss': 0.7168459892272949, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-10 09:22:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-09-10 09:22:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:22:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=50, num_train_batch_last_epoch=50, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:22:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-09-10 09:22:45 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=100, loss_sum=65.359192, avg_loss=0.653592, seen=100, correct=62, accuracy=0.620000
2025-09-10 09:22:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:22:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:22:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:22:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2146MB allocated=2104MB
2025-09-10 09:22:46 (federatedscope.llm.llm_local.client:480) INFO: Client 45 Adapter 1 with val results: {'val_total': 100, 'val_loss': 65.35919189453125, 'val_avg_loss': 0.6535919189453125, 'val_seen': 100, 'val_correct': 62, 'val_acc': 0.62}
2025-09-10 09:22:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:22:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:22:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:22:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:22:47 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.093536, avg_loss=0.702338, seen=40, correct=22, accuracy=0.550000
2025-09-10 09:22:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:22:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:22:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:22:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2146MB allocated=2104MB
2025-09-10 09:22:48 (federatedscope.llm.llm_local.client:501) INFO: Client 45 Adapter 1 with test results: {'test_total': 40, 'test_loss': 28.093536376953125, 'test_avg_loss': 0.7023384094238281, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-10 09:22:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-09-10 09:22:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:22:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=50, num_train_batch_last_epoch=50, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:22:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-09-10 09:22:51 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=100, loss_sum=72.697136, avg_loss=0.726971, seen=100, correct=46, accuracy=0.460000
2025-09-10 09:22:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:22:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:22:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:22:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2146MB allocated=2104MB
2025-09-10 09:22:53 (federatedscope.llm.llm_local.client:480) INFO: Client 45 Adapter 2 with val results: {'val_total': 100, 'val_loss': 72.69713592529297, 'val_avg_loss': 0.7269713592529297, 'val_seen': 100, 'val_correct': 46, 'val_acc': 0.46}
2025-09-10 09:22:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:22:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:22:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:22:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:22:54 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.997381, avg_loss=0.699935, seen=40, correct=22, accuracy=0.550000
2025-09-10 09:22:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:22:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:22:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:22:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2146MB allocated=2104MB
2025-09-10 09:22:55 (federatedscope.llm.llm_local.client:501) INFO: Client 45 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.99738121032715, 'test_avg_loss': 0.6999345302581788, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-10 09:22:55 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:22:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-10 09:22:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:22:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=45, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:22:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-10 09:22:59 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=75.736008, avg_loss=0.688509, seen=110, correct=64, accuracy=0.581818
2025-09-10 09:22:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:22:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:22:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:23:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2146MB allocated=2137MB
2025-09-10 09:23:00 (federatedscope.llm.llm_local.client:480) INFO: Client 46 Adapter 0 with val results: {'val_total': 110, 'val_loss': 75.73600769042969, 'val_avg_loss': 0.688509160822088, 'val_seen': 110, 'val_correct': 64, 'val_acc': 0.5818181818181818}
2025-09-10 09:23:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:23:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:23:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:23:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:23:03 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.670368, avg_loss=0.616759, seen=40, correct=26, accuracy=0.650000
2025-09-10 09:23:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:23:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:23:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:23:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2146MB allocated=2137MB
2025-09-10 09:23:04 (federatedscope.llm.llm_local.client:501) INFO: Client 46 Adapter 0 with test results: {'test_total': 40, 'test_loss': 24.670368194580078, 'test_avg_loss': 0.616759204864502, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-10 09:23:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-10 09:23:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:23:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=45, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:23:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-10 09:23:07 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=75.264343, avg_loss=0.684221, seen=110, correct=65, accuracy=0.590909
2025-09-10 09:23:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:23:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:23:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:23:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2146MB allocated=2112MB
2025-09-10 09:23:09 (federatedscope.llm.llm_local.client:480) INFO: Client 46 Adapter 1 with val results: {'val_total': 110, 'val_loss': 75.26434326171875, 'val_avg_loss': 0.6842213023792614, 'val_seen': 110, 'val_correct': 65, 'val_acc': 0.5909090909090909}
2025-09-10 09:23:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:23:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:23:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:23:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:23:10 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.523226, avg_loss=0.638081, seen=40, correct=27, accuracy=0.675000
2025-09-10 09:23:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:23:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:23:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:23:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2146MB allocated=2112MB
2025-09-10 09:23:11 (federatedscope.llm.llm_local.client:501) INFO: Client 46 Adapter 1 with test results: {'test_total': 40, 'test_loss': 25.523225784301758, 'test_avg_loss': 0.638080644607544, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-10 09:23:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-10 09:23:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:23:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=45, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:23:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-10 09:23:14 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=76.898178, avg_loss=0.699074, seen=110, correct=64, accuracy=0.581818
2025-09-10 09:23:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:23:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:23:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:23:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2146MB allocated=2112MB
2025-09-10 09:23:15 (federatedscope.llm.llm_local.client:480) INFO: Client 46 Adapter 2 with val results: {'val_total': 110, 'val_loss': 76.89817810058594, 'val_avg_loss': 0.699074346368963, 'val_seen': 110, 'val_correct': 64, 'val_acc': 0.5818181818181818}
2025-09-10 09:23:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:23:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:23:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:23:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:23:17 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.686945, avg_loss=0.667174, seen=40, correct=21, accuracy=0.525000
2025-09-10 09:23:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:23:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:23:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:23:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2146MB allocated=2112MB
2025-09-10 09:23:18 (federatedscope.llm.llm_local.client:501) INFO: Client 46 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.68694496154785, 'test_avg_loss': 0.6671736240386963, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-10 09:23:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:23:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-09-10 09:23:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:23:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=74, num_train_batch_last_epoch=26, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:23:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-10 09:23:22 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=147, loss_sum=107.303368, avg_loss=0.729955, seen=147, correct=75, accuracy=0.510204
2025-09-10 09:23:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:23:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:23:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:23:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2166MB allocated=2145MB
2025-09-10 09:23:24 (federatedscope.llm.llm_local.client:480) INFO: Client 47 Adapter 0 with val results: {'val_total': 147, 'val_loss': 107.3033676147461, 'val_avg_loss': 0.7299548817329666, 'val_seen': 147, 'val_correct': 75, 'val_acc': 0.5102040816326531}
2025-09-10 09:23:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:23:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:23:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:23:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:23:26 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.943657, avg_loss=0.648591, seen=40, correct=20, accuracy=0.500000
2025-09-10 09:23:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:23:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:23:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:23:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2166MB allocated=2145MB
2025-09-10 09:23:28 (federatedscope.llm.llm_local.client:501) INFO: Client 47 Adapter 0 with test results: {'test_total': 40, 'test_loss': 25.94365692138672, 'test_avg_loss': 0.648591423034668, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-10 09:23:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-09-10 09:23:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:23:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=74, num_train_batch_last_epoch=26, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:23:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-10 09:23:31 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=147, loss_sum=104.040764, avg_loss=0.707760, seen=147, correct=76, accuracy=0.517007
2025-09-10 09:23:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:23:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:23:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:23:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2166MB allocated=2120MB
2025-09-10 09:23:33 (federatedscope.llm.llm_local.client:480) INFO: Client 47 Adapter 1 with val results: {'val_total': 147, 'val_loss': 104.04076385498047, 'val_avg_loss': 0.7077602983332004, 'val_seen': 147, 'val_correct': 76, 'val_acc': 0.5170068027210885}
2025-09-10 09:23:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:23:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:23:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:23:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:23:35 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.023048, avg_loss=0.625576, seen=40, correct=26, accuracy=0.650000
2025-09-10 09:23:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:23:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:23:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:23:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2166MB allocated=2120MB
2025-09-10 09:23:37 (federatedscope.llm.llm_local.client:501) INFO: Client 47 Adapter 1 with test results: {'test_total': 40, 'test_loss': 25.023048400878906, 'test_avg_loss': 0.6255762100219726, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-10 09:23:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-09-10 09:23:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:23:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=74, num_train_batch_last_epoch=26, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:23:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-10 09:23:40 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=147, loss_sum=106.123245, avg_loss=0.721927, seen=147, correct=83, accuracy=0.564626
2025-09-10 09:23:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:23:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:23:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:23:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2166MB allocated=2120MB
2025-09-10 09:23:42 (federatedscope.llm.llm_local.client:480) INFO: Client 47 Adapter 2 with val results: {'val_total': 147, 'val_loss': 106.12324523925781, 'val_avg_loss': 0.7219268383622981, 'val_seen': 147, 'val_correct': 83, 'val_acc': 0.564625850340136}
2025-09-10 09:23:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:23:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:23:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:23:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:23:43 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.622120, avg_loss=0.640553, seen=40, correct=24, accuracy=0.600000
2025-09-10 09:23:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:23:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:23:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:23:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2166MB allocated=2120MB
2025-09-10 09:23:45 (federatedscope.llm.llm_local.client:501) INFO: Client 47 Adapter 2 with test results: {'test_total': 40, 'test_loss': 25.622119903564453, 'test_avg_loss': 0.6405529975891113, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-10 09:23:45 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:23:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-09-10 09:23:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:23:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=23, num_train_batch_last_epoch=8, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:23:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-09-10 09:23:47 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=46, loss_sum=34.939640, avg_loss=0.759557, seen=46, correct=21, accuracy=0.456522
2025-09-10 09:23:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:23:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:23:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:23:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2166MB allocated=2154MB
2025-09-10 09:23:48 (federatedscope.llm.llm_local.client:480) INFO: Client 48 Adapter 0 with val results: {'val_total': 46, 'val_loss': 34.939640045166016, 'val_avg_loss': 0.7595573922862178, 'val_seen': 46, 'val_correct': 21, 'val_acc': 0.45652173913043476}
2025-09-10 09:23:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:23:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:23:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:23:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:23:50 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.882568, avg_loss=0.772064, seen=40, correct=20, accuracy=0.500000
2025-09-10 09:23:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:23:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:23:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:23:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2166MB allocated=2154MB
2025-09-10 09:23:52 (federatedscope.llm.llm_local.client:501) INFO: Client 48 Adapter 0 with test results: {'test_total': 40, 'test_loss': 30.882568359375, 'test_avg_loss': 0.772064208984375, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-10 09:23:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-09-10 09:23:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:23:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=23, num_train_batch_last_epoch=8, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:23:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-09-10 09:23:54 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=46, loss_sum=31.845770, avg_loss=0.692299, seen=46, correct=26, accuracy=0.565217
2025-09-10 09:23:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:23:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:23:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:23:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2166MB allocated=2129MB
2025-09-10 09:23:56 (federatedscope.llm.llm_local.client:480) INFO: Client 48 Adapter 1 with val results: {'val_total': 46, 'val_loss': 31.84576988220215, 'val_avg_loss': 0.692299345265264, 'val_seen': 46, 'val_correct': 26, 'val_acc': 0.5652173913043478}
2025-09-10 09:23:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:23:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:23:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:23:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:23:58 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.690863, avg_loss=0.742272, seen=40, correct=18, accuracy=0.450000
2025-09-10 09:23:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:23:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:23:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:23:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2166MB allocated=2129MB
2025-09-10 09:23:59 (federatedscope.llm.llm_local.client:501) INFO: Client 48 Adapter 1 with test results: {'test_total': 40, 'test_loss': 29.69086265563965, 'test_avg_loss': 0.7422715663909912, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-10 09:24:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-09-10 09:24:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:24:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=23, num_train_batch_last_epoch=8, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:24:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-09-10 09:24:01 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=46, loss_sum=33.558575, avg_loss=0.729534, seen=46, correct=18, accuracy=0.391304
2025-09-10 09:24:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:24:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:24:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:24:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2166MB allocated=2129MB
2025-09-10 09:24:03 (federatedscope.llm.llm_local.client:480) INFO: Client 48 Adapter 2 with val results: {'val_total': 46, 'val_loss': 33.55857467651367, 'val_avg_loss': 0.7295342320981233, 'val_seen': 46, 'val_correct': 18, 'val_acc': 0.391304347826087}
2025-09-10 09:24:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:24:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:24:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:24:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:24:05 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.217216, avg_loss=0.655430, seen=40, correct=28, accuracy=0.700000
2025-09-10 09:24:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:24:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:24:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:24:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2166MB allocated=2129MB
2025-09-10 09:24:06 (federatedscope.llm.llm_local.client:501) INFO: Client 48 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.21721649169922, 'test_avg_loss': 0.6554304122924804, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}
2025-09-10 09:24:06 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:24:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-09-10 09:24:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:24:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=66, num_train_batch_last_epoch=34, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:24:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-09-10 09:24:10 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=132, loss_sum=85.844482, avg_loss=0.650337, seen=132, correct=85, accuracy=0.643939
2025-09-10 09:24:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:24:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:24:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:24:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2166MB allocated=2162MB
2025-09-10 09:24:11 (federatedscope.llm.llm_local.client:480) INFO: Client 49 Adapter 0 with val results: {'val_total': 132, 'val_loss': 85.844482421875, 'val_avg_loss': 0.6503369880445076, 'val_seen': 132, 'val_correct': 85, 'val_acc': 0.6439393939393939}
2025-09-10 09:24:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:24:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:24:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:24:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:24:14 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=32.986794, avg_loss=0.824670, seen=40, correct=19, accuracy=0.475000
2025-09-10 09:24:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:24:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:24:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:24:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2166MB allocated=2162MB
2025-09-10 09:24:16 (federatedscope.llm.llm_local.client:501) INFO: Client 49 Adapter 0 with test results: {'test_total': 40, 'test_loss': 32.986793518066406, 'test_avg_loss': 0.8246698379516602, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-10 09:24:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-09-10 09:24:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:24:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=66, num_train_batch_last_epoch=34, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:24:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-09-10 09:24:20 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=132, loss_sum=86.918503, avg_loss=0.658474, seen=132, correct=87, accuracy=0.659091
2025-09-10 09:24:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:24:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:24:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:24:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2166MB allocated=2137MB
2025-09-10 09:24:21 (federatedscope.llm.llm_local.client:480) INFO: Client 49 Adapter 1 with val results: {'val_total': 132, 'val_loss': 86.91850280761719, 'val_avg_loss': 0.658473506118312, 'val_seen': 132, 'val_correct': 87, 'val_acc': 0.6590909090909091}
2025-09-10 09:24:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:24:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:24:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:24:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:24:22 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.447533, avg_loss=0.761188, seen=40, correct=17, accuracy=0.425000
2025-09-10 09:24:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:24:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:24:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:24:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2166MB allocated=2137MB
2025-09-10 09:24:25 (federatedscope.llm.llm_local.client:501) INFO: Client 49 Adapter 1 with test results: {'test_total': 40, 'test_loss': 30.447532653808594, 'test_avg_loss': 0.7611883163452149, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-09-10 09:24:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-09-10 09:24:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:24:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=66, num_train_batch_last_epoch=34, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:24:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-09-10 09:24:29 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=132, loss_sum=89.376282, avg_loss=0.677093, seen=132, correct=79, accuracy=0.598485
2025-09-10 09:24:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:24:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:24:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:24:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2166MB allocated=2137MB
2025-09-10 09:24:30 (federatedscope.llm.llm_local.client:480) INFO: Client 49 Adapter 2 with val results: {'val_total': 132, 'val_loss': 89.37628173828125, 'val_avg_loss': 0.6770930434718276, 'val_seen': 132, 'val_correct': 79, 'val_acc': 0.5984848484848485}
2025-09-10 09:24:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:24:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:24:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:24:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:24:32 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.742786, avg_loss=0.743570, seen=40, correct=20, accuracy=0.500000
2025-09-10 09:24:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:24:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:24:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:24:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2166MB allocated=2137MB
2025-09-10 09:24:33 (federatedscope.llm.llm_local.client:501) INFO: Client 49 Adapter 2 with test results: {'test_total': 40, 'test_loss': 29.742786407470703, 'test_avg_loss': 0.7435696601867676, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-10 09:24:33 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:24:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-09-10 09:24:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:24:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=33, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:24:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 09:24:38 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=133, loss_sum=90.515244, avg_loss=0.680566, seen=133, correct=73, accuracy=0.548872
2025-09-10 09:24:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:24:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:24:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:24:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2186MB allocated=2171MB
2025-09-10 09:24:40 (federatedscope.llm.llm_local.client:480) INFO: Client 50 Adapter 0 with val results: {'val_total': 133, 'val_loss': 90.51524353027344, 'val_avg_loss': 0.6805657408291236, 'val_seen': 133, 'val_correct': 73, 'val_acc': 0.5488721804511278}
2025-09-10 09:24:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:24:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:24:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:24:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:24:41 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.987129, avg_loss=0.774678, seen=40, correct=17, accuracy=0.425000
2025-09-10 09:24:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:24:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:24:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:24:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2186MB allocated=2171MB
2025-09-10 09:24:43 (federatedscope.llm.llm_local.client:501) INFO: Client 50 Adapter 0 with test results: {'test_total': 40, 'test_loss': 30.98712921142578, 'test_avg_loss': 0.7746782302856445, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-09-10 09:24:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-09-10 09:24:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:24:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=33, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:24:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 09:24:46 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=133, loss_sum=91.677437, avg_loss=0.689304, seen=133, correct=79, accuracy=0.593985
2025-09-10 09:24:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:24:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:24:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:24:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2186MB allocated=2145MB
2025-09-10 09:24:47 (federatedscope.llm.llm_local.client:480) INFO: Client 50 Adapter 1 with val results: {'val_total': 133, 'val_loss': 91.67743682861328, 'val_avg_loss': 0.689304036305363, 'val_seen': 133, 'val_correct': 79, 'val_acc': 0.5939849624060151}
2025-09-10 09:24:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:24:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:24:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:24:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:24:49 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.170019, avg_loss=0.729250, seen=40, correct=22, accuracy=0.550000
2025-09-10 09:24:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:24:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:24:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:24:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2186MB allocated=2145MB
2025-09-10 09:24:50 (federatedscope.llm.llm_local.client:501) INFO: Client 50 Adapter 1 with test results: {'test_total': 40, 'test_loss': 29.170019149780273, 'test_avg_loss': 0.7292504787445069, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-10 09:24:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-09-10 09:24:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:24:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=33, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:24:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 09:24:53 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=133, loss_sum=92.026680, avg_loss=0.691930, seen=133, correct=72, accuracy=0.541353
2025-09-10 09:24:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:24:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:24:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:24:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2186MB allocated=2145MB
2025-09-10 09:24:55 (federatedscope.llm.llm_local.client:480) INFO: Client 50 Adapter 2 with val results: {'val_total': 133, 'val_loss': 92.02667999267578, 'val_avg_loss': 0.6919299247569608, 'val_seen': 133, 'val_correct': 72, 'val_acc': 0.5413533834586466}
2025-09-10 09:24:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:24:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:24:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:24:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:24:58 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.597076, avg_loss=0.714927, seen=40, correct=21, accuracy=0.525000
2025-09-10 09:24:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:24:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:24:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:25:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2186MB allocated=2145MB
2025-09-10 09:25:00 (federatedscope.llm.llm_local.client:501) INFO: Client 50 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.597076416015625, 'test_avg_loss': 0.7149269104003906, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-10 09:25:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:25:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-10 09:25:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:25:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=16, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:25:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-10 09:25:02 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=59.153091, avg_loss=0.712688, seen=83, correct=47, accuracy=0.566265
2025-09-10 09:25:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:25:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:25:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:25:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2186MB allocated=2179MB
2025-09-10 09:25:03 (federatedscope.llm.llm_local.client:480) INFO: Client 51 Adapter 0 with val results: {'val_total': 83, 'val_loss': 59.15309143066406, 'val_avg_loss': 0.7126878485622177, 'val_seen': 83, 'val_correct': 47, 'val_acc': 0.5662650602409639}
2025-09-10 09:25:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:25:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:25:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:25:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:25:04 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.982571, avg_loss=0.699564, seen=40, correct=25, accuracy=0.625000
2025-09-10 09:25:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:25:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:25:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:25:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2186MB allocated=2179MB
2025-09-10 09:25:05 (federatedscope.llm.llm_local.client:501) INFO: Client 51 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.98257064819336, 'test_avg_loss': 0.699564266204834, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-10 09:25:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-10 09:25:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:25:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=16, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:25:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-10 09:25:08 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=58.026718, avg_loss=0.699117, seen=83, correct=49, accuracy=0.590361
2025-09-10 09:25:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:25:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:25:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:25:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2186MB allocated=2154MB
2025-09-10 09:25:09 (federatedscope.llm.llm_local.client:480) INFO: Client 51 Adapter 1 with val results: {'val_total': 83, 'val_loss': 58.02671813964844, 'val_avg_loss': 0.6991170860198607, 'val_seen': 83, 'val_correct': 49, 'val_acc': 0.5903614457831325}
2025-09-10 09:25:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:25:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:25:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:25:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:25:11 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.330257, avg_loss=0.683256, seen=40, correct=25, accuracy=0.625000
2025-09-10 09:25:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:25:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:25:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:25:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2186MB allocated=2154MB
2025-09-10 09:25:13 (federatedscope.llm.llm_local.client:501) INFO: Client 51 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.330257415771484, 'test_avg_loss': 0.6832564353942872, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-10 09:25:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-10 09:25:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:25:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=16, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:25:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-10 09:25:16 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=57.255859, avg_loss=0.689830, seen=83, correct=48, accuracy=0.578313
2025-09-10 09:25:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:25:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:25:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:25:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2186MB allocated=2154MB
2025-09-10 09:25:19 (federatedscope.llm.llm_local.client:480) INFO: Client 51 Adapter 2 with val results: {'val_total': 83, 'val_loss': 57.255859375, 'val_avg_loss': 0.6898296310240963, 'val_seen': 83, 'val_correct': 48, 'val_acc': 0.5783132530120482}
2025-09-10 09:25:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:25:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:25:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:25:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:25:20 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.285465, avg_loss=0.632137, seen=40, correct=24, accuracy=0.600000
2025-09-10 09:25:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:25:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:25:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:25:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2186MB allocated=2154MB
2025-09-10 09:25:21 (federatedscope.llm.llm_local.client:501) INFO: Client 51 Adapter 2 with test results: {'test_total': 40, 'test_loss': 25.285465240478516, 'test_avg_loss': 0.6321366310119629, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-10 09:25:22 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:25:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-10 09:25:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:25:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=6, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:25:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-10 09:25:26 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=126.428444, avg_loss=0.672492, seen=188, correct=109, accuracy=0.579787
2025-09-10 09:25:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:25:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:25:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:25:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2206MB allocated=2187MB
2025-09-10 09:25:27 (federatedscope.llm.llm_local.client:480) INFO: Client 52 Adapter 0 with val results: {'val_total': 188, 'val_loss': 126.4284439086914, 'val_avg_loss': 0.6724917229185713, 'val_seen': 188, 'val_correct': 109, 'val_acc': 0.5797872340425532}
2025-09-10 09:25:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:25:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:25:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:25:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:25:29 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.729603, avg_loss=0.643240, seen=40, correct=24, accuracy=0.600000
2025-09-10 09:25:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:25:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:25:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:25:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2206MB allocated=2187MB
2025-09-10 09:25:31 (federatedscope.llm.llm_local.client:501) INFO: Client 52 Adapter 0 with test results: {'test_total': 40, 'test_loss': 25.729602813720703, 'test_avg_loss': 0.6432400703430176, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-10 09:25:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-10 09:25:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:25:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=6, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:25:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-10 09:25:35 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=129.267609, avg_loss=0.687594, seen=188, correct=101, accuracy=0.537234
2025-09-10 09:25:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:25:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:25:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:25:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2206MB allocated=2162MB
2025-09-10 09:25:38 (federatedscope.llm.llm_local.client:480) INFO: Client 52 Adapter 1 with val results: {'val_total': 188, 'val_loss': 129.26760864257812, 'val_avg_loss': 0.6875936629924368, 'val_seen': 188, 'val_correct': 101, 'val_acc': 0.5372340425531915}
2025-09-10 09:25:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:25:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:25:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:25:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:25:39 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.075132, avg_loss=0.651878, seen=40, correct=26, accuracy=0.650000
2025-09-10 09:25:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:25:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:25:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:25:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2206MB allocated=2162MB
2025-09-10 09:25:40 (federatedscope.llm.llm_local.client:501) INFO: Client 52 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.075132369995117, 'test_avg_loss': 0.6518783092498779, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-10 09:25:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-10 09:25:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:25:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=6, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:25:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-10 09:25:45 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=136.322556, avg_loss=0.725120, seen=188, correct=102, accuracy=0.542553
2025-09-10 09:25:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:25:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:25:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:25:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2206MB allocated=2162MB
2025-09-10 09:25:47 (federatedscope.llm.llm_local.client:480) INFO: Client 52 Adapter 2 with val results: {'val_total': 188, 'val_loss': 136.3225555419922, 'val_avg_loss': 0.7251199762871925, 'val_seen': 188, 'val_correct': 102, 'val_acc': 0.5425531914893617}
2025-09-10 09:25:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:25:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:25:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:25:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:25:49 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.101110, avg_loss=0.652528, seen=40, correct=25, accuracy=0.625000
2025-09-10 09:25:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:25:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:25:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:25:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2206MB allocated=2162MB
2025-09-10 09:25:50 (federatedscope.llm.llm_local.client:501) INFO: Client 52 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.101110458374023, 'test_avg_loss': 0.6525277614593505, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-10 09:25:50 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:25:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:25:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:25:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:25:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:25:55 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=133.953094, avg_loss=0.669765, seen=200, correct=119, accuracy=0.595000
2025-09-10 09:25:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:25:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:25:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:25:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2206MB allocated=2196MB
2025-09-10 09:25:58 (federatedscope.llm.llm_local.client:480) INFO: Client 53 Adapter 0 with val results: {'val_total': 200, 'val_loss': 133.95309448242188, 'val_avg_loss': 0.6697654724121094, 'val_seen': 200, 'val_correct': 119, 'val_acc': 0.595}
2025-09-10 09:25:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:25:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:25:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:25:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:25:59 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.065966, avg_loss=0.626649, seen=40, correct=27, accuracy=0.675000
2025-09-10 09:25:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:25:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:26:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:26:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2206MB allocated=2196MB
2025-09-10 09:26:02 (federatedscope.llm.llm_local.client:501) INFO: Client 53 Adapter 0 with test results: {'test_total': 40, 'test_loss': 25.06596565246582, 'test_avg_loss': 0.6266491413116455, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-10 09:26:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:26:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:26:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:26:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:26:06 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=137.743454, avg_loss=0.688717, seen=200, correct=112, accuracy=0.560000
2025-09-10 09:26:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:26:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:26:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:26:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2206MB allocated=2171MB
2025-09-10 09:26:08 (federatedscope.llm.llm_local.client:480) INFO: Client 53 Adapter 1 with val results: {'val_total': 200, 'val_loss': 137.7434539794922, 'val_avg_loss': 0.688717269897461, 'val_seen': 200, 'val_correct': 112, 'val_acc': 0.56}
2025-09-10 09:26:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:26:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:26:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:26:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:26:10 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.887621, avg_loss=0.697191, seen=40, correct=23, accuracy=0.575000
2025-09-10 09:26:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:26:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:26:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:26:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2206MB allocated=2171MB
2025-09-10 09:26:11 (federatedscope.llm.llm_local.client:501) INFO: Client 53 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.88762092590332, 'test_avg_loss': 0.697190523147583, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-10 09:26:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:26:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:26:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:26:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:26:16 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=138.611847, avg_loss=0.693059, seen=200, correct=110, accuracy=0.550000
2025-09-10 09:26:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:26:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:26:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:26:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2206MB allocated=2171MB
2025-09-10 09:26:18 (federatedscope.llm.llm_local.client:480) INFO: Client 53 Adapter 2 with val results: {'val_total': 200, 'val_loss': 138.61184692382812, 'val_avg_loss': 0.6930592346191407, 'val_seen': 200, 'val_correct': 110, 'val_acc': 0.55}
2025-09-10 09:26:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:26:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:26:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:26:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:26:19 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.782639, avg_loss=0.719566, seen=40, correct=20, accuracy=0.500000
2025-09-10 09:26:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:26:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:26:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:26:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2206MB allocated=2171MB
2025-09-10 09:26:20 (federatedscope.llm.llm_local.client:501) INFO: Client 53 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.782638549804688, 'test_avg_loss': 0.7195659637451172, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-10 09:26:21 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:26:21 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-10 09:26:22 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-10 09:26:22 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-10 09:26:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:26:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:26:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:26:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:26:25 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=141.186523, avg_loss=0.705933, seen=200, correct=110, accuracy=0.550000
2025-09-10 09:26:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:26:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:26:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:26:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2206MB allocated=2179MB
2025-09-10 09:26:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:26:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:26:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:26:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:26:29 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.158291, avg_loss=0.603957, seen=40, correct=25, accuracy=0.625000
2025-09-10 09:26:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:26:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:26:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:26:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2206MB allocated=2179MB
2025-09-10 09:26:30 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-10 09:26:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1979, total=7916)
2025-09-10 09:26:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:26:30 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-10 09:26:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:26:30 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=990, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-10 09:26:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:26:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:26:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:26:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:26:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:26:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:26:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:26:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:26:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:26:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:26:39 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-10 09:26:39 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-10 09:26:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:26:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:26:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:26:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:26:42 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=141.648575, avg_loss=0.708243, seen=200, correct=109, accuracy=0.545000
2025-09-10 09:26:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:26:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:26:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:26:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:26:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:26:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:26:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:26:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:26:45 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.220036, avg_loss=0.605501, seen=40, correct=26, accuracy=0.650000
2025-09-10 09:26:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:26:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:26:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:26:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:26:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:26:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:26:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:26:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:26:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:26:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:26:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:26:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:26:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:26:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:26:55 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-10 09:26:56 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-10 09:26:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:26:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:26:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:26:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:26:59 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=141.054749, avg_loss=0.705274, seen=200, correct=114, accuracy=0.570000
2025-09-10 09:26:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:26:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:27:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:27:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:27:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:27:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:27:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:27:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:27:03 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.691277, avg_loss=0.617282, seen=40, correct=25, accuracy=0.625000
2025-09-10 09:27:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:27:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:27:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:27:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:27:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:27:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:27:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:27:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:27:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:27:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:27:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:27:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:27:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:27:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:27:13 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-10 09:27:13 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-10 09:27:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:27:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:27:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:27:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:27:16 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=140.553452, avg_loss=0.702767, seen=200, correct=115, accuracy=0.575000
2025-09-10 09:27:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:27:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:27:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:27:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:27:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:27:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:27:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:27:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:27:22 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.178192, avg_loss=0.629455, seen=40, correct=24, accuracy=0.600000
2025-09-10 09:27:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:27:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:27:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:27:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:27:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=31 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:27:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=32 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:27:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=33 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:27:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=34 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:27:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=35 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:27:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=36 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:27:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=37 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:27:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=38 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:27:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=39 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:27:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=40 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:27:30 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-10 09:27:30 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-10 09:27:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:27:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:27:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:27:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:27:33 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=140.117126, avg_loss=0.700586, seen=200, correct=117, accuracy=0.585000
2025-09-10 09:27:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:27:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:27:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:27:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:27:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:27:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:27:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:27:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:27:42 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.575069, avg_loss=0.614377, seen=40, correct=26, accuracy=0.650000
2025-09-10 09:27:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:27:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:27:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:27:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:27:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=41 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:27:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=42 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:27:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=43 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:27:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=44 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:27:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=45 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:27:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=46 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:27:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=47 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:27:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=48 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:27:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=49 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:27:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=50 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:27:52 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-10 09:27:52 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-10 09:27:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:27:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:27:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:27:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:27:55 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=140.942963, avg_loss=0.704715, seen=200, correct=111, accuracy=0.555000
2025-09-10 09:27:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:27:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:27:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:27:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:27:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:27:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:27:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:27:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:27:58 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.091602, avg_loss=0.602290, seen=40, correct=27, accuracy=0.675000
2025-09-10 09:27:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:27:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:27:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:28:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:28:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=51 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:28:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=52 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:28:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=53 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:28:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=54 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:28:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=55 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:28:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=56 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:28:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=57 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:28:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=58 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:28:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=59 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:28:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=60 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:28:12 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-10 09:28:12 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-10 09:28:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:28:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:28:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:28:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:28:15 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=142.056152, avg_loss=0.710281, seen=200, correct=114, accuracy=0.570000
2025-09-10 09:28:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:28:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:28:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:28:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:28:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:28:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:28:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:28:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:28:19 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.064751, avg_loss=0.601619, seen=40, correct=27, accuracy=0.675000
2025-09-10 09:28:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:28:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:28:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:28:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:28:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=61 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:28:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=62 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:28:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=63 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:28:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=64 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:28:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=65 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:28:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=66 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:28:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=67 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:28:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=68 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:28:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=69 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:28:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=70 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:28:29 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-10 09:28:29 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-10 09:28:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:28:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:28:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:28:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:28:32 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=141.220718, avg_loss=0.706104, seen=200, correct=104, accuracy=0.520000
2025-09-10 09:28:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:28:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:28:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:28:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:28:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:28:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:28:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:28:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:28:35 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.394485, avg_loss=0.584862, seen=40, correct=28, accuracy=0.700000
2025-09-10 09:28:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:28:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:28:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:28:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:28:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=71 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:28:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=72 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:28:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=73 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:28:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=74 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:28:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=75 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:28:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=76 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:28:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=77 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:28:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=78 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:28:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=79 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:28:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=80 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:28:44 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-10 09:28:45 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-10 09:28:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:28:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:28:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:28:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:28:48 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=141.991547, avg_loss=0.709958, seen=200, correct=110, accuracy=0.550000
2025-09-10 09:28:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:28:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:28:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:28:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:28:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:28:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:28:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:28:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:28:52 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.798914, avg_loss=0.594973, seen=40, correct=25, accuracy=0.625000
2025-09-10 09:28:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:28:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:28:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:28:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:28:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=81 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:28:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=82 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:28:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=83 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:28:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=84 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:28:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=85 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:29:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=86 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:29:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=87 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:29:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=88 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:29:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=89 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:29:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=90 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:29:03 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-10 09:29:03 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-10 09:29:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:29:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:29:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:29:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:29:06 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=141.404343, avg_loss=0.707022, seen=200, correct=105, accuracy=0.525000
2025-09-10 09:29:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:29:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:29:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:29:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:29:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:29:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:29:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:29:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:29:12 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.299961, avg_loss=0.607499, seen=40, correct=25, accuracy=0.625000
2025-09-10 09:29:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:29:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:29:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:29:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:29:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=91 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:29:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=92 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:29:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=93 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:29:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=94 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:29:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=95 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:29:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=96 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:29:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=97 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:29:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=98 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:29:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=99 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:29:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=100 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:29:26 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-10 09:29:26 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-10 09:29:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:29:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:29:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:29:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:29:29 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=140.139664, avg_loss=0.700698, seen=200, correct=107, accuracy=0.535000
2025-09-10 09:29:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:29:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:29:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:29:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:29:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:29:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:29:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:29:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:29:35 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.347313, avg_loss=0.608683, seen=40, correct=22, accuracy=0.550000
2025-09-10 09:29:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:29:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:29:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:29:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:29:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-10 09:29:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-10 09:29:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:29:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:29:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:29:38 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #44', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-10 09:29:38 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #44', 'Round': 0, 'Results_raw': {}}
2025-09-10 09:29:38 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:29:38 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 2 for training...
2025-09-10 09:29:39 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-10 09:29:39 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-10 09:29:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-09-10 09:29:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:29:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=37, num_train_batch_last_epoch=26, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:29:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-10 09:29:41 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=74, loss_sum=52.216740, avg_loss=0.705632, seen=74, correct=44, accuracy=0.594595
2025-09-10 09:29:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:29:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:29:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:29:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 09:29:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:29:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:29:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:29:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:29:46 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.734932, avg_loss=0.668373, seen=40, correct=21, accuracy=0.525000
2025-09-10 09:29:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:29:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:29:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:29:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 09:29:49 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-10 09:29:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=353, total=1409)
2025-09-10 09:29:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:29:49 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-10 09:29:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:29:49 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=177, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-10 09:29:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:29:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:29:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:29:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:29:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:29:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:29:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:29:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:29:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:29:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:29:55 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-10 09:29:55 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-10 09:29:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-09-10 09:29:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:29:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=37, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:29:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-10 09:29:56 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=74, loss_sum=51.602585, avg_loss=0.697332, seen=74, correct=43, accuracy=0.581081
2025-09-10 09:29:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:29:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:29:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:30:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:30:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:30:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:30:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:30:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:30:01 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.789837, avg_loss=0.694746, seen=40, correct=24, accuracy=0.600000
2025-09-10 09:30:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:30:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:30:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:30:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:30:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:30:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:30:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:30:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:30:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:30:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:30:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:30:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:30:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:30:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:30:14 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-10 09:30:14 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-10 09:30:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-09-10 09:30:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:30:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=37, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:30:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-10 09:30:16 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=74, loss_sum=52.236580, avg_loss=0.705900, seen=74, correct=44, accuracy=0.594595
2025-09-10 09:30:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:30:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:30:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:30:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:30:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:30:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:30:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:30:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:30:20 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.558338, avg_loss=0.663958, seen=40, correct=22, accuracy=0.550000
2025-09-10 09:30:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:30:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:30:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:30:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:30:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:30:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:30:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:30:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:30:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:30:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:30:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:30:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:30:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:30:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:30:30 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-10 09:30:30 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-10 09:30:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-09-10 09:30:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:30:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=37, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:30:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-10 09:30:32 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=74, loss_sum=52.139420, avg_loss=0.704587, seen=74, correct=39, accuracy=0.527027
2025-09-10 09:30:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:30:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:30:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:30:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:30:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:30:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:30:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:30:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:30:38 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.749966, avg_loss=0.643749, seen=40, correct=23, accuracy=0.575000
2025-09-10 09:30:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:30:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:30:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:30:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:30:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=31 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:30:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=32 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:30:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=33 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:30:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=34 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:30:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=35 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:30:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=36 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:30:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=37 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:30:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=38 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:30:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=39 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:30:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=40 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:30:46 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-10 09:30:47 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-10 09:30:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-09-10 09:30:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:30:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=37, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:30:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-10 09:30:48 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=74, loss_sum=53.219036, avg_loss=0.719176, seen=74, correct=40, accuracy=0.540541
2025-09-10 09:30:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:30:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:30:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:30:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:30:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:30:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:30:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:30:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:30:53 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.301821, avg_loss=0.657546, seen=40, correct=23, accuracy=0.575000
2025-09-10 09:30:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:30:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:30:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:30:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:31:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=41 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:31:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=42 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:31:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=43 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:31:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=44 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:31:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=45 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:31:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=46 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:31:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=47 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:31:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=48 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:31:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=49 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:31:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=50 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:31:07 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-10 09:31:07 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-10 09:31:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-09-10 09:31:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:31:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=37, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:31:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-10 09:31:08 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=74, loss_sum=52.839149, avg_loss=0.714043, seen=74, correct=41, accuracy=0.554054
2025-09-10 09:31:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:31:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:31:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:31:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:31:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:31:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:31:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:31:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:31:13 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.634693, avg_loss=0.665867, seen=40, correct=25, accuracy=0.625000
2025-09-10 09:31:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:31:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:31:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:31:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:31:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=51 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:31:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=52 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:31:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=53 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:31:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=54 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:31:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=55 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:31:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=56 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:31:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=57 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:31:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=58 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:31:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=59 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:31:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=60 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:31:22 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-10 09:31:23 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-10 09:31:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-09-10 09:31:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:31:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=37, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:31:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-10 09:31:25 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=74, loss_sum=52.978020, avg_loss=0.715919, seen=74, correct=40, accuracy=0.540541
2025-09-10 09:31:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:31:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:31:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:31:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:31:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:31:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:31:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:31:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:31:31 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.080656, avg_loss=0.652016, seen=40, correct=24, accuracy=0.600000
2025-09-10 09:31:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:31:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:31:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:31:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:31:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=61 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:31:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=62 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:31:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=63 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:31:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=64 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:31:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=65 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:31:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=66 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:31:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=67 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:31:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=68 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:31:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=69 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:31:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=70 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:31:41 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-10 09:31:41 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-10 09:31:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-09-10 09:31:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:31:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=37, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:31:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-10 09:31:43 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=74, loss_sum=52.720837, avg_loss=0.712444, seen=74, correct=39, accuracy=0.527027
2025-09-10 09:31:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:31:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:31:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:31:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:31:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:31:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:31:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:31:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:31:47 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.481916, avg_loss=0.662048, seen=40, correct=24, accuracy=0.600000
2025-09-10 09:31:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:31:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:31:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:31:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:31:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=71 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:31:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=72 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:31:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=73 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:31:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=74 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:31:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=75 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:31:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=76 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:31:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=77 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:31:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=78 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:31:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=79 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:31:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=80 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:31:58 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-10 09:31:58 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-10 09:31:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-09-10 09:31:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:31:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=37, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:31:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-10 09:31:59 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=74, loss_sum=53.100803, avg_loss=0.717578, seen=74, correct=40, accuracy=0.540541
2025-09-10 09:31:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:31:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:32:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:32:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:32:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:32:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:32:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:32:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:32:02 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.831486, avg_loss=0.645787, seen=40, correct=22, accuracy=0.550000
2025-09-10 09:32:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:32:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:32:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:32:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:32:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=81 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:32:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=82 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:32:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=83 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:32:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=84 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:32:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=85 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:32:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=86 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:32:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=87 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:32:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=88 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:32:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=89 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:32:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=90 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:32:09 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-10 09:32:11 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-10 09:32:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-09-10 09:32:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:32:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=37, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:32:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-10 09:32:12 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=74, loss_sum=54.076584, avg_loss=0.730765, seen=74, correct=39, accuracy=0.527027
2025-09-10 09:32:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:32:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:32:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:32:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:32:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:32:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:32:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:32:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:32:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.318058, avg_loss=0.657951, seen=40, correct=21, accuracy=0.525000
2025-09-10 09:32:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:32:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:32:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:32:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:32:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=91 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:32:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=92 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:32:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=93 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:32:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=94 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:32:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=95 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:32:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=96 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:32:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=97 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:32:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=98 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:32:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=99 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:32:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=100 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:32:30 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-10 09:32:30 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-10 09:32:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-09-10 09:32:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:32:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=37, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:32:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-10 09:32:32 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=74, loss_sum=53.706799, avg_loss=0.725768, seen=74, correct=41, accuracy=0.554054
2025-09-10 09:32:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:32:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:32:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:32:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:32:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:32:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:32:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:32:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:32:36 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.737455, avg_loss=0.668436, seen=40, correct=22, accuracy=0.550000
2025-09-10 09:32:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:32:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:32:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:32:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:32:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-10 09:32:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-10 09:32:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:32:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:32:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:32:39 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #33', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-10 09:32:39 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #33', 'Round': 0, 'Results_raw': {}}
2025-09-10 09:32:39 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:32:39 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-10 09:32:39 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-10 09:32:39 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-10 09:32:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-10 09:32:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:32:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=16, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:32:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-10 09:32:42 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=58.259201, avg_loss=0.701918, seen=83, correct=43, accuracy=0.518072
2025-09-10 09:32:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:32:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:32:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:32:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 09:32:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:32:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:32:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:32:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:32:45 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.887308, avg_loss=0.747183, seen=40, correct=19, accuracy=0.475000
2025-09-10 09:32:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:32:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:32:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:32:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 09:32:48 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-10 09:32:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=399, total=1594)
2025-09-10 09:32:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:32:48 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-10 09:32:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:32:48 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=200, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-10 09:32:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:32:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:32:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:32:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:32:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:32:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:32:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:32:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:32:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:32:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:32:55 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-10 09:32:56 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-10 09:32:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-10 09:32:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:32:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:32:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-10 09:32:57 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=57.308815, avg_loss=0.690468, seen=83, correct=53, accuracy=0.638554
2025-09-10 09:32:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:32:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:32:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:33:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:33:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:33:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:33:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:33:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:33:02 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.289604, avg_loss=0.757240, seen=40, correct=21, accuracy=0.525000
2025-09-10 09:33:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:33:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:33:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:33:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:33:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:33:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:33:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:33:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:33:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:33:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:33:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:33:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:33:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:33:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:33:13 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-10 09:33:13 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-10 09:33:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-10 09:33:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:33:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:33:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-10 09:33:15 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=57.220604, avg_loss=0.689405, seen=83, correct=50, accuracy=0.602410
2025-09-10 09:33:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:33:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:33:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:33:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:33:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:33:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:33:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:33:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:33:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.505121, avg_loss=0.762628, seen=40, correct=21, accuracy=0.525000
2025-09-10 09:33:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:33:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:33:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:33:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:33:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:33:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:33:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:33:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:33:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:33:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:33:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:33:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:33:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:33:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:33:26 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-10 09:33:26 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-10 09:33:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-10 09:33:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:33:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:33:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-10 09:33:28 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=56.520226, avg_loss=0.680967, seen=83, correct=47, accuracy=0.566265
2025-09-10 09:33:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:33:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:33:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:33:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:33:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:33:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:33:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:33:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:33:34 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.320564, avg_loss=0.733014, seen=40, correct=21, accuracy=0.525000
2025-09-10 09:33:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:33:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:33:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:33:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:33:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=31 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:33:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=32 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:33:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=33 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:33:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=34 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:33:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=35 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:33:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=36 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:33:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=37 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:33:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=38 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:33:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=39 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:33:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=40 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:33:43 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-10 09:33:43 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-10 09:33:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-10 09:33:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:33:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:33:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-10 09:33:44 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=58.462959, avg_loss=0.704373, seen=83, correct=41, accuracy=0.493976
2025-09-10 09:33:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:33:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:33:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:33:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:33:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:33:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:33:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:33:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:33:49 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.848707, avg_loss=0.721218, seen=40, correct=21, accuracy=0.525000
2025-09-10 09:33:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:33:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:33:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:33:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:33:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=41 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:33:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=42 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:33:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=43 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:33:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=44 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:33:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=45 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:33:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=46 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:34:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=47 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:34:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=48 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:34:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=49 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:34:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=50 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:34:02 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-10 09:34:02 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-10 09:34:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-10 09:34:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:34:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:34:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-10 09:34:03 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=59.974854, avg_loss=0.722589, seen=83, correct=43, accuracy=0.518072
2025-09-10 09:34:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:34:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:34:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:34:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:34:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:34:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:34:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:34:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:34:06 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.970741, avg_loss=0.724269, seen=40, correct=21, accuracy=0.525000
2025-09-10 09:34:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:34:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:34:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:34:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:34:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=51 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:34:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=52 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:34:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=53 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:34:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=54 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:34:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=55 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:34:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=56 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:34:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=57 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:34:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=58 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:34:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=59 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:34:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=60 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:34:15 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-10 09:34:16 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-10 09:34:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-10 09:34:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:34:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:34:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-10 09:34:18 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=58.715508, avg_loss=0.707416, seen=83, correct=45, accuracy=0.542169
2025-09-10 09:34:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:34:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:34:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:34:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:34:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:34:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:34:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:34:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:34:22 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.159218, avg_loss=0.728980, seen=40, correct=21, accuracy=0.525000
2025-09-10 09:34:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:34:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:34:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:34:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:34:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=61 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:34:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=62 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:34:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=63 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:34:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=64 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:34:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=65 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:34:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=66 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:34:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=67 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:34:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=68 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:34:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=69 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:34:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=70 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:34:31 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-10 09:34:32 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-10 09:34:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-10 09:34:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:34:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:34:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-10 09:34:33 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=57.238285, avg_loss=0.689618, seen=83, correct=48, accuracy=0.578313
2025-09-10 09:34:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:34:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:34:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:34:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:34:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:34:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:34:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:34:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:34:36 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.720776, avg_loss=0.718019, seen=40, correct=21, accuracy=0.525000
2025-09-10 09:34:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:34:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:34:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:34:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:34:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=71 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:34:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=72 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:34:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=73 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:34:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=74 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:34:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=75 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:34:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=76 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:34:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=77 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:34:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=78 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:34:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=79 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:34:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=80 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:34:47 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-10 09:34:47 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-10 09:34:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-10 09:34:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:34:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:34:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-10 09:34:49 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=57.739605, avg_loss=0.695658, seen=83, correct=45, accuracy=0.542169
2025-09-10 09:34:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:34:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:34:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:34:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:34:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:34:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:34:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:34:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:34:53 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.852808, avg_loss=0.721320, seen=40, correct=19, accuracy=0.475000
2025-09-10 09:34:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:34:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:34:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:34:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:34:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=81 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:34:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=82 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:34:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=83 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:34:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=84 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:35:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=85 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:35:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=86 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:35:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=87 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:35:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=88 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:35:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=89 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:35:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=90 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:35:03 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-10 09:35:05 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-10 09:35:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-10 09:35:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:35:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:35:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-10 09:35:07 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=58.018204, avg_loss=0.699015, seen=83, correct=46, accuracy=0.554217
2025-09-10 09:35:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:35:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:35:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:35:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:35:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:35:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:35:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:35:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:35:11 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.590515, avg_loss=0.714763, seen=40, correct=21, accuracy=0.525000
2025-09-10 09:35:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:35:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:35:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:35:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:35:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=91 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:35:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=92 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:35:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=93 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:35:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=94 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:35:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=95 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:35:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=96 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:35:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=97 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:35:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=98 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:35:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=99 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:35:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=100 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:35:19 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-10 09:35:20 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-10 09:35:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-10 09:35:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:35:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:35:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-10 09:35:21 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=57.804462, avg_loss=0.696439, seen=83, correct=43, accuracy=0.518072
2025-09-10 09:35:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:35:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:35:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:35:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:35:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:35:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:35:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:35:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:35:24 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.641548, avg_loss=0.716039, seen=40, correct=23, accuracy=0.575000
2025-09-10 09:35:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:35:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:35:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:35:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:35:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-10 09:35:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-10 09:35:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:35:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:35:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:35:26 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #39', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-10 09:35:26 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #39', 'Round': 0, 'Results_raw': {}}
2025-09-10 09:35:26 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:35:26 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-10 09:35:27 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-10 09:35:27 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-10 09:35:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:35:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:35:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:35:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:35:31 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=126.042274, avg_loss=0.630211, seen=200, correct=134, accuracy=0.670000
2025-09-10 09:35:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:35:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:35:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:35:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 09:35:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:35:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:35:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:35:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:35:33 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.428854, avg_loss=0.585721, seen=40, correct=29, accuracy=0.725000
2025-09-10 09:35:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:35:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:35:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:35:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 09:35:34 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-10 09:35:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1122, total=4486)
2025-09-10 09:35:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:35:35 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-10 09:35:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:35:35 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=561, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-10 09:35:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:35:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:35:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:35:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:35:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:35:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:35:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:35:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:35:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:35:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:35:41 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-10 09:35:42 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-10 09:35:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:35:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:35:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:35:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:35:46 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=127.181396, avg_loss=0.635907, seen=200, correct=132, accuracy=0.660000
2025-09-10 09:35:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:35:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:35:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:35:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:35:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:35:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:35:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:35:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:35:48 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=22.315475, avg_loss=0.557887, seen=40, correct=28, accuracy=0.700000
2025-09-10 09:35:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:35:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:35:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:35:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:35:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:35:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:35:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:35:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:35:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:35:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:35:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:35:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:35:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:35:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:35:59 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-10 09:35:59 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-10 09:35:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:35:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:35:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:36:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:36:02 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=129.242310, avg_loss=0.646212, seen=200, correct=134, accuracy=0.670000
2025-09-10 09:36:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:36:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:36:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:36:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:36:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:36:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:36:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:36:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:36:05 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=22.472366, avg_loss=0.561809, seen=40, correct=29, accuracy=0.725000
2025-09-10 09:36:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:36:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:36:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:36:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:36:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:36:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:36:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:36:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:36:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:36:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:36:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:36:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:36:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:36:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:36:13 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-10 09:36:14 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-10 09:36:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:36:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:36:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:36:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:36:17 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=130.684158, avg_loss=0.653421, seen=200, correct=127, accuracy=0.635000
2025-09-10 09:36:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:36:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:36:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:36:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:36:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:36:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:36:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:36:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:36:21 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.047863, avg_loss=0.576197, seen=40, correct=31, accuracy=0.775000
2025-09-10 09:36:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:36:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:36:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:36:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:36:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=31 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:36:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=32 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:36:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=33 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:36:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=34 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:36:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=35 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:36:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=36 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:36:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=37 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:36:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=38 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:36:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=39 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:36:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=40 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:36:30 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-10 09:36:31 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-10 09:36:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:36:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:36:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:36:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:36:34 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=129.971756, avg_loss=0.649859, seen=200, correct=129, accuracy=0.645000
2025-09-10 09:36:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:36:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:36:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:36:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:36:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:36:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:36:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:36:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:36:39 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.472473, avg_loss=0.586812, seen=40, correct=27, accuracy=0.675000
2025-09-10 09:36:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:36:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:36:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:36:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:36:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=41 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:36:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=42 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:36:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=43 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:36:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=44 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:36:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=45 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:36:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=46 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:36:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=47 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:36:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=48 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:36:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=49 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:36:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=50 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:36:48 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-10 09:36:51 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-10 09:36:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:36:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:36:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:36:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:36:54 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=129.288681, avg_loss=0.646443, seen=200, correct=128, accuracy=0.640000
2025-09-10 09:36:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:36:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:36:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:36:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:36:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:36:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:36:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:36:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:36:57 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.391117, avg_loss=0.584778, seen=40, correct=28, accuracy=0.700000
2025-09-10 09:36:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:36:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:36:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:36:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:37:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=51 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:37:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=52 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:37:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=53 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:37:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=54 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:37:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=55 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:37:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=56 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:37:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=57 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:37:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=58 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:37:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=59 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:37:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=60 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:37:06 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-10 09:37:08 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-10 09:37:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:37:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:37:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:37:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:37:11 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=128.988678, avg_loss=0.644943, seen=200, correct=127, accuracy=0.635000
2025-09-10 09:37:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:37:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:37:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:37:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:37:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:37:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:37:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:37:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:37:15 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=22.771503, avg_loss=0.569288, seen=40, correct=29, accuracy=0.725000
2025-09-10 09:37:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:37:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:37:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:37:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:37:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=61 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:37:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=62 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:37:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=63 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:37:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=64 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:37:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=65 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:37:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=66 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:37:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=67 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:37:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=68 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:37:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=69 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:37:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=70 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:37:25 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-10 09:37:27 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-10 09:37:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:37:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:37:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:37:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:37:30 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=128.699371, avg_loss=0.643497, seen=200, correct=128, accuracy=0.640000
2025-09-10 09:37:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:37:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:37:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:37:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:37:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:37:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:37:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:37:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:37:33 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=22.601465, avg_loss=0.565037, seen=40, correct=28, accuracy=0.700000
2025-09-10 09:37:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:37:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:37:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:37:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:37:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=71 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:37:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=72 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:37:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=73 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:37:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=74 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:37:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=75 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:37:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=76 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:37:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=77 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:37:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=78 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:37:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=79 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:37:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=80 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:37:44 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-10 09:37:44 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-10 09:37:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:37:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:37:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:37:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:37:47 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=130.023193, avg_loss=0.650116, seen=200, correct=126, accuracy=0.630000
2025-09-10 09:37:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:37:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:37:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:37:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:37:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:37:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:37:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:37:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:37:53 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.151318, avg_loss=0.578783, seen=40, correct=26, accuracy=0.650000
2025-09-10 09:37:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:37:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:37:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:37:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:37:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=81 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:37:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=82 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:37:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=83 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:37:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=84 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:37:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=85 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:37:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=86 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:37:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=87 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:38:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=88 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:38:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=89 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:38:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=90 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:38:01 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-10 09:38:02 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-10 09:38:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:38:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:38:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:38:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:38:05 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=129.064545, avg_loss=0.645323, seen=200, correct=123, accuracy=0.615000
2025-09-10 09:38:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:38:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:38:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:38:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:38:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:38:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:38:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:38:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:38:10 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.624319, avg_loss=0.590608, seen=40, correct=26, accuracy=0.650000
2025-09-10 09:38:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:38:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:38:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:38:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:38:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=91 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:38:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=92 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:38:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=93 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:38:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=94 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:38:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=95 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:38:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=96 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:38:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=97 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:38:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=98 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:38:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=99 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:38:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=100 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:38:18 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-10 09:38:19 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-10 09:38:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:38:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:38:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:38:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:38:22 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=127.372147, avg_loss=0.636861, seen=200, correct=133, accuracy=0.665000
2025-09-10 09:38:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:38:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:38:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:38:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:38:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:38:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:38:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:38:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:38:27 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.138100, avg_loss=0.578452, seen=40, correct=28, accuracy=0.700000
2025-09-10 09:38:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:38:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:38:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:38:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:38:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-10 09:38:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-10 09:38:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:38:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:38:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:38:30 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #34', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-10 09:38:30 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #34', 'Round': 0, 'Results_raw': {}}
2025-09-10 09:38:30 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:38:30 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-10 09:38:32 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-10 09:38:32 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-10 09:38:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-09-10 09:38:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:38:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=69, num_train_batch_last_epoch=31, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:38:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-09-10 09:38:35 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=137, loss_sum=91.303375, avg_loss=0.666448, seen=137, correct=81, accuracy=0.591241
2025-09-10 09:38:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:38:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:38:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:38:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 09:38:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:38:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:38:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:38:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:38:37 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.508492, avg_loss=0.637712, seen=40, correct=27, accuracy=0.675000
2025-09-10 09:38:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:38:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:38:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:38:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 09:38:38 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-10 09:38:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=652, total=2605)
2025-09-10 09:38:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:38:39 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-10 09:38:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:38:39 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=326, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-10 09:38:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:38:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:38:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:38:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:38:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:38:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:38:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:38:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:38:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:38:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:38:45 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-10 09:38:46 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-10 09:38:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-09-10 09:38:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:38:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=69, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:38:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-09-10 09:38:48 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=137, loss_sum=89.010269, avg_loss=0.649710, seen=137, correct=88, accuracy=0.642336
2025-09-10 09:38:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:38:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:38:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:38:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:38:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:38:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:38:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:38:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:38:53 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.977049, avg_loss=0.649426, seen=40, correct=24, accuracy=0.600000
2025-09-10 09:38:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:38:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:38:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:38:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:38:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:38:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:38:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:38:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:38:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:39:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:39:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:39:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:39:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:39:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:39:02 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-10 09:39:02 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-10 09:39:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-09-10 09:39:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:39:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=69, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:39:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-09-10 09:39:04 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=137, loss_sum=88.891144, avg_loss=0.648840, seen=137, correct=89, accuracy=0.649635
2025-09-10 09:39:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:39:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:39:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:39:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:39:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:39:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:39:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:39:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:39:08 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.123829, avg_loss=0.653096, seen=40, correct=27, accuracy=0.675000
2025-09-10 09:39:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:39:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:39:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:39:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:39:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:39:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:39:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:39:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:39:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:39:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:39:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:39:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:39:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:39:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:39:16 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-10 09:39:16 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-10 09:39:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-09-10 09:39:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:39:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=69, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:39:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-09-10 09:39:19 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=137, loss_sum=88.590248, avg_loss=0.646644, seen=137, correct=83, accuracy=0.605839
2025-09-10 09:39:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:39:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:39:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:39:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:39:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:39:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:39:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:39:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:39:24 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.548874, avg_loss=0.663722, seen=40, correct=27, accuracy=0.675000
2025-09-10 09:39:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:39:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:39:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:39:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:39:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=31 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:39:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=32 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:39:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=33 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:39:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=34 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:39:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=35 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:39:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=36 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:39:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=37 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:39:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=38 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:39:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=39 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:39:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=40 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:39:32 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-10 09:39:33 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-10 09:39:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-09-10 09:39:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:39:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=69, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:39:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-09-10 09:39:35 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=137, loss_sum=89.081749, avg_loss=0.650232, seen=137, correct=87, accuracy=0.635036
2025-09-10 09:39:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:39:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:39:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:39:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:39:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:39:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:39:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:39:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:39:40 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.468744, avg_loss=0.661719, seen=40, correct=25, accuracy=0.625000
2025-09-10 09:39:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:39:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:39:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:39:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:39:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=41 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:39:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=42 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:39:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=43 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:39:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=44 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:39:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=45 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:39:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=46 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:39:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=47 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:39:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=48 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:39:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=49 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:39:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=50 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:39:49 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-10 09:39:49 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-10 09:39:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-09-10 09:39:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:39:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=69, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:39:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-09-10 09:39:52 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=137, loss_sum=90.156372, avg_loss=0.658076, seen=137, correct=85, accuracy=0.620438
2025-09-10 09:39:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:39:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:39:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:39:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:39:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:39:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:39:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:39:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:39:58 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.741631, avg_loss=0.643541, seen=40, correct=27, accuracy=0.675000
2025-09-10 09:39:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:39:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:39:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:39:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:40:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=51 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:40:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=52 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:40:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=53 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:40:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=54 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:40:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=55 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:40:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=56 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:40:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=57 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:40:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=58 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:40:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=59 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:40:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=60 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:40:06 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-10 09:40:07 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-10 09:40:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-09-10 09:40:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:40:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=69, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:40:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-09-10 09:40:09 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=137, loss_sum=92.713737, avg_loss=0.676743, seen=137, correct=77, accuracy=0.562044
2025-09-10 09:40:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:40:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:40:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:40:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:40:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:40:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:40:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:40:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:40:15 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.309896, avg_loss=0.632747, seen=40, correct=26, accuracy=0.650000
2025-09-10 09:40:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:40:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:40:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:40:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:40:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=61 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:40:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=62 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:40:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=63 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:40:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=64 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:40:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=65 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:40:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=66 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:40:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=67 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:40:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=68 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:40:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=69 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:40:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=70 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:40:25 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-10 09:40:25 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-10 09:40:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-09-10 09:40:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:40:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=69, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:40:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-09-10 09:40:27 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=137, loss_sum=91.244919, avg_loss=0.666021, seen=137, correct=84, accuracy=0.613139
2025-09-10 09:40:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:40:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:40:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:40:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:40:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:40:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:40:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:40:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:40:32 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.153835, avg_loss=0.628846, seen=40, correct=28, accuracy=0.700000
2025-09-10 09:40:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:40:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:40:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:40:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:40:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=71 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:40:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=72 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:40:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=73 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:40:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=74 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:40:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=75 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:40:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=76 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:40:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=77 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:40:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=78 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:40:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=79 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:40:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=80 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:40:40 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-10 09:40:41 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-10 09:40:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-09-10 09:40:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:40:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=69, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:40:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-09-10 09:40:43 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=137, loss_sum=90.013718, avg_loss=0.657034, seen=137, correct=85, accuracy=0.620438
2025-09-10 09:40:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:40:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:40:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:40:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:40:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:40:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:40:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:40:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:40:47 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.873846, avg_loss=0.646846, seen=40, correct=23, accuracy=0.575000
2025-09-10 09:40:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:40:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:40:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:40:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:40:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=81 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:40:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=82 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:40:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=83 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:40:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=84 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:40:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=85 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:40:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=86 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:40:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=87 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:40:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=88 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:40:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=89 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:40:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=90 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:40:56 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-10 09:40:56 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-10 09:40:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-09-10 09:40:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:40:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=69, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:40:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-09-10 09:40:59 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=137, loss_sum=88.476448, avg_loss=0.645813, seen=137, correct=88, accuracy=0.642336
2025-09-10 09:40:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:40:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:41:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:41:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:41:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:41:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:41:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:41:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:41:03 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.940857, avg_loss=0.673521, seen=40, correct=21, accuracy=0.525000
2025-09-10 09:41:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:41:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:41:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:41:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:41:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=91 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:41:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=92 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:41:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=93 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:41:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=94 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:41:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=95 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:41:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=96 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:41:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=97 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:41:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=98 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:41:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=99 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:41:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=100 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:41:13 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-10 09:41:14 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-10 09:41:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-09-10 09:41:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:41:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=69, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:41:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-09-10 09:41:16 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=137, loss_sum=87.548836, avg_loss=0.639043, seen=137, correct=89, accuracy=0.649635
2025-09-10 09:41:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:41:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:41:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:41:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:41:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:41:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:41:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:41:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:41:19 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.499943, avg_loss=0.662499, seen=40, correct=26, accuracy=0.650000
2025-09-10 09:41:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:41:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:41:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:41:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:41:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-10 09:41:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-10 09:41:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:41:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:41:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:41:21 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #12', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-10 09:41:21 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #12', 'Round': 0, 'Results_raw': {}}
2025-09-10 09:41:21 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:41:21 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 2 for training...
2025-09-10 09:41:22 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-10 09:41:22 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-10 09:41:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-09-10 09:41:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:41:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=18, num_train_batch_last_epoch=10, num_train_epoch=6, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:41:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-09-10 09:41:23 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=36, loss_sum=26.420765, avg_loss=0.733910, seen=36, correct=18, accuracy=0.500000
2025-09-10 09:41:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:41:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:41:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:41:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 09:41:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:41:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:41:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:41:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:41:27 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.402245, avg_loss=0.710056, seen=40, correct=23, accuracy=0.575000
2025-09-10 09:41:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:41:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:41:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:41:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 09:41:28 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-10 09:41:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=173, total=691)
2025-09-10 09:41:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:41:28 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-10 09:41:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:41:28 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=87, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-10 09:41:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:41:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:41:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:41:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:41:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:41:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:41:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:41:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:41:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:41:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:41:36 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-10 09:41:36 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-10 09:41:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-09-10 09:41:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:41:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=18, num_train_batch_last_epoch=200, num_train_epoch=6, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:41:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-09-10 09:41:37 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=36, loss_sum=26.888763, avg_loss=0.746910, seen=36, correct=19, accuracy=0.527778
2025-09-10 09:41:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:41:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:41:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:41:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:41:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:41:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:41:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:41:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:41:41 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.694809, avg_loss=0.692370, seen=40, correct=23, accuracy=0.575000
2025-09-10 09:41:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:41:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:41:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:41:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:41:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:41:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:41:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:41:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:41:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:41:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:41:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:41:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:41:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:41:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:41:51 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-10 09:41:51 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-10 09:41:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-09-10 09:41:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:41:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=18, num_train_batch_last_epoch=200, num_train_epoch=6, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:41:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-09-10 09:41:52 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=36, loss_sum=27.147711, avg_loss=0.754103, seen=36, correct=18, accuracy=0.500000
2025-09-10 09:41:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:41:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:41:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:41:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:41:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:41:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:41:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:41:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:41:55 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.347036, avg_loss=0.708676, seen=40, correct=23, accuracy=0.575000
2025-09-10 09:41:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:41:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:41:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:41:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:41:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:41:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:41:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:42:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:42:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:42:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:42:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:42:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:42:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:42:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:42:03 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-10 09:42:04 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-10 09:42:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-09-10 09:42:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:42:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=18, num_train_batch_last_epoch=200, num_train_epoch=6, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:42:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-09-10 09:42:04 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=36, loss_sum=27.000425, avg_loss=0.750012, seen=36, correct=17, accuracy=0.472222
2025-09-10 09:42:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:42:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:42:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:42:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:42:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:42:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:42:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:42:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:42:08 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.606123, avg_loss=0.715153, seen=40, correct=19, accuracy=0.475000
2025-09-10 09:42:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:42:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:42:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:42:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:42:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=31 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:42:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=32 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:42:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=33 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:42:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=34 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:42:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=35 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:42:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=36 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:42:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=37 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:42:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=38 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:42:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=39 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:42:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=40 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:42:16 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-10 09:42:16 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-10 09:42:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-09-10 09:42:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:42:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=18, num_train_batch_last_epoch=200, num_train_epoch=6, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:42:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-09-10 09:42:17 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=36, loss_sum=26.664242, avg_loss=0.740673, seen=36, correct=19, accuracy=0.527778
2025-09-10 09:42:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:42:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:42:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:42:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:42:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:42:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:42:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:42:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:42:20 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.257107, avg_loss=0.731428, seen=40, correct=18, accuracy=0.450000
2025-09-10 09:42:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:42:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:42:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:42:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:42:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=41 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:42:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=42 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:42:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=43 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:42:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=44 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:42:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=45 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:42:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=46 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:42:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=47 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:42:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=48 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:42:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=49 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:42:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=50 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:42:28 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-10 09:42:29 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-10 09:42:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-09-10 09:42:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:42:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=18, num_train_batch_last_epoch=200, num_train_epoch=6, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:42:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-09-10 09:42:30 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=36, loss_sum=26.774359, avg_loss=0.743732, seen=36, correct=18, accuracy=0.500000
2025-09-10 09:42:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:42:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:42:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:42:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:42:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:42:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:42:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:42:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:42:34 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.455462, avg_loss=0.736387, seen=40, correct=19, accuracy=0.475000
2025-09-10 09:42:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:42:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:42:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:42:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:42:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=51 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:42:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=52 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:42:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=53 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:42:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=54 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:42:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=55 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:42:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=56 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:42:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=57 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:42:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=58 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:42:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=59 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:42:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=60 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:42:42 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-10 09:42:43 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-10 09:42:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-09-10 09:42:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:42:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=18, num_train_batch_last_epoch=200, num_train_epoch=6, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:42:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-09-10 09:42:44 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=36, loss_sum=27.439041, avg_loss=0.762196, seen=36, correct=18, accuracy=0.500000
2025-09-10 09:42:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:42:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:42:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:42:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:42:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:42:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:42:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:42:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:42:47 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.563053, avg_loss=0.714076, seen=40, correct=21, accuracy=0.525000
2025-09-10 09:42:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:42:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:42:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:42:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:42:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=61 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:42:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=62 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:42:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=63 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:42:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=64 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:42:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=65 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:42:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=66 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:42:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=67 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:42:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=68 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:42:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=69 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:42:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=70 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:42:56 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-10 09:42:57 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-10 09:42:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-09-10 09:42:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:42:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=18, num_train_batch_last_epoch=200, num_train_epoch=6, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:42:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-09-10 09:42:57 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=36, loss_sum=27.083149, avg_loss=0.752310, seen=36, correct=20, accuracy=0.555556
2025-09-10 09:42:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:42:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:42:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:42:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:42:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:42:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:42:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:43:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:43:01 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.444485, avg_loss=0.711112, seen=40, correct=21, accuracy=0.525000
2025-09-10 09:43:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:43:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:43:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:43:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:43:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=71 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:43:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=72 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:43:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=73 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:43:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=74 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:43:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=75 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:43:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=76 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:43:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=77 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:43:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=78 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:43:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=79 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:43:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=80 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:43:08 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-10 09:43:10 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-10 09:43:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-09-10 09:43:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:43:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=18, num_train_batch_last_epoch=200, num_train_epoch=6, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:43:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-09-10 09:43:11 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=36, loss_sum=27.000584, avg_loss=0.750016, seen=36, correct=18, accuracy=0.500000
2025-09-10 09:43:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:43:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:43:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:43:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:43:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:43:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:43:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:43:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:43:15 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.047415, avg_loss=0.726185, seen=40, correct=18, accuracy=0.450000
2025-09-10 09:43:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:43:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:43:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:43:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:43:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=81 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:43:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=82 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:43:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=83 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:43:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=84 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:43:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=85 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:43:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=86 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:43:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=87 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:43:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=88 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:43:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=89 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:43:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=90 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:43:24 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-10 09:43:25 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-10 09:43:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-09-10 09:43:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:43:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=18, num_train_batch_last_epoch=200, num_train_epoch=6, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:43:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-09-10 09:43:26 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=36, loss_sum=27.223122, avg_loss=0.756198, seen=36, correct=19, accuracy=0.527778
2025-09-10 09:43:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:43:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:43:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:43:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:43:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:43:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:43:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:43:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:43:30 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.389185, avg_loss=0.734730, seen=40, correct=20, accuracy=0.500000
2025-09-10 09:43:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:43:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:43:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:43:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:43:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=91 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:43:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=92 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:43:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=93 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:43:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=94 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:43:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=95 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:43:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=96 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:43:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=97 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:43:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=98 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:43:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=99 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:43:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=100 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:43:39 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-10 09:43:39 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-10 09:43:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-09-10 09:43:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:43:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=18, num_train_batch_last_epoch=200, num_train_epoch=6, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:43:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-09-10 09:43:40 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=36, loss_sum=27.722813, avg_loss=0.770078, seen=36, correct=20, accuracy=0.555556
2025-09-10 09:43:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:43:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:43:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:43:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:43:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:43:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:43:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:43:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:43:46 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.652788, avg_loss=0.716320, seen=40, correct=19, accuracy=0.475000
2025-09-10 09:43:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:43:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:43:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:43:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:43:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-10 09:43:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-10 09:43:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:43:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:43:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 09:43:50 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #3', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-10 09:43:50 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #3', 'Round': 0, 'Results_raw': {}}
2025-09-10 09:43:50 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:43:50 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-10 09:43:50 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-10 09:43:50 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-10 09:43:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-09-10 09:43:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:43:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=56, num_train_batch_last_epoch=44, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:43:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-10 09:43:54 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=112, loss_sum=66.521698, avg_loss=0.593944, seen=112, correct=76, accuracy=0.678571
2025-09-10 09:43:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:43:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:43:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:43:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 09:43:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:43:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:43:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:43:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:43:56 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.103971, avg_loss=0.702599, seen=40, correct=21, accuracy=0.525000
2025-09-10 09:43:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:43:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:43:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:43:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 09:43:59 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-10 09:43:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=536, total=2144)
2025-09-10 09:43:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:43:59 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-10 09:43:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:43:59 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=268, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-10 09:44:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:44:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:44:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:44:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:44:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:44:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:44:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:44:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:44:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:44:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:44:06 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-10 09:44:07 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-10 09:44:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-09-10 09:44:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:44:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=56, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:44:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-10 09:44:09 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=112, loss_sum=66.439484, avg_loss=0.593210, seen=112, correct=78, accuracy=0.696429
2025-09-10 09:44:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:44:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:44:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:44:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:44:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:44:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:44:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:44:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:44:13 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.776814, avg_loss=0.694420, seen=40, correct=19, accuracy=0.475000
2025-09-10 09:44:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:44:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:44:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:44:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:44:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:44:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:44:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:44:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:44:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:44:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:44:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:44:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:44:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:44:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:44:21 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-10 09:44:21 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-10 09:44:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-09-10 09:44:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:44:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=56, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:44:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-10 09:44:23 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=112, loss_sum=66.049507, avg_loss=0.589728, seen=112, correct=76, accuracy=0.678571
2025-09-10 09:44:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:44:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:44:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:44:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:44:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:44:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:44:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:44:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:44:29 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.062836, avg_loss=0.676571, seen=40, correct=23, accuracy=0.575000
2025-09-10 09:44:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:44:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:44:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:44:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:44:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:44:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:44:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:44:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:44:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:44:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:44:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:44:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:44:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:44:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:44:37 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-10 09:44:38 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-10 09:44:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-09-10 09:44:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:44:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=56, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:44:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-10 09:44:40 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=112, loss_sum=67.252495, avg_loss=0.600469, seen=112, correct=74, accuracy=0.660714
2025-09-10 09:44:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:44:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:44:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:44:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:44:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:44:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:44:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:44:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:44:43 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.403114, avg_loss=0.685078, seen=40, correct=25, accuracy=0.625000
2025-09-10 09:44:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:44:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:44:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:44:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:44:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=31 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:44:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=32 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:44:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=33 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:44:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=34 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:44:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=35 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:44:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=36 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:44:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=37 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:44:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=38 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:44:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=39 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:44:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=40 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:44:52 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-10 09:44:53 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-10 09:44:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-09-10 09:44:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:44:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=56, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:44:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-10 09:44:55 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=112, loss_sum=68.269081, avg_loss=0.609545, seen=112, correct=74, accuracy=0.660714
2025-09-10 09:44:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:44:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:44:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:44:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:44:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:44:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:44:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:44:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:44:59 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.716488, avg_loss=0.692912, seen=40, correct=24, accuracy=0.600000
2025-09-10 09:44:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:44:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:45:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:45:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:45:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=41 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:45:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=42 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:45:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=43 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:45:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=44 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:45:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=45 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:45:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=46 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:45:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=47 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:45:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=48 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:45:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=49 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:45:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=50 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:45:08 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-10 09:45:09 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-10 09:45:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-09-10 09:45:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:45:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=56, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:45:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-10 09:45:11 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=112, loss_sum=66.875046, avg_loss=0.597099, seen=112, correct=73, accuracy=0.651786
2025-09-10 09:45:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:45:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:45:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:45:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:45:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:45:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:45:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:45:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:45:17 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.224600, avg_loss=0.705615, seen=40, correct=21, accuracy=0.525000
2025-09-10 09:45:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:45:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:45:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:45:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:45:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=51 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:45:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=52 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:45:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=53 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:45:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=54 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:45:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=55 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:45:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=56 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:45:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=57 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:45:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=58 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:45:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=59 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:45:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=60 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:45:26 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-10 09:45:27 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-10 09:45:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-09-10 09:45:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:45:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=56, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:45:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-10 09:45:30 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=112, loss_sum=66.838226, avg_loss=0.596770, seen=112, correct=77, accuracy=0.687500
2025-09-10 09:45:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:45:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:45:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:45:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:45:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:45:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:45:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:45:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:45:33 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.953144, avg_loss=0.698829, seen=40, correct=21, accuracy=0.525000
2025-09-10 09:45:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:45:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:45:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:45:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:45:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=61 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:45:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=62 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:45:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=63 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:45:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=64 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:45:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=65 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:45:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=66 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:45:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=67 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:45:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=68 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:45:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=69 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:45:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=70 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:45:41 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-10 09:45:43 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-10 09:45:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-09-10 09:45:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:45:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=56, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:45:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-10 09:45:45 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=112, loss_sum=66.434685, avg_loss=0.593167, seen=112, correct=78, accuracy=0.696429
2025-09-10 09:45:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:45:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:45:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:45:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:45:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:45:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:45:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:45:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:45:49 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.769400, avg_loss=0.694235, seen=40, correct=21, accuracy=0.525000
2025-09-10 09:45:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:45:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:45:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:45:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:45:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=71 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:45:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=72 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:45:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=73 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:45:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=74 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:45:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=75 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:45:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=76 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:45:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=77 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:45:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=78 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:45:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=79 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:45:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=80 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:45:58 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-10 09:45:58 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-10 09:45:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-09-10 09:45:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:45:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=56, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:46:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-10 09:46:00 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=112, loss_sum=65.695091, avg_loss=0.586563, seen=112, correct=77, accuracy=0.687500
2025-09-10 09:46:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:46:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:46:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:46:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:46:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:46:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:46:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:46:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:46:05 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.292002, avg_loss=0.682300, seen=40, correct=22, accuracy=0.550000
2025-09-10 09:46:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:46:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:46:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:46:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:46:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=81 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:46:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=82 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:46:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=83 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:46:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=84 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:46:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=85 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:46:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=86 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:46:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=87 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:46:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=88 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:46:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=89 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:46:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=90 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:46:15 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-10 09:46:17 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-10 09:46:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-09-10 09:46:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:46:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=56, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:46:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-10 09:46:19 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=112, loss_sum=65.347977, avg_loss=0.583464, seen=112, correct=79, accuracy=0.705357
2025-09-10 09:46:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:46:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:46:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:46:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:46:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:46:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:46:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:46:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:46:24 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.138535, avg_loss=0.703463, seen=40, correct=22, accuracy=0.550000
2025-09-10 09:46:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:46:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:46:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:46:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:46:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=91 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:46:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=92 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:46:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=93 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:46:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=94 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:46:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=95 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:46:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=96 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:46:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=97 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:46:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=98 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:46:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=99 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:46:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=100 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:46:33 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-10 09:46:33 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-10 09:46:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-09-10 09:46:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:46:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=56, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:46:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-10 09:46:35 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=112, loss_sum=68.246216, avg_loss=0.609341, seen=112, correct=82, accuracy=0.732143
2025-09-10 09:46:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:46:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:46:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:46:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:46:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:46:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:46:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:46:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:46:40 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.220966, avg_loss=0.705524, seen=40, correct=22, accuracy=0.550000
2025-09-10 09:46:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:46:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:46:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:46:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:46:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-10 09:46:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-10 09:46:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:46:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:46:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:46:43 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #32', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-10 09:46:43 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #32', 'Round': 0, 'Results_raw': {}}
2025-09-10 09:46:43 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:46:43 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-10 09:46:44 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-10 09:46:44 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-10 09:46:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:46:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:46:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:46:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:46:48 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=129.265320, avg_loss=0.646327, seen=200, correct=123, accuracy=0.615000
2025-09-10 09:46:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:46:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:46:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:46:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 09:46:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:46:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:46:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:46:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:46:52 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.370506, avg_loss=0.684263, seen=40, correct=26, accuracy=0.650000
2025-09-10 09:46:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:46:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:46:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:46:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 09:46:54 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-10 09:46:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1443, total=5772)
2025-09-10 09:46:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:46:54 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-10 09:46:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:46:54 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=722, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-10 09:46:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:46:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:46:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:46:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:46:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:46:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:46:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:46:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:47:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:47:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:47:00 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-10 09:47:01 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-10 09:47:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:47:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:47:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:47:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:47:04 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=125.694687, avg_loss=0.628473, seen=200, correct=125, accuracy=0.625000
2025-09-10 09:47:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:47:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:47:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:47:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 09:47:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:47:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:47:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:47:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:47:09 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.277313, avg_loss=0.631933, seen=40, correct=27, accuracy=0.675000
2025-09-10 09:47:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:47:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:47:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:47:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 09:47:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:47:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:47:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:47:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:47:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:47:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:47:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:47:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:47:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:47:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:47:18 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-10 09:47:18 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-10 09:47:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:47:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:47:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:47:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:47:21 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=126.626389, avg_loss=0.633132, seen=200, correct=129, accuracy=0.645000
2025-09-10 09:47:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:47:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:47:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:47:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 09:47:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:47:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:47:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:47:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:47:28 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.364271, avg_loss=0.634107, seen=40, correct=27, accuracy=0.675000
2025-09-10 09:47:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:47:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:47:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:47:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 09:47:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:47:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:47:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:47:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:47:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:47:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:47:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:47:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:47:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:47:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:47:37 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-10 09:47:39 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-10 09:47:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:47:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:47:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:47:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:47:42 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=126.086578, avg_loss=0.630433, seen=200, correct=125, accuracy=0.625000
2025-09-10 09:47:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:47:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:47:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:47:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 09:47:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:47:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:47:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:47:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:47:48 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.139767, avg_loss=0.653494, seen=40, correct=26, accuracy=0.650000
2025-09-10 09:47:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:47:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:47:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:47:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 09:47:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=31 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:47:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=32 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:47:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=33 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:47:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=34 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:47:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=35 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:47:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=36 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:47:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=37 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:47:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=38 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:47:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=39 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:47:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=40 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:47:58 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-10 09:47:59 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-10 09:48:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:48:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:48:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:48:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:48:02 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=128.513336, avg_loss=0.642567, seen=200, correct=123, accuracy=0.615000
2025-09-10 09:48:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:48:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:48:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:48:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 09:48:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:48:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:48:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:48:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:48:07 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.055504, avg_loss=0.726388, seen=40, correct=23, accuracy=0.575000
2025-09-10 09:48:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:48:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:48:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:48:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 09:48:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=41 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:48:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=42 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:48:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=43 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:48:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=44 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:48:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=45 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:48:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=46 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:48:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=47 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:48:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=48 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:48:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=49 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:48:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=50 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:48:15 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-10 09:48:16 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-10 09:48:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:48:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:48:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:48:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:48:19 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=131.634964, avg_loss=0.658175, seen=200, correct=122, accuracy=0.610000
2025-09-10 09:48:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:48:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:48:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:48:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 09:48:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:48:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:48:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:48:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:48:23 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.820597, avg_loss=0.770515, seen=40, correct=23, accuracy=0.575000
2025-09-10 09:48:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:48:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:48:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:48:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 09:48:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=51 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:48:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=52 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:48:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=53 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:48:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=54 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:48:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=55 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:48:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=56 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:48:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=57 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:48:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=58 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:48:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=59 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:48:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=60 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:48:34 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-10 09:48:34 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-10 09:48:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:48:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:48:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:48:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:48:37 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=130.640533, avg_loss=0.653203, seen=200, correct=118, accuracy=0.590000
2025-09-10 09:48:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:48:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:48:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:48:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 09:48:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:48:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:48:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:48:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:48:40 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.495167, avg_loss=0.737379, seen=40, correct=23, accuracy=0.575000
2025-09-10 09:48:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:48:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:48:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:48:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 09:48:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=61 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:48:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=62 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:48:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=63 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:48:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=64 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:48:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=65 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:48:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=66 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:48:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=67 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:48:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=68 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:48:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=69 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:48:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=70 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:48:48 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-10 09:48:48 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-10 09:48:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:48:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:48:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:48:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:48:51 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=127.928787, avg_loss=0.639644, seen=200, correct=121, accuracy=0.605000
2025-09-10 09:48:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:48:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:48:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:48:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 09:48:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:48:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:48:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:48:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:48:54 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.239723, avg_loss=0.680993, seen=40, correct=27, accuracy=0.675000
2025-09-10 09:48:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:48:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:48:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:48:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 09:48:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=71 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:48:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=72 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:48:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=73 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:48:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=74 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:49:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=75 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:49:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=76 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:49:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=77 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:49:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=78 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:49:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=79 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:49:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=80 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:49:02 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-10 09:49:04 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-10 09:49:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:49:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:49:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:49:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:49:07 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=127.788643, avg_loss=0.638943, seen=200, correct=125, accuracy=0.625000
2025-09-10 09:49:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:49:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:49:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:49:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 09:49:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:49:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:49:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:49:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:49:10 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.361338, avg_loss=0.659033, seen=40, correct=28, accuracy=0.700000
2025-09-10 09:49:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:49:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:49:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:49:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 09:49:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=81 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:49:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=82 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:49:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=83 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:49:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=84 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:49:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=85 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:49:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=86 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:49:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=87 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:49:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=88 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:49:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=89 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:49:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=90 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:49:20 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-10 09:49:21 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-10 09:49:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:49:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:49:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:49:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:49:24 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=126.538620, avg_loss=0.632693, seen=200, correct=124, accuracy=0.620000
2025-09-10 09:49:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:49:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:49:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:49:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 09:49:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:49:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:49:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:49:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:49:31 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.887367, avg_loss=0.672184, seen=40, correct=24, accuracy=0.600000
2025-09-10 09:49:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:49:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:49:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:49:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 09:49:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=91 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:49:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=92 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:49:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=93 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:49:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=94 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:49:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=95 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:49:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=96 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:49:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=97 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:49:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=98 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:49:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=99 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:49:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=100 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:49:40 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-10 09:49:40 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-10 09:49:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 09:49:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:49:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:49:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 09:49:43 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=125.991867, avg_loss=0.629959, seen=200, correct=123, accuracy=0.615000
2025-09-10 09:49:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:49:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:49:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:49:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 09:49:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:49:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:49:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:49:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:49:52 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.484432, avg_loss=0.687111, seen=40, correct=24, accuracy=0.600000
2025-09-10 09:49:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:49:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:49:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:49:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 09:49:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-10 09:49:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-10 09:49:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:49:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:49:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 09:49:55 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #42', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-10 09:49:55 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #42', 'Round': 0, 'Results_raw': {}}
2025-09-10 09:49:55 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:49:55 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-10 09:49:55 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-10 09:49:55 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-10 09:49:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-09-10 09:49:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:49:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=85, num_train_batch_last_epoch=15, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:49:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-09-10 09:49:58 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=170, loss_sum=112.575615, avg_loss=0.662209, seen=170, correct=103, accuracy=0.605882
2025-09-10 09:49:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:49:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:49:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:49:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 09:49:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:49:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:50:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:50:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:50:01 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.904398, avg_loss=0.647610, seen=40, correct=24, accuracy=0.600000
2025-09-10 09:50:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:50:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:50:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:50:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 09:50:02 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-10 09:50:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=812, total=3247)
2025-09-10 09:50:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:50:02 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-10 09:50:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:50:02 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=406, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-10 09:50:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:50:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:50:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:50:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:50:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:50:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:50:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:50:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:50:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:50:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:50:12 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-10 09:50:12 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-10 09:50:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-09-10 09:50:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:50:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=85, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:50:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-09-10 09:50:15 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=170, loss_sum=114.652435, avg_loss=0.674426, seen=170, correct=106, accuracy=0.623529
2025-09-10 09:50:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:50:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:50:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:50:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:50:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:50:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:50:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:50:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:50:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.584030, avg_loss=0.689601, seen=40, correct=23, accuracy=0.575000
2025-09-10 09:50:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:50:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:50:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:50:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:50:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:50:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:50:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:50:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:50:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:50:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:50:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:50:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:50:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:50:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:50:26 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-10 09:50:27 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-10 09:50:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-09-10 09:50:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:50:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=85, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:50:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-09-10 09:50:29 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=170, loss_sum=113.554413, avg_loss=0.667967, seen=170, correct=110, accuracy=0.647059
2025-09-10 09:50:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:50:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:50:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:50:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:50:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:50:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:50:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:50:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:50:35 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.425135, avg_loss=0.685628, seen=40, correct=22, accuracy=0.550000
2025-09-10 09:50:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:50:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:50:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:50:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:50:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:50:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:50:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:50:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:50:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:50:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:50:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:50:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:50:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:50:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:50:46 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-10 09:50:47 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-10 09:50:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-09-10 09:50:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:50:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=85, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:50:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-09-10 09:50:50 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=170, loss_sum=112.873222, avg_loss=0.663960, seen=170, correct=105, accuracy=0.617647
2025-09-10 09:50:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:50:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:50:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:50:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:50:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:50:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:50:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:50:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:50:55 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.857832, avg_loss=0.671446, seen=40, correct=24, accuracy=0.600000
2025-09-10 09:50:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:50:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:50:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:50:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:51:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=31 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:51:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=32 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:51:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=33 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:51:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=34 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:51:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=35 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:51:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=36 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:51:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=37 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:51:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=38 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:51:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=39 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:51:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=40 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:51:07 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-10 09:51:08 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-10 09:51:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-09-10 09:51:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:51:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=85, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:51:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-09-10 09:51:10 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=170, loss_sum=114.634827, avg_loss=0.674323, seen=170, correct=99, accuracy=0.582353
2025-09-10 09:51:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:51:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:51:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:51:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:51:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:51:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:51:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:51:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:51:13 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.874233, avg_loss=0.646856, seen=40, correct=24, accuracy=0.600000
2025-09-10 09:51:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:51:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:51:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:51:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:51:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=41 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:51:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=42 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:51:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=43 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:51:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=44 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:51:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=45 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:51:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=46 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:51:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=47 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:51:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=48 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:51:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=49 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:51:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=50 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:51:23 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-10 09:51:23 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-10 09:51:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-09-10 09:51:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:51:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=85, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:51:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-09-10 09:51:26 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=170, loss_sum=111.430756, avg_loss=0.655475, seen=170, correct=103, accuracy=0.605882
2025-09-10 09:51:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:51:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:51:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:51:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:51:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:51:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:51:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:51:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:51:33 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.252930, avg_loss=0.656323, seen=40, correct=23, accuracy=0.575000
2025-09-10 09:51:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:51:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:51:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:51:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:51:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=51 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:51:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=52 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:51:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=53 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:51:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=54 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:51:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=55 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:51:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=56 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:51:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=57 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:51:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=58 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:51:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=59 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:51:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=60 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:51:41 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-10 09:51:41 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-10 09:51:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-09-10 09:51:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:51:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=85, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:51:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-09-10 09:51:44 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=170, loss_sum=113.063225, avg_loss=0.665078, seen=170, correct=105, accuracy=0.617647
2025-09-10 09:51:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:51:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:51:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:51:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:51:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:51:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:51:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:51:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:51:48 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.703707, avg_loss=0.667593, seen=40, correct=24, accuracy=0.600000
2025-09-10 09:51:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:51:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:51:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:51:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:51:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=61 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:51:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=62 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:51:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=63 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:51:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=64 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:51:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=65 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:51:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=66 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:51:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=67 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:51:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=68 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:52:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=69 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:52:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=70 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:52:00 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-10 09:52:00 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-10 09:52:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-09-10 09:52:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:52:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=85, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:52:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-09-10 09:52:03 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=170, loss_sum=111.854782, avg_loss=0.657969, seen=170, correct=109, accuracy=0.641176
2025-09-10 09:52:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:52:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:52:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:52:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:52:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:52:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:52:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:52:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:52:06 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.915524, avg_loss=0.672888, seen=40, correct=24, accuracy=0.600000
2025-09-10 09:52:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:52:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:52:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:52:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:52:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=71 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:52:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=72 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:52:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=73 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:52:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=74 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:52:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=75 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:52:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=76 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:52:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=77 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:52:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=78 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:52:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=79 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:52:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=80 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:52:16 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-10 09:52:20 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-10 09:52:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-09-10 09:52:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:52:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=85, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:52:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-09-10 09:52:22 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=170, loss_sum=112.015388, avg_loss=0.658914, seen=170, correct=102, accuracy=0.600000
2025-09-10 09:52:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:52:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:52:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:52:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:52:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:52:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:52:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:52:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:52:27 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.928318, avg_loss=0.648208, seen=40, correct=24, accuracy=0.600000
2025-09-10 09:52:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:52:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:52:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:52:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:52:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=81 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:52:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=82 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:52:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=83 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:52:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=84 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:52:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=85 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:52:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=86 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:52:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=87 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:52:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=88 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:52:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=89 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:52:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=90 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:52:35 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-10 09:52:36 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-10 09:52:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-09-10 09:52:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:52:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=85, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:52:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-09-10 09:52:39 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=170, loss_sum=111.077766, avg_loss=0.653399, seen=170, correct=104, accuracy=0.611765
2025-09-10 09:52:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:52:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:52:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:52:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:52:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:52:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:52:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:52:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:52:48 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.849380, avg_loss=0.646235, seen=40, correct=24, accuracy=0.600000
2025-09-10 09:52:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:52:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:52:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:52:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:52:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=91 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:52:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=92 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:52:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=93 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:52:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=94 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:52:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=95 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:52:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=96 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:52:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=97 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:52:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=98 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:52:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=99 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:52:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=100 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:52:56 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-10 09:52:57 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-10 09:52:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-09-10 09:52:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:52:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=85, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:52:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-09-10 09:52:59 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=170, loss_sum=111.180054, avg_loss=0.654000, seen=170, correct=99, accuracy=0.582353
2025-09-10 09:52:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:52:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:53:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:53:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:53:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:53:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:53:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:53:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:53:06 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.271580, avg_loss=0.656789, seen=40, correct=25, accuracy=0.625000
2025-09-10 09:53:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:53:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:53:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:53:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:53:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-10 09:53:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-10 09:53:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:53:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:53:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 09:53:08 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #30', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-10 09:53:08 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #30', 'Round': 0, 'Results_raw': {}}
2025-09-10 09:53:08 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:53:08 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-10 09:53:09 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-10 09:53:09 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-10 09:53:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-09-10 09:53:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:53:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=62, num_train_batch_last_epoch=38, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:53:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-09-10 09:53:12 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=123, loss_sum=81.165985, avg_loss=0.659886, seen=123, correct=80, accuracy=0.650407
2025-09-10 09:53:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:53:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:53:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:53:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 09:53:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:53:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:53:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:53:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:53:15 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.890369, avg_loss=0.672259, seen=40, correct=23, accuracy=0.575000
2025-09-10 09:53:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:53:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:53:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:53:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 09:53:18 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-10 09:53:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=586, total=2342)
2025-09-10 09:53:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:53:18 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-10 09:53:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:53:18 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=293, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-10 09:53:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:53:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:53:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:53:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:53:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:53:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:53:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:53:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:53:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:53:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:53:26 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-10 09:53:26 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-10 09:53:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-09-10 09:53:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:53:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=62, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:53:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-09-10 09:53:28 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=123, loss_sum=80.971107, avg_loss=0.658302, seen=123, correct=75, accuracy=0.609756
2025-09-10 09:53:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:53:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:53:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:53:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2244MB allocated=2204MB
2025-09-10 09:53:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:53:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:53:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:53:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:53:31 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.130705, avg_loss=0.703268, seen=40, correct=21, accuracy=0.525000
2025-09-10 09:53:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:53:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:53:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:53:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2244MB allocated=2204MB
2025-09-10 09:53:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:53:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:53:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:53:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:53:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:53:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:53:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:53:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:53:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:53:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:53:40 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-10 09:53:41 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-10 09:53:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-09-10 09:53:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:53:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=62, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:53:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-09-10 09:53:43 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=123, loss_sum=80.488365, avg_loss=0.654377, seen=123, correct=74, accuracy=0.601626
2025-09-10 09:53:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:53:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:53:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:53:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2244MB allocated=2204MB
2025-09-10 09:53:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:53:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:53:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:53:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:53:48 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.322407, avg_loss=0.733060, seen=40, correct=17, accuracy=0.425000
2025-09-10 09:53:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:53:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:53:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:53:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2244MB allocated=2204MB
2025-09-10 09:53:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:53:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:53:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:53:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:53:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:53:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:53:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:54:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:54:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:54:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:54:01 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-10 09:54:02 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-10 09:54:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-09-10 09:54:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:54:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=62, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:54:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-09-10 09:54:03 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=123, loss_sum=81.338654, avg_loss=0.661290, seen=123, correct=74, accuracy=0.601626
2025-09-10 09:54:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:54:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:54:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:54:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2244MB allocated=2204MB
2025-09-10 09:54:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:54:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:54:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:54:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:54:08 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.332878, avg_loss=0.708322, seen=40, correct=22, accuracy=0.550000
2025-09-10 09:54:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:54:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:54:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:54:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2244MB allocated=2204MB
2025-09-10 09:54:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=31 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:54:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=32 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:54:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=33 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:54:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=34 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:54:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=35 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:54:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=36 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:54:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=37 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:54:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=38 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:54:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=39 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:54:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=40 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:54:19 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-10 09:54:20 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-10 09:54:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-09-10 09:54:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:54:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=62, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:54:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-09-10 09:54:22 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=123, loss_sum=80.567245, avg_loss=0.655018, seen=123, correct=80, accuracy=0.650407
2025-09-10 09:54:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:54:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:54:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:54:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2244MB allocated=2204MB
2025-09-10 09:54:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:54:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:54:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:54:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:54:26 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.273911, avg_loss=0.681848, seen=40, correct=21, accuracy=0.525000
2025-09-10 09:54:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:54:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:54:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:54:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2244MB allocated=2204MB
2025-09-10 09:54:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=41 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:54:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=42 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:54:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=43 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:54:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=44 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:54:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=45 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:54:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=46 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:54:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=47 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:54:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=48 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:54:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=49 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:54:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=50 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:54:34 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-10 09:54:35 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-10 09:54:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-09-10 09:54:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:54:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=62, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:54:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-09-10 09:54:37 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=123, loss_sum=79.395943, avg_loss=0.645495, seen=123, correct=76, accuracy=0.617886
2025-09-10 09:54:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:54:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:54:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:54:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2244MB allocated=2204MB
2025-09-10 09:54:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:54:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:54:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:54:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:54:41 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.034222, avg_loss=0.675856, seen=40, correct=22, accuracy=0.550000
2025-09-10 09:54:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:54:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:54:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:54:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2244MB allocated=2204MB
2025-09-10 09:54:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=51 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:54:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=52 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:54:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=53 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:54:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=54 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:54:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=55 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:54:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=56 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:54:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=57 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:54:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=58 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:54:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=59 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:54:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=60 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:54:52 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-10 09:54:53 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-10 09:54:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-09-10 09:54:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:54:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=62, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:54:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-09-10 09:54:55 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=123, loss_sum=79.730255, avg_loss=0.648213, seen=123, correct=78, accuracy=0.634146
2025-09-10 09:54:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:54:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:54:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:54:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2244MB allocated=2204MB
2025-09-10 09:54:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:54:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:54:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:55:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:55:00 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.785027, avg_loss=0.669626, seen=40, correct=24, accuracy=0.600000
2025-09-10 09:55:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:55:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:55:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:55:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2244MB allocated=2204MB
2025-09-10 09:55:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=61 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:55:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=62 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:55:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=63 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:55:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=64 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:55:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=65 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:55:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=66 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:55:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=67 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:55:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=68 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:55:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=69 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:55:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=70 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:55:15 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-10 09:55:15 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-10 09:55:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-09-10 09:55:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:55:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=62, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:55:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-09-10 09:55:17 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=123, loss_sum=79.863350, avg_loss=0.649296, seen=123, correct=76, accuracy=0.617886
2025-09-10 09:55:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:55:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:55:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:55:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2244MB allocated=2204MB
2025-09-10 09:55:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:55:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:55:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:55:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:55:22 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.091722, avg_loss=0.652293, seen=40, correct=23, accuracy=0.575000
2025-09-10 09:55:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:55:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:55:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:55:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2244MB allocated=2204MB
2025-09-10 09:55:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=71 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:55:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=72 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:55:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=73 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:55:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=74 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:55:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=75 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:55:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=76 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:55:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=77 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:55:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=78 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:55:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=79 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:55:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=80 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:55:32 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-10 09:55:32 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-10 09:55:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-09-10 09:55:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:55:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=62, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:55:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-09-10 09:55:34 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=123, loss_sum=79.542130, avg_loss=0.646684, seen=123, correct=79, accuracy=0.642276
2025-09-10 09:55:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:55:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:55:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:55:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2244MB allocated=2204MB
2025-09-10 09:55:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:55:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:55:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:55:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:55:37 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.538155, avg_loss=0.638454, seen=40, correct=24, accuracy=0.600000
2025-09-10 09:55:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:55:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:55:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:55:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2244MB allocated=2204MB
2025-09-10 09:55:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=81 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:55:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=82 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:55:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=83 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:55:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=84 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:55:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=85 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:55:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=86 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:55:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=87 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:55:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=88 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:55:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=89 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:55:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=90 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:55:47 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-10 09:55:47 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-10 09:55:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-09-10 09:55:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:55:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=62, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:55:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-09-10 09:55:49 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=123, loss_sum=80.470238, avg_loss=0.654230, seen=123, correct=83, accuracy=0.674797
2025-09-10 09:55:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:55:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:55:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:55:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2244MB allocated=2204MB
2025-09-10 09:55:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:55:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:55:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:55:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:55:53 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.379442, avg_loss=0.659486, seen=40, correct=24, accuracy=0.600000
2025-09-10 09:55:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:55:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:55:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:55:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2244MB allocated=2204MB
2025-09-10 09:55:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=91 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:55:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=92 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:56:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=93 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:56:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=94 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:56:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=95 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:56:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=96 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:56:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=97 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:56:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=98 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:56:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=99 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:56:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=100 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:56:06 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-10 09:56:07 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-10 09:56:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-09-10 09:56:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:56:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=62, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:56:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-09-10 09:56:08 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=123, loss_sum=81.362518, avg_loss=0.661484, seen=123, correct=82, accuracy=0.666667
2025-09-10 09:56:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:56:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:56:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:56:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2244MB allocated=2204MB
2025-09-10 09:56:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:56:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:56:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:56:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:56:12 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.618124, avg_loss=0.665453, seen=40, correct=25, accuracy=0.625000
2025-09-10 09:56:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:56:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:56:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:56:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2244MB allocated=2204MB
2025-09-10 09:56:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-10 09:56:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-10 09:56:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:56:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:56:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2244MB allocated=2204MB
2025-09-10 09:56:16 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #27', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-10 09:56:16 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #27', 'Round': 0, 'Results_raw': {}}
2025-09-10 09:56:16 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:56:16 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 2 for training...
2025-09-10 09:56:16 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-10 09:56:16 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-10 09:56:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-09-10 09:56:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:56:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=7, num_train_batch_last_epoch=2, num_train_epoch=15, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:56:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-09-10 09:56:18 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=14, loss_sum=9.031166, avg_loss=0.645083, seen=14, correct=9, accuracy=0.642857
2025-09-10 09:56:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:56:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:56:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:56:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 09:56:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:56:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:56:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:56:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:56:22 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.361305, avg_loss=0.609033, seen=40, correct=27, accuracy=0.675000
2025-09-10 09:56:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:56:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:56:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:56:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 09:56:24 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-10 09:56:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=72, total=285)
2025-09-10 09:56:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:56:25 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-10 09:56:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:56:25 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=36, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-10 09:56:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:56:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:56:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:56:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:56:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:56:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:56:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:56:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:56:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:56:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:56:32 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-10 09:56:33 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-10 09:56:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-09-10 09:56:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:56:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=7, num_train_batch_last_epoch=200, num_train_epoch=15, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:56:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-09-10 09:56:33 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=14, loss_sum=9.125447, avg_loss=0.651818, seen=14, correct=10, accuracy=0.714286
2025-09-10 09:56:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:56:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:56:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:56:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2248MB allocated=2204MB
2025-09-10 09:56:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:56:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:56:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:56:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:56:37 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.377331, avg_loss=0.659433, seen=40, correct=25, accuracy=0.625000
2025-09-10 09:56:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:56:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:56:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:56:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2248MB allocated=2204MB
2025-09-10 09:56:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:56:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:56:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:56:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:56:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:56:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:56:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:56:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:56:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:56:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:56:47 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-10 09:56:49 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-10 09:56:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-09-10 09:56:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:56:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=7, num_train_batch_last_epoch=200, num_train_epoch=15, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:56:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-09-10 09:56:50 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=14, loss_sum=9.044897, avg_loss=0.646064, seen=14, correct=9, accuracy=0.642857
2025-09-10 09:56:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:56:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:56:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:56:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2248MB allocated=2204MB
2025-09-10 09:56:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:56:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:56:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:56:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:56:53 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.102865, avg_loss=0.677572, seen=40, correct=21, accuracy=0.525000
2025-09-10 09:56:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:56:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:56:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:56:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2248MB allocated=2204MB
2025-09-10 09:56:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:56:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:56:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:56:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:56:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:56:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:57:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:57:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:57:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:57:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:57:01 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-10 09:57:01 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-10 09:57:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-09-10 09:57:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:57:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=7, num_train_batch_last_epoch=200, num_train_epoch=15, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:57:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-09-10 09:57:02 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=14, loss_sum=8.868259, avg_loss=0.633447, seen=14, correct=10, accuracy=0.714286
2025-09-10 09:57:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:57:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:57:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:57:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2248MB allocated=2204MB
2025-09-10 09:57:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:57:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:57:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:57:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:57:06 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.255075, avg_loss=0.681377, seen=40, correct=21, accuracy=0.525000
2025-09-10 09:57:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:57:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:57:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:57:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2248MB allocated=2204MB
2025-09-10 09:57:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=31 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:57:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=32 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:57:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=33 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:57:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=34 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:57:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=35 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:57:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=36 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:57:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=37 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:57:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=38 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:57:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=39 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:57:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=40 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:57:18 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-10 09:57:19 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-10 09:57:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-09-10 09:57:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:57:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=7, num_train_batch_last_epoch=200, num_train_epoch=15, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:57:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-09-10 09:57:19 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=14, loss_sum=8.590351, avg_loss=0.613597, seen=14, correct=10, accuracy=0.714286
2025-09-10 09:57:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:57:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:57:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:57:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2248MB allocated=2204MB
2025-09-10 09:57:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:57:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:57:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:57:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:57:24 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.885952, avg_loss=0.672149, seen=40, correct=24, accuracy=0.600000
2025-09-10 09:57:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:57:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:57:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:57:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2248MB allocated=2204MB
2025-09-10 09:57:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=41 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:57:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=42 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:57:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=43 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:57:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=44 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:57:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=45 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:57:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=46 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:57:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=47 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:57:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=48 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:57:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=49 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:57:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=50 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:57:34 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-10 09:57:34 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-10 09:57:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-09-10 09:57:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:57:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=7, num_train_batch_last_epoch=200, num_train_epoch=15, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:57:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-09-10 09:57:35 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=14, loss_sum=8.608982, avg_loss=0.614927, seen=14, correct=10, accuracy=0.714286
2025-09-10 09:57:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:57:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:57:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:57:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2248MB allocated=2204MB
2025-09-10 09:57:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:57:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:57:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:57:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:57:38 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.810188, avg_loss=0.670255, seen=40, correct=23, accuracy=0.575000
2025-09-10 09:57:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:57:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:57:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:57:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2248MB allocated=2204MB
2025-09-10 09:57:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=51 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:57:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=52 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:57:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=53 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:57:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=54 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:57:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=55 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:57:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=56 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:57:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=57 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:57:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=58 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:57:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=59 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:57:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=60 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:57:46 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-10 09:57:47 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-10 09:57:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-09-10 09:57:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:57:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=7, num_train_batch_last_epoch=200, num_train_epoch=15, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:57:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-09-10 09:57:47 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=14, loss_sum=8.494876, avg_loss=0.606777, seen=14, correct=10, accuracy=0.714286
2025-09-10 09:57:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:57:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:57:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:57:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2248MB allocated=2204MB
2025-09-10 09:57:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:57:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:57:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:57:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:57:54 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.845736, avg_loss=0.671143, seen=40, correct=23, accuracy=0.575000
2025-09-10 09:57:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:57:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:57:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:57:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2248MB allocated=2204MB
2025-09-10 09:57:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=61 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:57:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=62 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:57:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=63 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:57:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=64 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:57:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=65 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:58:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=66 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:58:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=67 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:58:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=68 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:58:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=69 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:58:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=70 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:58:02 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-10 09:58:02 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-10 09:58:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-09-10 09:58:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:58:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=7, num_train_batch_last_epoch=200, num_train_epoch=15, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:58:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-09-10 09:58:03 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=14, loss_sum=8.212309, avg_loss=0.586593, seen=14, correct=10, accuracy=0.714286
2025-09-10 09:58:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:58:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:58:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:58:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2248MB allocated=2204MB
2025-09-10 09:58:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:58:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:58:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:58:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:58:08 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.995975, avg_loss=0.674899, seen=40, correct=22, accuracy=0.550000
2025-09-10 09:58:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:58:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:58:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:58:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2248MB allocated=2204MB
2025-09-10 09:58:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=71 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:58:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=72 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:58:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=73 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:58:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=74 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:58:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=75 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:58:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=76 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:58:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=77 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:58:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=78 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:58:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=79 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:58:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=80 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:58:20 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-10 09:58:21 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-10 09:58:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-09-10 09:58:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:58:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=7, num_train_batch_last_epoch=200, num_train_epoch=15, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:58:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-09-10 09:58:21 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=14, loss_sum=8.378937, avg_loss=0.598495, seen=14, correct=10, accuracy=0.714286
2025-09-10 09:58:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:58:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:58:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:58:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2248MB allocated=2204MB
2025-09-10 09:58:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:58:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:58:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:58:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:58:25 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.087185, avg_loss=0.677180, seen=40, correct=23, accuracy=0.575000
2025-09-10 09:58:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:58:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:58:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:58:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2248MB allocated=2204MB
2025-09-10 09:58:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=81 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:58:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=82 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:58:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=83 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:58:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=84 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:58:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=85 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:58:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=86 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:58:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=87 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:58:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=88 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:58:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=89 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:58:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=90 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:58:33 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-10 09:58:36 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-10 09:58:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-09-10 09:58:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:58:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=7, num_train_batch_last_epoch=200, num_train_epoch=15, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:58:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-09-10 09:58:37 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=14, loss_sum=8.050455, avg_loss=0.575033, seen=14, correct=10, accuracy=0.714286
2025-09-10 09:58:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:58:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:58:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:58:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2248MB allocated=2204MB
2025-09-10 09:58:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:58:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:58:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:58:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:58:45 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.873831, avg_loss=0.671846, seen=40, correct=22, accuracy=0.550000
2025-09-10 09:58:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:58:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:58:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:58:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2248MB allocated=2204MB
2025-09-10 09:58:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=91 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:58:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=92 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:58:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=93 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:58:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=94 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:58:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=95 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:58:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=96 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:58:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=97 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:58:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=98 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:58:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=99 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:58:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=100 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:58:53 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-10 09:58:54 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-10 09:58:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-09-10 09:58:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:58:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=7, num_train_batch_last_epoch=200, num_train_epoch=15, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:58:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-09-10 09:58:55 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=14, loss_sum=7.995712, avg_loss=0.571122, seen=14, correct=9, accuracy=0.642857
2025-09-10 09:58:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:58:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:58:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:58:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2248MB allocated=2204MB
2025-09-10 09:58:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:58:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:58:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:59:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:59:01 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.847019, avg_loss=0.671175, seen=40, correct=22, accuracy=0.550000
2025-09-10 09:59:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:59:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:59:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:59:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2248MB allocated=2204MB
2025-09-10 09:59:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-10 09:59:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-10 09:59:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:59:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:59:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2248MB allocated=2204MB
2025-09-10 09:59:03 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #5', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-10 09:59:03 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #5', 'Round': 0, 'Results_raw': {}}
2025-09-10 09:59:03 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 09:59:03 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 2 for training...
2025-09-10 09:59:04 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-10 09:59:04 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-10 09:59:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-09-10 09:59:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:59:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=16, num_train_batch_last_epoch=4, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:59:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-10 09:59:06 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=32, loss_sum=19.565603, avg_loss=0.611425, seen=32, correct=20, accuracy=0.625000
2025-09-10 09:59:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:59:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:59:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:59:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 09:59:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:59:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:59:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:59:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:59:10 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.473003, avg_loss=0.736825, seen=40, correct=19, accuracy=0.475000
2025-09-10 09:59:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:59:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:59:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:59:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 09:59:11 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-10 09:59:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=156, total=621)
2025-09-10 09:59:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:59:11 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-10 09:59:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:59:11 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=78, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-10 09:59:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:59:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:59:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:59:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:59:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:59:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:59:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:59:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:59:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:59:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:59:18 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-10 09:59:19 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-10 09:59:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-09-10 09:59:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:59:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=16, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:59:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-10 09:59:20 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=32, loss_sum=18.508009, avg_loss=0.578375, seen=32, correct=22, accuracy=0.687500
2025-09-10 09:59:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:59:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:59:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:59:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2248MB allocated=2204MB
2025-09-10 09:59:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:59:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:59:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:59:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:59:23 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.723127, avg_loss=0.693078, seen=40, correct=20, accuracy=0.500000
2025-09-10 09:59:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:59:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:59:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:59:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2248MB allocated=2204MB
2025-09-10 09:59:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:59:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:59:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:59:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:59:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:59:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:59:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:59:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:59:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:59:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:59:32 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-10 09:59:33 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-10 09:59:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-09-10 09:59:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:59:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=16, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:59:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-10 09:59:34 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=32, loss_sum=18.943272, avg_loss=0.591977, seen=32, correct=21, accuracy=0.656250
2025-09-10 09:59:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:59:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:59:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:59:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2248MB allocated=2204MB
2025-09-10 09:59:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:59:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:59:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:59:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:59:38 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.760590, avg_loss=0.694015, seen=40, correct=21, accuracy=0.525000
2025-09-10 09:59:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:59:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:59:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:59:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2248MB allocated=2204MB
2025-09-10 09:59:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:59:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:59:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:59:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:59:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:59:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:59:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:59:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:59:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:59:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:59:47 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-10 09:59:48 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-10 09:59:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-09-10 09:59:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:59:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=16, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:59:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-10 09:59:48 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=32, loss_sum=19.776083, avg_loss=0.618003, seen=32, correct=21, accuracy=0.656250
2025-09-10 09:59:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:59:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:59:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:59:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2248MB allocated=2204MB
2025-09-10 09:59:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 09:59:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:59:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 09:59:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 09:59:56 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.459129, avg_loss=0.736478, seen=40, correct=19, accuracy=0.475000
2025-09-10 09:59:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 09:59:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 09:59:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 09:59:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2248MB allocated=2204MB
2025-09-10 09:59:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=31 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:59:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=32 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 09:59:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=33 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:00:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=34 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:00:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=35 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:00:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=36 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:00:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=37 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:00:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=38 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:00:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=39 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:00:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=40 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:00:04 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-10 10:00:05 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-10 10:00:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-09-10 10:00:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:00:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=16, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:00:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-10 10:00:06 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=32, loss_sum=19.391636, avg_loss=0.605989, seen=32, correct=20, accuracy=0.625000
2025-09-10 10:00:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:00:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:00:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:00:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2248MB allocated=2204MB
2025-09-10 10:00:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:00:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:00:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:00:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:00:11 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.564774, avg_loss=0.739119, seen=40, correct=18, accuracy=0.450000
2025-09-10 10:00:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:00:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:00:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:00:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2248MB allocated=2204MB
2025-09-10 10:00:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=41 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:00:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=42 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:00:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=43 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:00:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=44 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:00:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=45 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:00:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=46 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:00:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=47 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:00:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=48 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:00:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=49 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:00:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=50 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:00:22 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-10 10:00:23 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-10 10:00:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-09-10 10:00:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:00:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=16, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:00:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-10 10:00:24 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=32, loss_sum=18.812487, avg_loss=0.587890, seen=32, correct=20, accuracy=0.625000
2025-09-10 10:00:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:00:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:00:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:00:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2248MB allocated=2204MB
2025-09-10 10:00:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:00:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:00:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:00:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:00:29 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.814648, avg_loss=0.720366, seen=40, correct=18, accuracy=0.450000
2025-09-10 10:00:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:00:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:00:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:00:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2248MB allocated=2204MB
2025-09-10 10:00:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=51 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:00:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=52 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:00:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=53 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:00:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=54 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:00:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=55 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:00:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=56 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:00:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=57 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:00:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=58 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:00:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=59 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:00:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=60 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:00:39 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-10 10:00:43 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-10 10:00:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-09-10 10:00:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:00:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=16, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:00:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-10 10:00:44 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=32, loss_sum=18.535778, avg_loss=0.579243, seen=32, correct=21, accuracy=0.656250
2025-09-10 10:00:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:00:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:00:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:00:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2248MB allocated=2204MB
2025-09-10 10:00:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:00:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:00:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:00:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:00:47 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.802900, avg_loss=0.695073, seen=40, correct=19, accuracy=0.475000
2025-09-10 10:00:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:00:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:00:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:00:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2248MB allocated=2204MB
2025-09-10 10:00:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=61 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:00:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=62 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:00:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=63 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:00:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=64 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:00:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=65 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:00:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=66 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:00:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=67 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:00:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=68 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:00:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=69 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:00:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=70 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:00:56 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-10 10:00:57 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-10 10:00:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-09-10 10:00:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:00:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=16, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:00:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-10 10:00:58 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=32, loss_sum=19.337463, avg_loss=0.604296, seen=32, correct=20, accuracy=0.625000
2025-09-10 10:00:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:00:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:00:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:01:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2248MB allocated=2204MB
2025-09-10 10:01:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:01:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:01:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:01:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:01:05 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.304768, avg_loss=0.732619, seen=40, correct=20, accuracy=0.500000
2025-09-10 10:01:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:01:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:01:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:01:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2248MB allocated=2204MB
2025-09-10 10:01:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=71 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:01:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=72 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:01:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=73 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:01:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=74 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:01:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=75 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:01:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=76 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:01:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=77 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:01:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=78 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:01:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=79 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:01:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=80 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:01:14 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-10 10:01:16 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-10 10:01:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-09-10 10:01:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:01:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=16, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:01:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-10 10:01:17 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=32, loss_sum=19.119553, avg_loss=0.597486, seen=32, correct=20, accuracy=0.625000
2025-09-10 10:01:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:01:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:01:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:01:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2248MB allocated=2204MB
2025-09-10 10:01:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:01:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:01:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:01:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:01:21 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.402872, avg_loss=0.735072, seen=40, correct=19, accuracy=0.475000
2025-09-10 10:01:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:01:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:01:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:01:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2248MB allocated=2204MB
2025-09-10 10:01:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=81 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:01:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=82 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:01:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=83 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:01:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=84 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:01:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=85 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:01:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=86 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:01:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=87 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:01:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=88 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:01:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=89 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:01:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=90 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:01:32 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-10 10:01:33 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-10 10:01:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-09-10 10:01:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:01:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=16, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:01:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-10 10:01:34 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=32, loss_sum=18.591742, avg_loss=0.580992, seen=32, correct=21, accuracy=0.656250
2025-09-10 10:01:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:01:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:01:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:01:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2248MB allocated=2204MB
2025-09-10 10:01:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:01:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:01:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:01:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:01:38 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.669275, avg_loss=0.716732, seen=40, correct=20, accuracy=0.500000
2025-09-10 10:01:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:01:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:01:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:01:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2248MB allocated=2204MB
2025-09-10 10:01:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=91 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:01:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=92 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:01:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=93 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:01:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=94 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:01:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=95 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:01:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=96 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:01:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=97 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:01:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=98 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:01:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=99 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:01:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=100 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:01:46 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-10 10:01:47 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-10 10:01:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-09-10 10:01:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:01:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=16, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:01:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-10 10:01:48 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=32, loss_sum=18.499817, avg_loss=0.578119, seen=32, correct=20, accuracy=0.625000
2025-09-10 10:01:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:01:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:01:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:01:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2248MB allocated=2204MB
2025-09-10 10:01:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:01:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:01:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:01:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:01:58 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.219925, avg_loss=0.705498, seen=40, correct=20, accuracy=0.500000
2025-09-10 10:01:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:01:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:01:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:02:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2248MB allocated=2204MB
2025-09-10 10:02:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-10 10:02:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-10 10:02:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:02:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:02:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2248MB allocated=2204MB
2025-09-10 10:02:01 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #11', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-10 10:02:01 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #11', 'Round': 0, 'Results_raw': {}}
2025-09-10 10:02:01 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 10:02:01 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-10 10:02:02 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-10 10:02:02 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-10 10:02:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-09-10 10:02:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:02:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=38, num_train_batch_last_epoch=24, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:02:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-10 10:02:04 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=75, loss_sum=47.913254, avg_loss=0.638843, seen=75, correct=49, accuracy=0.653333
2025-09-10 10:02:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:02:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:02:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:02:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 10:02:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:02:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:02:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:02:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:02:08 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.905750, avg_loss=0.697644, seen=40, correct=22, accuracy=0.550000
2025-09-10 10:02:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:02:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:02:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:02:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 10:02:09 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-10 10:02:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=359, total=1434)
2025-09-10 10:02:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:02:09 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-10 10:02:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:02:09 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=180, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-10 10:02:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:02:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:02:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:02:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:02:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:02:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:02:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:02:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:02:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:02:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:02:16 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-10 10:02:17 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-10 10:02:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-09-10 10:02:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:02:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=38, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:02:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-10 10:02:18 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=75, loss_sum=48.389652, avg_loss=0.645195, seen=75, correct=46, accuracy=0.613333
2025-09-10 10:02:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:02:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:02:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:02:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:02:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:02:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:02:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:02:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:02:22 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.049343, avg_loss=0.676234, seen=40, correct=23, accuracy=0.575000
2025-09-10 10:02:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:02:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:02:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:02:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:02:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:02:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:02:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:02:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:02:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:02:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:02:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:02:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:02:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:02:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:02:32 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-10 10:02:33 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-10 10:02:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-09-10 10:02:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:02:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=38, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:02:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-10 10:02:34 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=75, loss_sum=48.766731, avg_loss=0.650223, seen=75, correct=46, accuracy=0.613333
2025-09-10 10:02:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:02:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:02:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:02:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:02:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:02:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:02:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:02:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:02:40 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.499239, avg_loss=0.662481, seen=40, correct=23, accuracy=0.575000
2025-09-10 10:02:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:02:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:02:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:02:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:02:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:02:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:02:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:02:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:02:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:02:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:02:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:02:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:02:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:02:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:02:50 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-10 10:02:51 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-10 10:02:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-09-10 10:02:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:02:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=38, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:02:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-10 10:02:52 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=75, loss_sum=48.933945, avg_loss=0.652453, seen=75, correct=45, accuracy=0.600000
2025-09-10 10:02:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:02:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:02:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:02:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:02:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:02:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:02:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:02:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:02:57 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.479511, avg_loss=0.661988, seen=40, correct=19, accuracy=0.475000
2025-09-10 10:02:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:02:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:02:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:02:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:03:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=31 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:03:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=32 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:03:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=33 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:03:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=34 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:03:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=35 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:03:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=36 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:03:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=37 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:03:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=38 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:03:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=39 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:03:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=40 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:03:07 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-10 10:03:07 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-10 10:03:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-09-10 10:03:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:03:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=38, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:03:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-10 10:03:08 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=75, loss_sum=48.614254, avg_loss=0.648190, seen=75, correct=46, accuracy=0.613333
2025-09-10 10:03:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:03:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:03:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:03:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:03:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:03:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:03:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:03:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:03:14 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.526066, avg_loss=0.663152, seen=40, correct=20, accuracy=0.500000
2025-09-10 10:03:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:03:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:03:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:03:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:03:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=41 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:03:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=42 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:03:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=43 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:03:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=44 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:03:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=45 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:03:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=46 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:03:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=47 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:03:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=48 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:03:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=49 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:03:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=50 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:03:23 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-10 10:03:23 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-10 10:03:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-09-10 10:03:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:03:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=38, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:03:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-10 10:03:25 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=75, loss_sum=48.216045, avg_loss=0.642881, seen=75, correct=49, accuracy=0.653333
2025-09-10 10:03:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:03:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:03:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:03:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:03:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:03:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:03:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:03:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:03:30 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.088499, avg_loss=0.677212, seen=40, correct=23, accuracy=0.575000
2025-09-10 10:03:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:03:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:03:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:03:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:03:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=51 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:03:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=52 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:03:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=53 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:03:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=54 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:03:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=55 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:03:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=56 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:03:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=57 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:03:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=58 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:03:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=59 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:03:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=60 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:03:40 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-10 10:03:40 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-10 10:03:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-09-10 10:03:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:03:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=38, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:03:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-10 10:03:41 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=75, loss_sum=47.653214, avg_loss=0.635376, seen=75, correct=49, accuracy=0.653333
2025-09-10 10:03:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:03:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:03:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:03:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:03:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:03:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:03:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:03:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:03:45 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.832205, avg_loss=0.695805, seen=40, correct=23, accuracy=0.575000
2025-09-10 10:03:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:03:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:03:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:03:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:03:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=61 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:03:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=62 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:03:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=63 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:03:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=64 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:03:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=65 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:03:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=66 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:03:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=67 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:03:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=68 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:03:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=69 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:03:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=70 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:03:54 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-10 10:03:55 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-10 10:03:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-09-10 10:03:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:03:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=38, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:03:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-10 10:03:56 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=75, loss_sum=47.742386, avg_loss=0.636565, seen=75, correct=46, accuracy=0.613333
2025-09-10 10:03:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:03:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:03:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:03:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:03:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:03:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:03:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:04:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:04:00 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.279343, avg_loss=0.681984, seen=40, correct=22, accuracy=0.550000
2025-09-10 10:04:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:04:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:04:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:04:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:04:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=71 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:04:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=72 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:04:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=73 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:04:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=74 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:04:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=75 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:04:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=76 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:04:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=77 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:04:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=78 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:04:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=79 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:04:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=80 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:04:10 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-10 10:04:11 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-10 10:04:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-09-10 10:04:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:04:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=38, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:04:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-10 10:04:13 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=75, loss_sum=48.585117, avg_loss=0.647802, seen=75, correct=46, accuracy=0.613333
2025-09-10 10:04:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:04:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:04:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:04:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:04:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:04:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:04:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:04:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:04:16 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.604519, avg_loss=0.665113, seen=40, correct=21, accuracy=0.525000
2025-09-10 10:04:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:04:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:04:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:04:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:04:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=81 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:04:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=82 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:04:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=83 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:04:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=84 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:04:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=85 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:04:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=86 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:04:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=87 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:04:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=88 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:04:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=89 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:04:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=90 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:04:25 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-10 10:04:25 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-10 10:04:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-09-10 10:04:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:04:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=38, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:04:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-10 10:04:27 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=75, loss_sum=49.859066, avg_loss=0.664788, seen=75, correct=48, accuracy=0.640000
2025-09-10 10:04:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:04:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:04:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:04:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:04:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:04:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:04:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:04:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:04:30 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.787872, avg_loss=0.644697, seen=40, correct=23, accuracy=0.575000
2025-09-10 10:04:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:04:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:04:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:04:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:04:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=91 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:04:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=92 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:04:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=93 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:04:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=94 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:04:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=95 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:04:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=96 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:04:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=97 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:04:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=98 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:04:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=99 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:04:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=100 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:04:38 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-10 10:04:40 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-10 10:04:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-09-10 10:04:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:04:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=38, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:04:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-10 10:04:41 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=75, loss_sum=48.503544, avg_loss=0.646714, seen=75, correct=44, accuracy=0.586667
2025-09-10 10:04:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:04:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:04:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:04:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:04:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:04:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:04:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:04:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:04:46 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.795376, avg_loss=0.644884, seen=40, correct=23, accuracy=0.575000
2025-09-10 10:04:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:04:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:04:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:04:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:04:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-10 10:04:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-10 10:04:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:04:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:04:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:04:49 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #28', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-10 10:04:49 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #28', 'Round': 0, 'Results_raw': {}}
2025-09-10 10:04:49 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 10:04:49 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-10 10:04:49 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-10 10:04:49 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-10 10:04:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:04:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:04:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:04:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:04:53 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=133.953094, avg_loss=0.669765, seen=200, correct=119, accuracy=0.595000
2025-09-10 10:04:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:04:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:04:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:04:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 10:04:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:04:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:04:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:04:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:04:57 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.065966, avg_loss=0.626649, seen=40, correct=27, accuracy=0.675000
2025-09-10 10:04:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:04:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:04:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:04:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 10:04:58 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-10 10:04:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1698, total=6791)
2025-09-10 10:04:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:04:58 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-10 10:04:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:04:58 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=849, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-10 10:05:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:05:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:05:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:05:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:05:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:05:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:05:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:05:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:05:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:05:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:05:06 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-10 10:05:08 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-10 10:05:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:05:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:05:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:05:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:05:11 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.751862, avg_loss=0.678759, seen=200, correct=125, accuracy=0.625000
2025-09-10 10:05:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:05:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:05:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:05:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:05:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:05:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:05:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:05:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:05:14 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.563559, avg_loss=0.639089, seen=40, correct=27, accuracy=0.675000
2025-09-10 10:05:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:05:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:05:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:05:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:05:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:05:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:05:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:05:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:05:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:05:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:05:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:05:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:05:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:05:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:05:22 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-10 10:05:22 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-10 10:05:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:05:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:05:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:05:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:05:25 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=134.157196, avg_loss=0.670786, seen=200, correct=120, accuracy=0.600000
2025-09-10 10:05:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:05:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:05:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:05:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:05:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:05:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:05:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:05:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:05:31 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.335394, avg_loss=0.658385, seen=40, correct=24, accuracy=0.600000
2025-09-10 10:05:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:05:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:05:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:05:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:05:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:05:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:05:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:05:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:05:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:05:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:05:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:05:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:05:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:05:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:05:41 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-10 10:05:42 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-10 10:05:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:05:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:05:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:05:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:05:45 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=134.942917, avg_loss=0.674715, seen=200, correct=122, accuracy=0.610000
2025-09-10 10:05:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:05:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:05:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:05:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:05:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:05:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:05:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:05:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:05:51 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.319126, avg_loss=0.657978, seen=40, correct=26, accuracy=0.650000
2025-09-10 10:05:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:05:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:05:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:05:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:05:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=31 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:05:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=32 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:05:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=33 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:05:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=34 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:05:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=35 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:05:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=36 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:05:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=37 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:05:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=38 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:06:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=39 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:06:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=40 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:06:00 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-10 10:06:01 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-10 10:06:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:06:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:06:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:06:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:06:04 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=133.928802, avg_loss=0.669644, seen=200, correct=118, accuracy=0.590000
2025-09-10 10:06:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:06:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:06:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:06:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:06:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:06:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:06:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:06:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:06:07 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.115240, avg_loss=0.652881, seen=40, correct=26, accuracy=0.650000
2025-09-10 10:06:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:06:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:06:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:06:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:06:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=41 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:06:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=42 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:06:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=43 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:06:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=44 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:06:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=45 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:06:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=46 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:06:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=47 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:06:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=48 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:06:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=49 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:06:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=50 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:06:16 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-10 10:06:17 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-10 10:06:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:06:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:06:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:06:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:06:20 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=134.711594, avg_loss=0.673558, seen=200, correct=122, accuracy=0.610000
2025-09-10 10:06:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:06:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:06:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:06:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:06:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:06:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:06:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:06:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:06:24 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.248362, avg_loss=0.656209, seen=40, correct=25, accuracy=0.625000
2025-09-10 10:06:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:06:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:06:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:06:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:06:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=51 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:06:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=52 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:06:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=53 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:06:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=54 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:06:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=55 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:06:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=56 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:06:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=57 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:06:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=58 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:06:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=59 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:06:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=60 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:06:33 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-10 10:06:33 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-10 10:06:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:06:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:06:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:06:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:06:36 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=133.290222, avg_loss=0.666451, seen=200, correct=123, accuracy=0.615000
2025-09-10 10:06:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:06:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:06:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:06:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:06:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:06:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:06:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:06:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:06:42 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.317188, avg_loss=0.657930, seen=40, correct=27, accuracy=0.675000
2025-09-10 10:06:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:06:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:06:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:06:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:06:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=61 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:06:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=62 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:06:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=63 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:06:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=64 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:06:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=65 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:06:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=66 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:06:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=67 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:06:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=68 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:06:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=69 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:06:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=70 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:06:50 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-10 10:06:51 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-10 10:06:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:06:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:06:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:06:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:06:54 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=132.401901, avg_loss=0.662010, seen=200, correct=122, accuracy=0.610000
2025-09-10 10:06:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:06:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:06:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:06:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:06:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:06:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:06:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:06:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:06:58 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.663242, avg_loss=0.641581, seen=40, correct=26, accuracy=0.650000
2025-09-10 10:06:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:06:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:07:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:07:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:07:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=71 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:07:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=72 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:07:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=73 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:07:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=74 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:07:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=75 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:07:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=76 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:07:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=77 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:07:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=78 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:07:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=79 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:07:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=80 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:07:08 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-10 10:07:08 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-10 10:07:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:07:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:07:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:07:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:07:11 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=131.870728, avg_loss=0.659354, seen=200, correct=127, accuracy=0.635000
2025-09-10 10:07:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:07:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:07:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:07:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:07:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:07:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:07:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:07:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:07:17 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.012278, avg_loss=0.650307, seen=40, correct=25, accuracy=0.625000
2025-09-10 10:07:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:07:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:07:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:07:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:07:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=81 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:07:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=82 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:07:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=83 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:07:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=84 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:07:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=85 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:07:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=86 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:07:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=87 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:07:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=88 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:07:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=89 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:07:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=90 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:07:26 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-10 10:07:26 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-10 10:07:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:07:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:07:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:07:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:07:29 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=132.683868, avg_loss=0.663419, seen=200, correct=119, accuracy=0.595000
2025-09-10 10:07:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:07:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:07:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:07:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:07:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:07:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:07:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:07:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:07:35 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.369450, avg_loss=0.659236, seen=40, correct=25, accuracy=0.625000
2025-09-10 10:07:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:07:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:07:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:07:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:07:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=91 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:07:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=92 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:07:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=93 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:07:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=94 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:07:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=95 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:07:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=96 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:07:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=97 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:07:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=98 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:07:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=99 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:07:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=100 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:07:44 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-10 10:07:44 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-10 10:07:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:07:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:07:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:07:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:07:47 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=133.437668, avg_loss=0.667188, seen=200, correct=120, accuracy=0.600000
2025-09-10 10:07:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:07:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:07:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:07:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:07:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:07:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:07:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:07:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:07:53 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.545284, avg_loss=0.663632, seen=40, correct=27, accuracy=0.675000
2025-09-10 10:07:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:07:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:07:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:07:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:07:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-10 10:07:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-10 10:07:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:07:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:07:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:07:57 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #53', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-10 10:07:57 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #53', 'Round': 0, 'Results_raw': {}}
2025-09-10 10:07:57 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 10:07:57 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 2 for training...
2025-09-10 10:07:58 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-10 10:07:58 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-10 10:07:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-09-10 10:07:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:07:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=97, num_train_batch_last_epoch=3, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:08:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-09-10 10:08:02 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=193, loss_sum=133.792099, avg_loss=0.693223, seen=193, correct=110, accuracy=0.569948
2025-09-10 10:08:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:08:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:08:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:08:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 10:08:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:08:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:08:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:08:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:08:04 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.187391, avg_loss=0.654685, seen=40, correct=23, accuracy=0.575000
2025-09-10 10:08:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:08:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:08:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:08:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 10:08:06 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-10 10:08:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=920, total=3679)
2025-09-10 10:08:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:08:07 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-10 10:08:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:08:07 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=460, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-10 10:08:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:08:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:08:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:08:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:08:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:08:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:08:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:08:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:08:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:08:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:08:14 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-10 10:08:16 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-10 10:08:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-09-10 10:08:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:08:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=97, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:08:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-09-10 10:08:19 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=193, loss_sum=134.688141, avg_loss=0.697866, seen=193, correct=107, accuracy=0.554404
2025-09-10 10:08:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:08:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:08:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:08:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:08:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:08:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:08:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:08:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:08:23 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.851562, avg_loss=0.621289, seen=40, correct=24, accuracy=0.600000
2025-09-10 10:08:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:08:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:08:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:08:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:08:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:08:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:08:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:08:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:08:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:08:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:08:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:08:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:08:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:08:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:08:32 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-10 10:08:32 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-10 10:08:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-09-10 10:08:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:08:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=97, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:08:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-09-10 10:08:35 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=193, loss_sum=134.824127, avg_loss=0.698571, seen=193, correct=106, accuracy=0.549223
2025-09-10 10:08:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:08:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:08:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:08:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:08:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:08:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:08:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:08:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:08:40 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.384449, avg_loss=0.634611, seen=40, correct=27, accuracy=0.675000
2025-09-10 10:08:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:08:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:08:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:08:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:08:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:08:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:08:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:08:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:08:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:08:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:08:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:08:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:08:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:08:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:08:50 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-10 10:08:50 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-10 10:08:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-09-10 10:08:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:08:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=97, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:08:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-09-10 10:08:53 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=193, loss_sum=137.870468, avg_loss=0.714355, seen=193, correct=106, accuracy=0.549223
2025-09-10 10:08:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:08:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:08:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:08:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:08:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:08:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:08:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:08:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:08:58 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.681770, avg_loss=0.617044, seen=40, correct=28, accuracy=0.700000
2025-09-10 10:08:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:08:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:08:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:09:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:09:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=31 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:09:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=32 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:09:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=33 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:09:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=34 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:09:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=35 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:09:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=36 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:09:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=37 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:09:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=38 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:09:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=39 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:09:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=40 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:09:07 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-10 10:09:08 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-10 10:09:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-09-10 10:09:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:09:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=97, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:09:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-09-10 10:09:11 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=193, loss_sum=136.583893, avg_loss=0.707689, seen=193, correct=105, accuracy=0.544041
2025-09-10 10:09:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:09:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:09:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:09:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:09:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:09:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:09:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:09:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:09:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.162451, avg_loss=0.629061, seen=40, correct=27, accuracy=0.675000
2025-09-10 10:09:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:09:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:09:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:09:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:09:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=41 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:09:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=42 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:09:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=43 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:09:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=44 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:09:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=45 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:09:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=46 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:09:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=47 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:09:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=48 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:09:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=49 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:09:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=50 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:09:28 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-10 10:09:28 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-10 10:09:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-09-10 10:09:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:09:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=97, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:09:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-09-10 10:09:31 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=193, loss_sum=133.584030, avg_loss=0.692145, seen=193, correct=110, accuracy=0.569948
2025-09-10 10:09:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:09:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:09:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:09:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:09:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:09:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:09:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:09:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:09:40 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.200548, avg_loss=0.655014, seen=40, correct=23, accuracy=0.575000
2025-09-10 10:09:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:09:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:09:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:09:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:09:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=51 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:09:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=52 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:09:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=53 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:09:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=54 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:09:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=55 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:09:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=56 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:09:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=57 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:09:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=58 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:09:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=59 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:09:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=60 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:09:49 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-10 10:09:50 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-10 10:09:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-09-10 10:09:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:09:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=97, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:09:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-09-10 10:09:53 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=193, loss_sum=132.902100, avg_loss=0.688612, seen=193, correct=110, accuracy=0.569948
2025-09-10 10:09:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:09:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:09:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:09:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:09:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:09:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:09:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:09:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:09:59 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.335047, avg_loss=0.633376, seen=40, correct=23, accuracy=0.575000
2025-09-10 10:09:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:09:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:10:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:10:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:10:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=61 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:10:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=62 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:10:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=63 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:10:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=64 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:10:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=65 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:10:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=66 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:10:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=67 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:10:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=68 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:10:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=69 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:10:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=70 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:10:11 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-10 10:10:11 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-10 10:10:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-09-10 10:10:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:10:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=97, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:10:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-09-10 10:10:14 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=193, loss_sum=136.326309, avg_loss=0.706354, seen=193, correct=100, accuracy=0.518135
2025-09-10 10:10:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:10:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:10:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:10:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:10:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:10:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:10:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:10:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:10:20 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.829710, avg_loss=0.620743, seen=40, correct=26, accuracy=0.650000
2025-09-10 10:10:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:10:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:10:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:10:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:10:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=71 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:10:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=72 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:10:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=73 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:10:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=74 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:10:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=75 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:10:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=76 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:10:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=77 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:10:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=78 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:10:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=79 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:10:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=80 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:10:29 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-10 10:10:30 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-10 10:10:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-09-10 10:10:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:10:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=97, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:10:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-09-10 10:10:32 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=193, loss_sum=139.980026, avg_loss=0.725285, seen=193, correct=85, accuracy=0.440415
2025-09-10 10:10:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:10:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:10:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:10:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:10:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:10:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:10:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:10:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:10:35 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.829350, avg_loss=0.620734, seen=40, correct=24, accuracy=0.600000
2025-09-10 10:10:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:10:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:10:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:10:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:10:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=81 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:10:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=82 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:10:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=83 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:10:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=84 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:10:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=85 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:10:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=86 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:10:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=87 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:10:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=88 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:10:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=89 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:10:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=90 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:10:45 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-10 10:10:45 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-10 10:10:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-09-10 10:10:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:10:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=97, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:10:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-09-10 10:10:48 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=193, loss_sum=136.141022, avg_loss=0.705394, seen=193, correct=97, accuracy=0.502591
2025-09-10 10:10:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:10:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:10:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:10:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:10:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:10:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:10:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:10:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:10:54 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.833681, avg_loss=0.645842, seen=40, correct=25, accuracy=0.625000
2025-09-10 10:10:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:10:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:10:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:10:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:10:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=91 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:10:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=92 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:10:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=93 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:11:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=94 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:11:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=95 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:11:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=96 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:11:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=97 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:11:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=98 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:11:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=99 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:11:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=100 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:11:03 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-10 10:11:05 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-10 10:11:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-09-10 10:11:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:11:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=97, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:11:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-09-10 10:11:08 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=193, loss_sum=133.645386, avg_loss=0.692463, seen=193, correct=111, accuracy=0.575130
2025-09-10 10:11:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:11:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:11:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:11:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:11:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:11:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:11:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:11:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:11:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.039877, avg_loss=0.625997, seen=40, correct=24, accuracy=0.600000
2025-09-10 10:11:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:11:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:11:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:11:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:11:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-10 10:11:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-10 10:11:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:11:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:11:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:11:21 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #31', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-10 10:11:21 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #31', 'Round': 0, 'Results_raw': {}}
2025-09-10 10:11:21 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 10:11:21 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-10 10:11:22 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-10 10:11:22 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-10 10:11:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:11:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:11:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:11:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:11:26 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=130.153564, avg_loss=0.650768, seen=200, correct=120, accuracy=0.600000
2025-09-10 10:11:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:11:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:11:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:11:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 10:11:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:11:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:11:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:11:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:11:28 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.033464, avg_loss=0.625837, seen=40, correct=26, accuracy=0.650000
2025-09-10 10:11:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:11:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:11:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:11:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 10:11:30 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-10 10:11:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1543, total=6171)
2025-09-10 10:11:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:11:30 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-10 10:11:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:11:30 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=772, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-10 10:11:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:11:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:11:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:11:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:11:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:11:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:11:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:11:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:11:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:11:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:11:37 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-10 10:11:40 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-10 10:11:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:11:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:11:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:11:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:11:43 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=133.943420, avg_loss=0.669717, seen=200, correct=119, accuracy=0.595000
2025-09-10 10:11:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:11:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:11:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:11:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:11:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:11:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:11:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:11:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:11:47 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.187891, avg_loss=0.629697, seen=40, correct=26, accuracy=0.650000
2025-09-10 10:11:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:11:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:11:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:11:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:11:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:11:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:11:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:11:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:11:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:11:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:11:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:11:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:11:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:11:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:11:57 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-10 10:11:57 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-10 10:11:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:11:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:11:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:12:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:12:00 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=132.888504, avg_loss=0.664443, seen=200, correct=117, accuracy=0.585000
2025-09-10 10:12:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:12:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:12:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:12:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:12:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:12:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:12:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:12:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:12:10 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.420258, avg_loss=0.635506, seen=40, correct=26, accuracy=0.650000
2025-09-10 10:12:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:12:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:12:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:12:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:12:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:12:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:12:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:12:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:12:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:12:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:12:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:12:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:12:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:12:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:12:19 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-10 10:12:19 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-10 10:12:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:12:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:12:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:12:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:12:22 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=131.058929, avg_loss=0.655295, seen=200, correct=128, accuracy=0.640000
2025-09-10 10:12:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:12:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:12:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:12:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:12:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:12:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:12:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:12:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:12:27 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.700918, avg_loss=0.642523, seen=40, correct=22, accuracy=0.550000
2025-09-10 10:12:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:12:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:12:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:12:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:12:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=31 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:12:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=32 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:12:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=33 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:12:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=34 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:12:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=35 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:12:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=36 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:12:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=37 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:12:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=38 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:12:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=39 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:12:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=40 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:12:38 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-10 10:12:38 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-10 10:12:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:12:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:12:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:12:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:12:41 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=129.749084, avg_loss=0.648745, seen=200, correct=130, accuracy=0.650000
2025-09-10 10:12:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:12:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:12:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:12:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:12:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:12:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:12:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:12:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:12:45 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.974789, avg_loss=0.649370, seen=40, correct=21, accuracy=0.525000
2025-09-10 10:12:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:12:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:12:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:12:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:12:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=41 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:12:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=42 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:12:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=43 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:12:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=44 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:12:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=45 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:12:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=46 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:12:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=47 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:12:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=48 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:12:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=49 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:12:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=50 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:12:55 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-10 10:12:57 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-10 10:12:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:12:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:12:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:13:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:13:00 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=130.209106, avg_loss=0.651046, seen=200, correct=125, accuracy=0.625000
2025-09-10 10:13:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:13:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:13:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:13:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:13:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:13:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:13:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:13:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:13:03 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.527706, avg_loss=0.663193, seen=40, correct=18, accuracy=0.450000
2025-09-10 10:13:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:13:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:13:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:13:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:13:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=51 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:13:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=52 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:13:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=53 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:13:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=54 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:13:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=55 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:13:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=56 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:13:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=57 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:13:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=58 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:13:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=59 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:13:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=60 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:13:12 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-10 10:13:13 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-10 10:13:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:13:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:13:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:13:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:13:16 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=132.544754, avg_loss=0.662724, seen=200, correct=125, accuracy=0.625000
2025-09-10 10:13:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:13:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:13:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:13:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:13:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:13:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:13:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:13:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:13:21 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.677723, avg_loss=0.666943, seen=40, correct=21, accuracy=0.525000
2025-09-10 10:13:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:13:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:13:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:13:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:13:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=61 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:13:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=62 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:13:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=63 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:13:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=64 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:13:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=65 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:13:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=66 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:13:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=67 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:13:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=68 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:13:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=69 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:13:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=70 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:13:30 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-10 10:13:31 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-10 10:13:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:13:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:13:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:13:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:13:34 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=136.933670, avg_loss=0.684668, seen=200, correct=116, accuracy=0.580000
2025-09-10 10:13:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:13:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:13:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:13:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:13:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:13:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:13:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:13:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:13:38 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.505060, avg_loss=0.662627, seen=40, correct=26, accuracy=0.650000
2025-09-10 10:13:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:13:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:13:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:13:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:13:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=71 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:13:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=72 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:13:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=73 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:13:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=74 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:13:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=75 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:13:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=76 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:13:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=77 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:13:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=78 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:13:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=79 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:13:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=80 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:13:48 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-10 10:13:48 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-10 10:13:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:13:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:13:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:13:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:13:51 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=134.428833, avg_loss=0.672144, seen=200, correct=116, accuracy=0.580000
2025-09-10 10:13:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:13:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:13:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:13:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:13:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:13:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:13:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:13:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:13:59 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.829033, avg_loss=0.645726, seen=40, correct=24, accuracy=0.600000
2025-09-10 10:13:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:13:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:14:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:14:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:14:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=81 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:14:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=82 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:14:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=83 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:14:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=84 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:14:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=85 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:14:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=86 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:14:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=87 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:14:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=88 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:14:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=89 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:14:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=90 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:14:07 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-10 10:14:08 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-10 10:14:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:14:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:14:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:14:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:14:11 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=129.232056, avg_loss=0.646160, seen=200, correct=129, accuracy=0.645000
2025-09-10 10:14:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:14:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:14:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:14:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:14:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:14:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:14:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:14:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:14:15 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.774298, avg_loss=0.644357, seen=40, correct=21, accuracy=0.525000
2025-09-10 10:14:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:14:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:14:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:14:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:14:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=91 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:14:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=92 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:14:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=93 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:14:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=94 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:14:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=95 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:14:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=96 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:14:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=97 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:14:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=98 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:14:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=99 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:14:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=100 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:14:26 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-10 10:14:27 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-10 10:14:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:14:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:14:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:14:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:14:30 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=128.737335, avg_loss=0.643687, seen=200, correct=123, accuracy=0.615000
2025-09-10 10:14:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:14:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:14:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:14:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:14:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:14:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:14:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:14:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:14:35 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.558903, avg_loss=0.663973, seen=40, correct=20, accuracy=0.500000
2025-09-10 10:14:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:14:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:14:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:14:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:14:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-10 10:14:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-10 10:14:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:14:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:14:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:14:38 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #38', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-10 10:14:38 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #38', 'Round': 0, 'Results_raw': {}}
2025-09-10 10:14:38 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 10:14:38 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-10 10:14:39 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-10 10:14:39 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-10 10:14:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-09-10 10:14:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:14:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=15, num_train_batch_last_epoch=10, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:14:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-10 10:14:41 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=30, loss_sum=18.069229, avg_loss=0.602308, seen=30, correct=20, accuracy=0.666667
2025-09-10 10:14:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:14:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:14:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:14:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 10:14:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:14:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:14:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:14:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:14:45 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.220098, avg_loss=0.605502, seen=40, correct=24, accuracy=0.600000
2025-09-10 10:14:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:14:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:14:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:14:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 10:14:47 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-10 10:14:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=146, total=583)
2025-09-10 10:14:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:14:48 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-10 10:14:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:14:48 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=73, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-10 10:14:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:14:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:14:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:14:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:14:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:14:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:14:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:14:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:14:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:14:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:14:54 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-10 10:14:56 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-10 10:14:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-09-10 10:14:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:14:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=15, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:14:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-10 10:14:57 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=30, loss_sum=18.368668, avg_loss=0.612289, seen=30, correct=19, accuracy=0.633333
2025-09-10 10:14:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:14:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:14:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:15:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:15:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:15:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:15:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:15:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:15:01 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.629204, avg_loss=0.615730, seen=40, correct=25, accuracy=0.625000
2025-09-10 10:15:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:15:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:15:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:15:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:15:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:15:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:15:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:15:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:15:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:15:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:15:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:15:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:15:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:15:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:15:13 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-10 10:15:15 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-10 10:15:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-09-10 10:15:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:15:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=15, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:15:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-10 10:15:16 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=30, loss_sum=18.826321, avg_loss=0.627544, seen=30, correct=18, accuracy=0.600000
2025-09-10 10:15:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:15:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:15:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:15:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:15:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:15:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:15:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:15:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:15:19 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.743401, avg_loss=0.593585, seen=40, correct=26, accuracy=0.650000
2025-09-10 10:15:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:15:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:15:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:15:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:15:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:15:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:15:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:15:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:15:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:15:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:15:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:15:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:15:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:15:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:15:30 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-10 10:15:31 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-10 10:15:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-09-10 10:15:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:15:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=15, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:15:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-10 10:15:32 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=30, loss_sum=17.989010, avg_loss=0.599634, seen=30, correct=20, accuracy=0.666667
2025-09-10 10:15:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:15:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:15:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:15:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:15:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:15:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:15:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:15:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:15:35 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.654366, avg_loss=0.591359, seen=40, correct=28, accuracy=0.700000
2025-09-10 10:15:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:15:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:15:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:15:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:15:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=31 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:15:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=32 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:15:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=33 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:15:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=34 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:15:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=35 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:15:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=36 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:15:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=37 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:15:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=38 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:15:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=39 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:15:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=40 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:15:44 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-10 10:15:46 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-10 10:15:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-09-10 10:15:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:15:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=15, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:15:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-10 10:15:46 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=30, loss_sum=17.719707, avg_loss=0.590657, seen=30, correct=20, accuracy=0.666667
2025-09-10 10:15:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:15:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:15:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:15:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:15:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:15:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:15:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:15:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:15:51 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.296631, avg_loss=0.607416, seen=40, correct=26, accuracy=0.650000
2025-09-10 10:15:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:15:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:15:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:15:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:15:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=41 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:15:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=42 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:15:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=43 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:15:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=44 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:15:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=45 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:15:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=46 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:15:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=47 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:16:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=48 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:16:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=49 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:16:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=50 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:16:01 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-10 10:16:02 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-10 10:16:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-09-10 10:16:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:16:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=15, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:16:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-10 10:16:03 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=30, loss_sum=17.651287, avg_loss=0.588376, seen=30, correct=21, accuracy=0.700000
2025-09-10 10:16:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:16:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:16:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:16:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:16:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:16:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:16:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:16:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:16:07 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.186707, avg_loss=0.604668, seen=40, correct=28, accuracy=0.700000
2025-09-10 10:16:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:16:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:16:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:16:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:16:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=51 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:16:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=52 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:16:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=53 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:16:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=54 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:16:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=55 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:16:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=56 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:16:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=57 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:16:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=58 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:16:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=59 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:16:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=60 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:16:16 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-10 10:16:19 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-10 10:16:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-09-10 10:16:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:16:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=15, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:16:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-10 10:16:19 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=30, loss_sum=18.163048, avg_loss=0.605435, seen=30, correct=18, accuracy=0.600000
2025-09-10 10:16:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:16:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:16:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:16:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:16:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:16:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:16:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:16:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:16:23 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.510918, avg_loss=0.587773, seen=40, correct=27, accuracy=0.675000
2025-09-10 10:16:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:16:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:16:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:16:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:16:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=61 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:16:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=62 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:16:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=63 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:16:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=64 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:16:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=65 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:16:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=66 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:16:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=67 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:16:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=68 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:16:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=69 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:16:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=70 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:16:31 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-10 10:16:32 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-10 10:16:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-09-10 10:16:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:16:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=15, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:16:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-10 10:16:32 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=30, loss_sum=17.857975, avg_loss=0.595266, seen=30, correct=21, accuracy=0.700000
2025-09-10 10:16:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:16:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:16:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:16:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:16:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:16:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:16:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:16:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:16:38 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.751076, avg_loss=0.593777, seen=40, correct=29, accuracy=0.725000
2025-09-10 10:16:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:16:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:16:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:16:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:16:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=71 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:16:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=72 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:16:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=73 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:16:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=74 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:16:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=75 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:16:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=76 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:16:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=77 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:16:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=78 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:16:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=79 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:16:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=80 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:16:49 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-10 10:16:51 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-10 10:16:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-09-10 10:16:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:16:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=15, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:16:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-10 10:16:51 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=30, loss_sum=17.677229, avg_loss=0.589241, seen=30, correct=20, accuracy=0.666667
2025-09-10 10:16:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:16:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:16:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:16:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:16:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:16:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:16:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:16:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:16:55 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.166512, avg_loss=0.604163, seen=40, correct=28, accuracy=0.700000
2025-09-10 10:16:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:16:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:16:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:16:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:16:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=81 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:17:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=82 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:17:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=83 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:17:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=84 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:17:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=85 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:17:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=86 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:17:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=87 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:17:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=88 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:17:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=89 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:17:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=90 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:17:05 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-10 10:17:06 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-10 10:17:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-09-10 10:17:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:17:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=15, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:17:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-10 10:17:06 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=30, loss_sum=17.663662, avg_loss=0.588789, seen=30, correct=20, accuracy=0.666667
2025-09-10 10:17:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:17:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:17:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:17:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:17:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:17:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:17:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:17:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:17:11 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.633087, avg_loss=0.590827, seen=40, correct=28, accuracy=0.700000
2025-09-10 10:17:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:17:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:17:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:17:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:17:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=91 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:17:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=92 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:17:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=93 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:17:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=94 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:17:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=95 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:17:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=96 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:17:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=97 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:17:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=98 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:17:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=99 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:17:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=100 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:17:21 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-10 10:17:21 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-10 10:17:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-09-10 10:17:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:17:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=15, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:17:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-10 10:17:22 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=30, loss_sum=17.935484, avg_loss=0.597849, seen=30, correct=20, accuracy=0.666667
2025-09-10 10:17:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:17:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:17:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:17:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:17:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:17:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:17:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:17:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:17:26 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.279776, avg_loss=0.581994, seen=40, correct=28, accuracy=0.700000
2025-09-10 10:17:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:17:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:17:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:17:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:17:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-10 10:17:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-10 10:17:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:17:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:17:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 10:17:29 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #23', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-10 10:17:29 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #23', 'Round': 0, 'Results_raw': {}}
2025-09-10 10:17:29 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 10:17:29 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-10 10:17:29 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-10 10:17:29 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-10 10:17:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-09-10 10:17:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:17:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=35, num_train_batch_last_epoch=30, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:17:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-10 10:17:31 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=69, loss_sum=45.597900, avg_loss=0.660839, seen=69, correct=42, accuracy=0.608696
2025-09-10 10:17:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:17:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:17:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:17:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 10:17:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:17:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:17:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:17:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:17:36 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.644970, avg_loss=0.766124, seen=40, correct=19, accuracy=0.475000
2025-09-10 10:17:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:17:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:17:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:17:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 10:17:37 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-10 10:17:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=329, total=1316)
2025-09-10 10:17:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:17:37 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-10 10:17:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:17:37 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=165, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-10 10:17:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:17:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:17:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:17:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:17:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:17:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:17:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:17:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:17:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:17:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:17:44 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-10 10:17:44 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-10 10:17:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-09-10 10:17:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:17:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=35, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:17:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-10 10:17:46 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=69, loss_sum=50.502316, avg_loss=0.731918, seen=69, correct=35, accuracy=0.507246
2025-09-10 10:17:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:17:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:17:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:17:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:17:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:17:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:17:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:17:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:17:50 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.942602, avg_loss=0.748565, seen=40, correct=23, accuracy=0.575000
2025-09-10 10:17:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:17:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:17:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:17:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:17:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:17:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:17:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:17:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:17:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:17:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:17:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:17:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:17:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:17:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:17:59 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-10 10:18:00 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-10 10:18:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-09-10 10:18:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:18:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=35, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:18:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-10 10:18:01 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=69, loss_sum=49.480968, avg_loss=0.717115, seen=69, correct=38, accuracy=0.550725
2025-09-10 10:18:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:18:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:18:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:18:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:18:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:18:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:18:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:18:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:18:07 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.187511, avg_loss=0.754688, seen=40, correct=21, accuracy=0.525000
2025-09-10 10:18:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:18:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:18:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:18:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:18:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:18:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:18:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:18:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:18:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:18:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:18:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:18:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:18:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:18:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:18:15 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-10 10:18:15 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-10 10:18:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-09-10 10:18:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:18:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=35, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:18:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-10 10:18:17 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=69, loss_sum=46.911846, avg_loss=0.679882, seen=69, correct=39, accuracy=0.565217
2025-09-10 10:18:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:18:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:18:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:18:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:18:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:18:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:18:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:18:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:18:21 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.610596, avg_loss=0.740265, seen=40, correct=15, accuracy=0.375000
2025-09-10 10:18:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:18:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:18:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:18:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:18:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=31 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:18:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=32 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:18:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=33 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:18:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=34 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:18:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=35 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:18:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=36 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:18:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=37 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:18:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=38 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:18:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=39 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:18:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=40 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:18:31 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-10 10:18:32 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-10 10:18:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-09-10 10:18:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:18:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=35, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:18:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-10 10:18:33 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=69, loss_sum=45.562237, avg_loss=0.660322, seen=69, correct=37, accuracy=0.536232
2025-09-10 10:18:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:18:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:18:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:18:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:18:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:18:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:18:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:18:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:18:36 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.021616, avg_loss=0.750540, seen=40, correct=16, accuracy=0.400000
2025-09-10 10:18:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:18:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:18:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:18:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:18:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=41 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:18:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=42 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:18:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=43 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:18:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=44 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:18:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=45 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:18:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=46 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:18:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=47 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:18:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=48 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:18:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=49 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:18:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=50 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:18:45 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-10 10:18:46 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-10 10:18:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-09-10 10:18:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:18:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=35, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:18:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-10 10:18:47 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=69, loss_sum=45.152824, avg_loss=0.654389, seen=69, correct=41, accuracy=0.594203
2025-09-10 10:18:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:18:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:18:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:18:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:18:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:18:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:18:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:18:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:18:51 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.190262, avg_loss=0.754757, seen=40, correct=18, accuracy=0.450000
2025-09-10 10:18:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:18:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:18:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:18:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:18:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=51 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:18:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=52 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:18:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=53 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:18:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=54 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:18:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=55 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:18:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=56 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:18:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=57 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:18:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=58 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:18:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=59 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:18:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=60 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:18:58 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-10 10:18:59 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-10 10:19:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-09-10 10:19:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:19:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=35, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:19:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-10 10:19:01 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=69, loss_sum=46.809502, avg_loss=0.678399, seen=69, correct=44, accuracy=0.637681
2025-09-10 10:19:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:19:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:19:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:19:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:19:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:19:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:19:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:19:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:19:08 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.706120, avg_loss=0.767653, seen=40, correct=18, accuracy=0.450000
2025-09-10 10:19:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:19:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:19:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:19:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:19:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=61 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:19:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=62 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:19:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=63 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:19:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=64 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:19:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=65 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:19:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=66 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:19:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=67 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:19:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=68 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:19:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=69 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:19:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=70 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:19:17 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-10 10:19:17 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-10 10:19:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-09-10 10:19:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:19:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=35, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:19:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-10 10:19:18 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=69, loss_sum=48.717304, avg_loss=0.706048, seen=69, correct=41, accuracy=0.594203
2025-09-10 10:19:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:19:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:19:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:19:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:19:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:19:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:19:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:19:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:19:22 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.789864, avg_loss=0.769747, seen=40, correct=19, accuracy=0.475000
2025-09-10 10:19:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:19:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:19:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:19:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:19:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=71 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:19:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=72 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:19:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=73 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:19:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=74 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:19:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=75 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:19:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=76 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:19:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=77 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:19:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=78 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:19:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=79 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:19:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=80 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:19:32 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-10 10:19:33 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-10 10:19:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-09-10 10:19:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:19:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=35, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:19:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-10 10:19:34 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=69, loss_sum=47.512165, avg_loss=0.688582, seen=69, correct=43, accuracy=0.623188
2025-09-10 10:19:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:19:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:19:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:19:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:19:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:19:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:19:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:19:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:19:38 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.688065, avg_loss=0.742202, seen=40, correct=21, accuracy=0.525000
2025-09-10 10:19:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:19:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:19:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:19:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:19:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=81 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:19:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=82 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:19:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=83 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:19:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=84 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:19:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=85 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:19:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=86 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:19:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=87 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:19:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=88 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:19:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=89 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:19:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=90 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:19:47 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-10 10:19:48 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-10 10:19:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-09-10 10:19:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:19:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=35, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:19:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-10 10:19:49 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=69, loss_sum=46.885761, avg_loss=0.679504, seen=69, correct=44, accuracy=0.637681
2025-09-10 10:19:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:19:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:19:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:19:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:19:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:19:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:19:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:19:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:19:52 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.349148, avg_loss=0.733729, seen=40, correct=20, accuracy=0.500000
2025-09-10 10:19:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:19:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:19:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:19:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:19:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=91 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:19:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=92 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:19:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=93 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:19:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=94 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:19:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=95 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:19:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=96 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:20:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=97 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:20:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=98 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:20:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=99 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:20:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=100 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:20:02 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-10 10:20:02 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-10 10:20:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-09-10 10:20:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:20:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=35, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:20:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-10 10:20:04 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=69, loss_sum=47.664738, avg_loss=0.690793, seen=69, correct=35, accuracy=0.507246
2025-09-10 10:20:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:20:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:20:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:20:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:20:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:20:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:20:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:20:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:20:10 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.635563, avg_loss=0.740889, seen=40, correct=18, accuracy=0.450000
2025-09-10 10:20:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:20:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:20:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:20:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:20:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-10 10:20:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-10 10:20:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:20:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:20:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:20:13 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #8', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-10 10:20:13 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #8', 'Round': 0, 'Results_raw': {}}
2025-09-10 10:20:13 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 10:20:13 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-10 10:20:14 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-10 10:20:14 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-10 10:20:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:20:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:20:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:20:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:20:17 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=127.835892, avg_loss=0.639179, seen=200, correct=126, accuracy=0.630000
2025-09-10 10:20:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:20:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:20:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:20:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 10:20:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:20:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:20:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:20:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:20:19 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.584122, avg_loss=0.689603, seen=40, correct=26, accuracy=0.650000
2025-09-10 10:20:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:20:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:20:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:20:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 10:20:21 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-10 10:20:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3638, total=14550)
2025-09-10 10:20:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:20:21 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-10 10:20:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:20:21 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=1819, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-10 10:20:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:20:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:20:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:20:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:20:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:20:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:20:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:20:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:20:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:20:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:20:29 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-10 10:20:32 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-10 10:20:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:20:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:20:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:20:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:20:35 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=128.286011, avg_loss=0.641430, seen=200, correct=128, accuracy=0.640000
2025-09-10 10:20:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:20:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:20:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:20:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:20:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:20:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:20:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:20:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:20:38 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.262087, avg_loss=0.706552, seen=40, correct=22, accuracy=0.550000
2025-09-10 10:20:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:20:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:20:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:20:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:20:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:20:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:20:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:20:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:20:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:20:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:20:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:20:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:20:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:20:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:20:45 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-10 10:20:46 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-10 10:20:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:20:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:20:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:20:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:20:49 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=129.569916, avg_loss=0.647850, seen=200, correct=129, accuracy=0.645000
2025-09-10 10:20:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:20:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:20:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:20:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:20:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:20:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:20:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:20:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:20:54 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.414087, avg_loss=0.685352, seen=40, correct=27, accuracy=0.675000
2025-09-10 10:20:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:20:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:20:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:20:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:20:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:20:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:20:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:20:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:20:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:20:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:21:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:21:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:21:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:21:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:21:02 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-10 10:21:02 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-10 10:21:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:21:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:21:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:21:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:21:05 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=131.729080, avg_loss=0.658645, seen=200, correct=124, accuracy=0.620000
2025-09-10 10:21:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:21:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:21:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:21:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:21:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:21:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:21:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:21:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:21:09 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.969137, avg_loss=0.674228, seen=40, correct=24, accuracy=0.600000
2025-09-10 10:21:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:21:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:21:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:21:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:21:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=31 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:21:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=32 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:21:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=33 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:21:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=34 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:21:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=35 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:21:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=36 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:21:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=37 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:21:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=38 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:21:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=39 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:21:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=40 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:21:18 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-10 10:21:18 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-10 10:21:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:21:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:21:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:21:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:21:21 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.717285, avg_loss=0.678586, seen=200, correct=112, accuracy=0.560000
2025-09-10 10:21:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:21:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:21:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:21:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:21:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:21:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:21:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:21:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:21:24 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.850605, avg_loss=0.671265, seen=40, correct=24, accuracy=0.600000
2025-09-10 10:21:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:21:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:21:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:21:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:21:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=41 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:21:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=42 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:21:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=43 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:21:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=44 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:21:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=45 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:21:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=46 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:21:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=47 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:21:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=48 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:21:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=49 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:21:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=50 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:21:33 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-10 10:21:33 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-10 10:21:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:21:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:21:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:21:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:21:36 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=132.281921, avg_loss=0.661410, seen=200, correct=117, accuracy=0.585000
2025-09-10 10:21:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:21:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:21:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:21:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:21:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:21:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:21:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:21:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:21:40 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.202374, avg_loss=0.655059, seen=40, correct=25, accuracy=0.625000
2025-09-10 10:21:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:21:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:21:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:21:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:21:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=51 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:21:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=52 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:21:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=53 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:21:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=54 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:21:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=55 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:21:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=56 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:21:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=57 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:21:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=58 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:21:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=59 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:21:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=60 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:21:51 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-10 10:21:52 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-10 10:21:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:21:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:21:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:21:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:21:55 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=131.903717, avg_loss=0.659519, seen=200, correct=122, accuracy=0.610000
2025-09-10 10:21:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:21:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:21:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:21:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:21:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:21:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:21:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:21:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:21:59 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.455900, avg_loss=0.661398, seen=40, correct=28, accuracy=0.700000
2025-09-10 10:21:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:21:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:22:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:22:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:22:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=61 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:22:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=62 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:22:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=63 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:22:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=64 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:22:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=65 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:22:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=66 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:22:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=67 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:22:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=68 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:22:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=69 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:22:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=70 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:22:09 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-10 10:22:10 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-10 10:22:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:22:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:22:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:22:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:22:13 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=131.846146, avg_loss=0.659231, seen=200, correct=118, accuracy=0.590000
2025-09-10 10:22:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:22:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:22:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:22:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:22:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:22:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:22:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:22:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:22:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.762375, avg_loss=0.694059, seen=40, correct=24, accuracy=0.600000
2025-09-10 10:22:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:22:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:22:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:22:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:22:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=71 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:22:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=72 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:22:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=73 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:22:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=74 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:22:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=75 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:22:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=76 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:22:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=77 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:22:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=78 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:22:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=79 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:22:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=80 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:22:29 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-10 10:22:29 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-10 10:22:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:22:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:22:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:22:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:22:32 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=132.213501, avg_loss=0.661068, seen=200, correct=124, accuracy=0.620000
2025-09-10 10:22:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:22:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:22:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:22:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:22:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:22:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:22:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:22:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:22:36 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.091785, avg_loss=0.702295, seen=40, correct=24, accuracy=0.600000
2025-09-10 10:22:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:22:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:22:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:22:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:22:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=81 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:22:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=82 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:22:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=83 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:22:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=84 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:22:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=85 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:22:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=86 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:22:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=87 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:22:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=88 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:22:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=89 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:22:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=90 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:22:47 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-10 10:22:47 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-10 10:22:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:22:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:22:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:22:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:22:50 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=131.384613, avg_loss=0.656923, seen=200, correct=124, accuracy=0.620000
2025-09-10 10:22:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:22:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:22:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:22:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:22:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:22:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:22:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:22:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:22:55 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.970610, avg_loss=0.674265, seen=40, correct=26, accuracy=0.650000
2025-09-10 10:22:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:22:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:22:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:22:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:22:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=91 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:23:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=92 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:23:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=93 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:23:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=94 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:23:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=95 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:23:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=96 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:23:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=97 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:23:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=98 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:23:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=99 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:23:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=100 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:23:07 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-10 10:23:07 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-10 10:23:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:23:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:23:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:23:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:23:10 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=128.416092, avg_loss=0.642080, seen=200, correct=123, accuracy=0.615000
2025-09-10 10:23:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:23:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:23:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:23:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:23:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:23:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:23:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:23:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:23:15 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.613754, avg_loss=0.665344, seen=40, correct=27, accuracy=0.675000
2025-09-10 10:23:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:23:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:23:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:23:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:23:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-10 10:23:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-10 10:23:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:23:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:23:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 10:23:18 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #15', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-10 10:23:18 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #15', 'Round': 0, 'Results_raw': {}}
2025-09-10 10:23:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 10:23:18 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-10 10:23:19 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-10 10:23:19 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-10 10:23:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:23:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:23:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:23:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:23:23 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=128.196396, avg_loss=0.640982, seen=200, correct=124, accuracy=0.620000
2025-09-10 10:23:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:23:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:23:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:23:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 10:23:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:23:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:23:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:23:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:23:27 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.239861, avg_loss=0.655997, seen=40, correct=28, accuracy=0.700000
2025-09-10 10:23:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:23:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:23:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:23:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 10:23:29 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-10 10:23:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1184, total=4736)
2025-09-10 10:23:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:23:29 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-10 10:23:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:23:29 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=592, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-10 10:23:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:23:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:23:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:23:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:23:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:23:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:23:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:23:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:23:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:23:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:23:37 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-10 10:23:37 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-10 10:23:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:23:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:23:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:23:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:23:40 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=130.797455, avg_loss=0.653987, seen=200, correct=122, accuracy=0.610000
2025-09-10 10:23:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:23:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:23:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:23:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 10:23:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:23:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:23:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:23:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:23:45 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.616518, avg_loss=0.665413, seen=40, correct=22, accuracy=0.550000
2025-09-10 10:23:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:23:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:23:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:23:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 10:23:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:23:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:23:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:23:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:23:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:23:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:23:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:23:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:23:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:23:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:23:53 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-10 10:23:55 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-10 10:23:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:23:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:23:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:23:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:23:58 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=130.027283, avg_loss=0.650136, seen=200, correct=127, accuracy=0.635000
2025-09-10 10:23:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:23:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:24:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:24:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 10:24:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:24:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:24:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:24:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:24:04 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.679228, avg_loss=0.666981, seen=40, correct=25, accuracy=0.625000
2025-09-10 10:24:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:24:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:24:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:24:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 10:24:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:24:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:24:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:24:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:24:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:24:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:24:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:24:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:24:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:24:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:24:14 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-10 10:24:14 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-10 10:24:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:24:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:24:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:24:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:24:17 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=127.990112, avg_loss=0.639951, seen=200, correct=123, accuracy=0.615000
2025-09-10 10:24:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:24:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:24:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:24:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 10:24:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:24:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:24:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:24:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:24:21 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.056881, avg_loss=0.651422, seen=40, correct=25, accuracy=0.625000
2025-09-10 10:24:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:24:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:24:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:24:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 10:24:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=31 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:24:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=32 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:24:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=33 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:24:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=34 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:24:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=35 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:24:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=36 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:24:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=37 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:24:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=38 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:24:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=39 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:24:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=40 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:24:30 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-10 10:24:31 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-10 10:24:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:24:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:24:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:24:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:24:34 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=127.731049, avg_loss=0.638655, seen=200, correct=129, accuracy=0.645000
2025-09-10 10:24:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:24:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:24:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:24:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 10:24:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:24:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:24:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:24:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:24:39 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.684334, avg_loss=0.667108, seen=40, correct=26, accuracy=0.650000
2025-09-10 10:24:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:24:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:24:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:24:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 10:24:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=41 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:24:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=42 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:24:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=43 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:24:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=44 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:24:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=45 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:24:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=46 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:24:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=47 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:24:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=48 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:24:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=49 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:24:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=50 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:24:48 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-10 10:24:48 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-10 10:24:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:24:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:24:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:24:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:24:51 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=126.257202, avg_loss=0.631286, seen=200, correct=127, accuracy=0.635000
2025-09-10 10:24:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:24:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:24:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:24:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 10:24:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:24:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:24:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:24:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:24:57 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.351400, avg_loss=0.658785, seen=40, correct=25, accuracy=0.625000
2025-09-10 10:24:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:24:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:25:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:25:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 10:25:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=51 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:25:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=52 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:25:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=53 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:25:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=54 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:25:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=55 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:25:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=56 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:25:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=57 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:25:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=58 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:25:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=59 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:25:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=60 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:25:08 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-10 10:25:08 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-10 10:25:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:25:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:25:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:25:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:25:11 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=128.285507, avg_loss=0.641428, seen=200, correct=126, accuracy=0.630000
2025-09-10 10:25:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:25:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:25:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:25:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 10:25:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:25:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:25:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:25:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:25:17 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.076321, avg_loss=0.651908, seen=40, correct=26, accuracy=0.650000
2025-09-10 10:25:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:25:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:25:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:25:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 10:25:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=61 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:25:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=62 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:25:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=63 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:25:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=64 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:25:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=65 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:25:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=66 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:25:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=67 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:25:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=68 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:25:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=69 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:25:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=70 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:25:27 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-10 10:25:27 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-10 10:25:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:25:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:25:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:25:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:25:30 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=130.337723, avg_loss=0.651689, seen=200, correct=123, accuracy=0.615000
2025-09-10 10:25:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:25:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:25:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:25:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 10:25:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:25:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:25:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:25:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:25:35 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.482887, avg_loss=0.687072, seen=40, correct=23, accuracy=0.575000
2025-09-10 10:25:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:25:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:25:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:25:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 10:25:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=71 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:25:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=72 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:25:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=73 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:25:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=74 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:25:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=75 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:25:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=76 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:25:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=77 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:25:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=78 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:25:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=79 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:25:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=80 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:25:44 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-10 10:25:45 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-10 10:25:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:25:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:25:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:25:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:25:48 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=129.338135, avg_loss=0.646691, seen=200, correct=127, accuracy=0.635000
2025-09-10 10:25:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:25:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:25:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:25:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 10:25:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:25:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:25:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:25:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:25:53 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.329659, avg_loss=0.683241, seen=40, correct=23, accuracy=0.575000
2025-09-10 10:25:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:25:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:25:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:25:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 10:25:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=81 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:25:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=82 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:25:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=83 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:25:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=84 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:25:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=85 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:25:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=86 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:26:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=87 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:26:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=88 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:26:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=89 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:26:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=90 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:26:01 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-10 10:26:03 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-10 10:26:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:26:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:26:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:26:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:26:06 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=129.345703, avg_loss=0.646729, seen=200, correct=126, accuracy=0.630000
2025-09-10 10:26:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:26:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:26:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:26:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 10:26:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:26:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:26:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:26:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:26:10 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.124914, avg_loss=0.678123, seen=40, correct=24, accuracy=0.600000
2025-09-10 10:26:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:26:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:26:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:26:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 10:26:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=91 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:26:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=92 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:26:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=93 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:26:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=94 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:26:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=95 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:26:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=96 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:26:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=97 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:26:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=98 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:26:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=99 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:26:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=100 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:26:18 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-10 10:26:22 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-10 10:26:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:26:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:26:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:26:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:26:25 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=129.482452, avg_loss=0.647412, seen=200, correct=128, accuracy=0.640000
2025-09-10 10:26:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:26:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:26:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:26:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 10:26:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:26:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:26:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:26:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:26:28 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.499096, avg_loss=0.687477, seen=40, correct=22, accuracy=0.550000
2025-09-10 10:26:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:26:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:26:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:26:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 10:26:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-10 10:26:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-10 10:26:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:26:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:26:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 10:26:30 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #35', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-10 10:26:30 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #35', 'Round': 0, 'Results_raw': {}}
2025-09-10 10:26:30 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 10:26:30 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-10 10:26:31 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-10 10:26:31 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-10 10:26:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-09-10 10:26:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:26:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=66, num_train_batch_last_epoch=34, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:26:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-09-10 10:26:34 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=132, loss_sum=85.844482, avg_loss=0.650337, seen=132, correct=85, accuracy=0.643939
2025-09-10 10:26:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:26:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:26:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:26:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 10:26:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:26:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:26:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:26:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:26:37 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=32.986794, avg_loss=0.824670, seen=40, correct=19, accuracy=0.475000
2025-09-10 10:26:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:26:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:26:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:26:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 10:26:38 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-10 10:26:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=631, total=2521)
2025-09-10 10:26:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:26:39 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-10 10:26:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:26:39 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=316, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-10 10:26:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:26:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:26:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:26:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:26:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:26:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:26:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:26:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:26:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:26:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:26:47 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-10 10:26:47 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-10 10:26:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-09-10 10:26:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:26:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=66, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:26:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-09-10 10:26:49 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=132, loss_sum=83.434563, avg_loss=0.632080, seen=132, correct=88, accuracy=0.666667
2025-09-10 10:26:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:26:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:26:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:26:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 10:26:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:26:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:26:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:26:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:26:54 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=32.404030, avg_loss=0.810101, seen=40, correct=17, accuracy=0.425000
2025-09-10 10:26:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:26:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:26:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:26:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 10:26:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:26:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:26:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:26:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:27:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:27:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:27:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:27:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:27:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:27:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:27:03 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-10 10:27:03 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-10 10:27:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-09-10 10:27:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:27:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=66, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:27:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-09-10 10:27:05 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=132, loss_sum=86.154839, avg_loss=0.652688, seen=132, correct=87, accuracy=0.659091
2025-09-10 10:27:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:27:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:27:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:27:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 10:27:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:27:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:27:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:27:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:27:10 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=32.177338, avg_loss=0.804433, seen=40, correct=19, accuracy=0.475000
2025-09-10 10:27:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:27:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:27:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:27:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 10:27:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:27:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:27:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:27:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:27:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:27:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:27:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:27:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:27:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:27:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:27:20 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-10 10:27:20 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-10 10:27:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-09-10 10:27:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:27:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=66, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:27:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-09-10 10:27:22 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=132, loss_sum=89.783691, avg_loss=0.680179, seen=132, correct=77, accuracy=0.583333
2025-09-10 10:27:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:27:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:27:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:27:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 10:27:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:27:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:27:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:27:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:27:27 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=32.080105, avg_loss=0.802003, seen=40, correct=19, accuracy=0.475000
2025-09-10 10:27:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:27:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:27:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:27:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 10:27:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=31 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:27:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=32 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:27:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=33 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:27:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=34 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:27:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=35 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:27:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=36 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:27:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=37 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:27:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=38 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:27:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=39 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:27:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=40 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:27:35 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-10 10:27:36 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-10 10:27:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-09-10 10:27:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:27:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=66, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:27:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-09-10 10:27:38 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=132, loss_sum=88.413742, avg_loss=0.669801, seen=132, correct=84, accuracy=0.636364
2025-09-10 10:27:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:27:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:27:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:27:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 10:27:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:27:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:27:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:27:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:27:41 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.792881, avg_loss=0.794822, seen=40, correct=19, accuracy=0.475000
2025-09-10 10:27:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:27:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:27:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:27:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 10:27:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=41 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:27:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=42 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:27:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=43 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:27:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=44 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:27:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=45 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:27:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=46 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:27:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=47 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:27:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=48 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:27:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=49 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:27:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=50 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:27:50 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-10 10:27:50 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-10 10:27:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-09-10 10:27:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:27:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=66, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:27:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-09-10 10:27:53 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=132, loss_sum=86.296631, avg_loss=0.653762, seen=132, correct=86, accuracy=0.651515
2025-09-10 10:27:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:27:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:27:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:27:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 10:27:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:27:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:27:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:27:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:27:58 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.310717, avg_loss=0.782768, seen=40, correct=18, accuracy=0.450000
2025-09-10 10:27:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:27:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:27:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:28:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 10:28:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=51 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:28:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=52 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:28:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=53 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:28:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=54 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:28:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=55 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:28:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=56 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:28:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=57 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:28:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=58 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:28:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=59 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:28:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=60 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:28:08 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-10 10:28:08 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-10 10:28:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-09-10 10:28:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:28:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=66, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:28:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-09-10 10:28:10 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=132, loss_sum=85.430107, avg_loss=0.647198, seen=132, correct=87, accuracy=0.659091
2025-09-10 10:28:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:28:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:28:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:28:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 10:28:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:28:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:28:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:28:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:28:13 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.106434, avg_loss=0.777661, seen=40, correct=20, accuracy=0.500000
2025-09-10 10:28:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:28:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:28:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:28:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 10:28:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=61 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:28:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=62 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:28:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=63 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:28:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=64 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:28:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=65 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:28:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=66 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:28:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=67 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:28:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=68 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:28:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=69 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:28:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=70 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:28:22 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-10 10:28:23 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-10 10:28:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-09-10 10:28:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:28:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=66, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:28:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-09-10 10:28:25 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=132, loss_sum=86.782784, avg_loss=0.657445, seen=132, correct=83, accuracy=0.628788
2025-09-10 10:28:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:28:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:28:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:28:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 10:28:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:28:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:28:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:28:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:28:30 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.675371, avg_loss=0.766884, seen=40, correct=20, accuracy=0.500000
2025-09-10 10:28:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:28:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:28:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:28:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 10:28:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=71 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:28:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=72 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:28:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=73 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:28:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=74 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:28:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=75 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:28:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=76 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:28:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=77 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:28:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=78 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:28:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=79 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:28:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=80 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:28:39 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-10 10:28:39 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-10 10:28:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-09-10 10:28:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:28:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=66, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:28:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-09-10 10:28:41 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=132, loss_sum=88.910606, avg_loss=0.673565, seen=132, correct=79, accuracy=0.598485
2025-09-10 10:28:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:28:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:28:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:28:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 10:28:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:28:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:28:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:28:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:28:44 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.831522, avg_loss=0.770788, seen=40, correct=19, accuracy=0.475000
2025-09-10 10:28:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:28:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:28:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:28:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 10:28:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=81 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:28:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=82 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:28:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=83 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:28:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=84 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:28:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=85 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:28:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=86 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:28:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=87 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:28:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=88 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:28:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=89 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:28:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=90 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:28:54 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-10 10:28:54 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-10 10:28:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-09-10 10:28:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:28:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=66, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:28:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-09-10 10:28:56 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=132, loss_sum=88.512566, avg_loss=0.670550, seen=132, correct=80, accuracy=0.606061
2025-09-10 10:28:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:28:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:28:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:28:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 10:28:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:28:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:28:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:28:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:28:59 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.471643, avg_loss=0.761791, seen=40, correct=19, accuracy=0.475000
2025-09-10 10:28:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:28:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:29:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:29:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 10:29:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=91 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:29:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=92 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:29:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=93 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:29:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=94 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:29:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=95 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:29:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=96 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:29:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=97 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:29:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=98 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:29:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=99 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:29:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=100 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:29:10 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-10 10:29:11 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-10 10:29:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-09-10 10:29:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:29:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=66, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:29:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-09-10 10:29:13 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=132, loss_sum=88.110840, avg_loss=0.667506, seen=132, correct=78, accuracy=0.590909
2025-09-10 10:29:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:29:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:29:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:29:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 10:29:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:29:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:29:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:29:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:29:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.135746, avg_loss=0.753394, seen=40, correct=19, accuracy=0.475000
2025-09-10 10:29:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:29:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:29:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:29:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 10:29:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-10 10:29:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-10 10:29:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:29:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:29:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 10:29:20 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #49', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-10 10:29:20 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #49', 'Round': 0, 'Results_raw': {}}
2025-09-10 10:29:20 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 10:29:20 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-10 10:29:21 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-10 10:29:21 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-10 10:29:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-10 10:29:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:29:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=45, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:29:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-10 10:29:24 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=74.888779, avg_loss=0.680807, seen=110, correct=65, accuracy=0.590909
2025-09-10 10:29:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:29:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:29:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:29:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 10:29:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:29:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:29:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:29:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:29:27 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.449982, avg_loss=0.786250, seen=40, correct=20, accuracy=0.500000
2025-09-10 10:29:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:29:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:29:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:29:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 10:29:28 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-10 10:29:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=526, total=2102)
2025-09-10 10:29:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:29:29 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-10 10:29:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:29:29 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-10 10:29:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:29:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:29:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:29:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:29:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:29:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:29:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:29:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:29:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:29:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:29:35 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-10 10:29:35 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-10 10:29:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-10 10:29:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:29:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:29:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-10 10:29:37 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=75.334137, avg_loss=0.684856, seen=110, correct=70, accuracy=0.636364
2025-09-10 10:29:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:29:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:29:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:29:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 10:29:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:29:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:29:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:29:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:29:43 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.854614, avg_loss=0.796365, seen=40, correct=20, accuracy=0.500000
2025-09-10 10:29:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:29:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:29:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:29:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 10:29:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:29:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:29:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:29:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:29:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:29:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:29:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:29:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:29:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:29:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:29:50 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-10 10:29:51 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-10 10:29:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-10 10:29:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:29:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:29:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-10 10:29:53 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=74.760162, avg_loss=0.679638, seen=110, correct=69, accuracy=0.627273
2025-09-10 10:29:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:29:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:29:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:29:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 10:29:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:29:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:29:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:29:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:29:56 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.926506, avg_loss=0.798163, seen=40, correct=20, accuracy=0.500000
2025-09-10 10:29:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:29:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:29:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:29:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 10:29:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:30:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:30:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:30:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:30:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:30:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:30:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:30:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:30:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:30:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:30:05 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-10 10:30:06 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-10 10:30:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-10 10:30:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:30:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:30:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-10 10:30:08 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=76.267921, avg_loss=0.693345, seen=110, correct=66, accuracy=0.600000
2025-09-10 10:30:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:30:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:30:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:30:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 10:30:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:30:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:30:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:30:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:30:12 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.570185, avg_loss=0.764255, seen=40, correct=23, accuracy=0.575000
2025-09-10 10:30:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:30:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:30:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:30:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 10:30:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=31 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:30:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=32 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:30:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=33 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:30:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=34 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:30:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=35 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:30:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=36 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:30:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=37 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:30:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=38 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:30:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=39 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:30:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=40 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:30:22 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-10 10:30:22 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-10 10:30:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-10 10:30:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:30:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:30:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-10 10:30:24 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=75.813766, avg_loss=0.689216, seen=110, correct=65, accuracy=0.590909
2025-09-10 10:30:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:30:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:30:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:30:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 10:30:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:30:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:30:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:30:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:30:27 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.280472, avg_loss=0.757012, seen=40, correct=22, accuracy=0.550000
2025-09-10 10:30:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:30:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:30:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:30:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 10:30:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=41 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:30:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=42 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:30:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=43 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:30:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=44 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:30:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=45 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:30:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=46 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:30:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=47 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:30:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=48 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:30:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=49 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:30:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=50 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:30:35 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-10 10:30:38 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-10 10:30:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-10 10:30:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:30:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:30:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-10 10:30:40 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=73.762405, avg_loss=0.670567, seen=110, correct=70, accuracy=0.636364
2025-09-10 10:30:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:30:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:30:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:30:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 10:30:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:30:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:30:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:30:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:30:43 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.502258, avg_loss=0.737556, seen=40, correct=24, accuracy=0.600000
2025-09-10 10:30:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:30:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:30:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:30:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 10:30:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=51 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:30:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=52 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:30:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=53 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:30:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=54 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:30:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=55 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:30:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=56 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:30:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=57 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:30:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=58 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:30:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=59 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:30:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=60 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:30:51 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-10 10:30:52 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-10 10:30:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-10 10:30:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:30:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:30:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-10 10:30:54 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=73.012207, avg_loss=0.663747, seen=110, correct=73, accuracy=0.663636
2025-09-10 10:30:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:30:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:30:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:30:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 10:30:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:30:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:30:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:30:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:30:58 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.886946, avg_loss=0.747174, seen=40, correct=23, accuracy=0.575000
2025-09-10 10:30:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:30:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:30:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:30:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 10:31:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=61 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:31:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=62 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:31:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=63 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:31:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=64 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:31:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=65 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:31:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=66 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:31:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=67 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:31:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=68 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:31:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=69 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:31:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=70 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:31:05 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-10 10:31:06 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-10 10:31:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-10 10:31:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:31:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:31:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-10 10:31:08 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=75.193001, avg_loss=0.683573, seen=110, correct=67, accuracy=0.609091
2025-09-10 10:31:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:31:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:31:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:31:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 10:31:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:31:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:31:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:31:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:31:13 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.583447, avg_loss=0.764586, seen=40, correct=23, accuracy=0.575000
2025-09-10 10:31:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:31:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:31:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:31:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 10:31:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=71 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:31:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=72 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:31:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=73 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:31:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=74 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:31:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=75 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:31:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=76 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:31:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=77 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:31:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=78 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:31:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=79 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:31:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=80 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:31:22 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-10 10:31:23 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-10 10:31:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-10 10:31:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:31:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:31:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-10 10:31:25 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=75.280655, avg_loss=0.684370, seen=110, correct=66, accuracy=0.600000
2025-09-10 10:31:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:31:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:31:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:31:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 10:31:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:31:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:31:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:31:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:31:29 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.433163, avg_loss=0.760829, seen=40, correct=22, accuracy=0.550000
2025-09-10 10:31:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:31:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:31:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:31:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 10:31:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=81 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:31:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=82 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:31:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=83 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:31:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=84 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:31:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=85 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:31:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=86 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:31:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=87 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:31:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=88 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:31:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=89 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:31:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=90 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:31:38 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-10 10:31:40 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-10 10:31:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-10 10:31:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:31:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:31:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-10 10:31:41 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=73.979744, avg_loss=0.672543, seen=110, correct=67, accuracy=0.609091
2025-09-10 10:31:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:31:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:31:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:31:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 10:31:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:31:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:31:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:31:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:31:45 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.379128, avg_loss=0.734478, seen=40, correct=22, accuracy=0.550000
2025-09-10 10:31:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:31:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:31:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:31:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 10:31:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=91 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:31:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=92 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:31:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=93 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:31:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=94 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:31:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=95 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:31:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=96 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:31:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=97 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:31:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=98 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:31:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=99 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:31:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=100 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:31:56 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-10 10:31:57 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-10 10:31:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-10 10:31:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:31:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:31:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-10 10:31:58 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=71.715683, avg_loss=0.651961, seen=110, correct=73, accuracy=0.663636
2025-09-10 10:31:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:31:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:32:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:32:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 10:32:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:32:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:32:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:32:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:32:02 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.493837, avg_loss=0.762346, seen=40, correct=20, accuracy=0.500000
2025-09-10 10:32:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:32:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:32:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:32:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 10:32:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-10 10:32:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-10 10:32:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:32:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:32:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 10:32:05 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #19', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-10 10:32:05 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #19', 'Round': 0, 'Results_raw': {}}
2025-09-10 10:32:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 10:32:05 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 2 for training...
2025-09-10 10:32:06 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-10 10:32:06 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-10 10:32:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-10 10:32:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:32:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=16, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:32:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-10 10:32:08 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=57.255859, avg_loss=0.689830, seen=83, correct=48, accuracy=0.578313
2025-09-10 10:32:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:32:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:32:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:32:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 10:32:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:32:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:32:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:32:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:32:12 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.285465, avg_loss=0.632137, seen=40, correct=24, accuracy=0.600000
2025-09-10 10:32:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:32:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:32:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:32:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 10:32:14 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-10 10:32:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=395, total=1580)
2025-09-10 10:32:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:32:14 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-10 10:32:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:32:14 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=198, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-10 10:32:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:32:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:32:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:32:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:32:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:32:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:32:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:32:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:32:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:32:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:32:21 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-10 10:32:22 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-10 10:32:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-10 10:32:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:32:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:32:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-10 10:32:23 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=55.905518, avg_loss=0.673560, seen=83, correct=51, accuracy=0.614458
2025-09-10 10:32:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:32:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:32:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:32:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 10:32:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:32:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:32:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:32:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:32:28 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.211176, avg_loss=0.655279, seen=40, correct=24, accuracy=0.600000
2025-09-10 10:32:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:32:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:32:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:32:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 10:32:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:32:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:32:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:32:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:32:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:32:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:32:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:32:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:32:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:32:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:32:35 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-10 10:32:38 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-10 10:32:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-10 10:32:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:32:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:32:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-10 10:32:39 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=53.983215, avg_loss=0.650400, seen=83, correct=52, accuracy=0.626506
2025-09-10 10:32:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:32:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:32:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:32:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 10:32:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:32:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:32:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:32:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:32:44 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.891546, avg_loss=0.647289, seen=40, correct=25, accuracy=0.625000
2025-09-10 10:32:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:32:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:32:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:32:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 10:32:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:32:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:32:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:32:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:32:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:32:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:32:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:32:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:32:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:32:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:32:55 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-10 10:32:55 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-10 10:32:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-10 10:32:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:32:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:32:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-10 10:32:56 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=53.607204, avg_loss=0.645870, seen=83, correct=55, accuracy=0.662651
2025-09-10 10:32:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:32:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:32:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:33:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 10:33:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:33:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:33:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:33:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:33:01 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.076185, avg_loss=0.626905, seen=40, correct=27, accuracy=0.675000
2025-09-10 10:33:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:33:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:33:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:33:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 10:33:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=31 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:33:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=32 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:33:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=33 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:33:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=34 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:33:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=35 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:33:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=36 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:33:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=37 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:33:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=38 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:33:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=39 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:33:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=40 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:33:12 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-10 10:33:12 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-10 10:33:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-10 10:33:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:33:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:33:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-10 10:33:14 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=54.801399, avg_loss=0.660258, seen=83, correct=52, accuracy=0.626506
2025-09-10 10:33:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:33:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:33:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:33:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 10:33:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:33:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:33:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:33:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:33:22 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.989538, avg_loss=0.624738, seen=40, correct=28, accuracy=0.700000
2025-09-10 10:33:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:33:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:33:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:33:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 10:33:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=41 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:33:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=42 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:33:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=43 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:33:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=44 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:33:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=45 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:33:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=46 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:33:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=47 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:33:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=48 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:33:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=49 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:33:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=50 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:33:31 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-10 10:33:31 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-10 10:33:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-10 10:33:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:33:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:33:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-10 10:33:33 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=54.736912, avg_loss=0.659481, seen=83, correct=53, accuracy=0.638554
2025-09-10 10:33:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:33:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:33:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:33:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 10:33:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:33:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:33:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:33:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:33:37 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.236610, avg_loss=0.605915, seen=40, correct=27, accuracy=0.675000
2025-09-10 10:33:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:33:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:33:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:33:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 10:33:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=51 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:33:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=52 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:33:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=53 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:33:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=54 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:33:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=55 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:33:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=56 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:33:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=57 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:33:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=58 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:33:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=59 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:33:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=60 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:33:49 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-10 10:33:50 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-10 10:33:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-10 10:33:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:33:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:33:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-10 10:33:51 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=54.391705, avg_loss=0.655322, seen=83, correct=55, accuracy=0.662651
2025-09-10 10:33:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:33:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:33:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:33:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 10:33:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:33:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:33:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:33:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:33:57 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.692802, avg_loss=0.617320, seen=40, correct=28, accuracy=0.700000
2025-09-10 10:33:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:33:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:33:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:33:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 10:34:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=61 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:34:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=62 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:34:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=63 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:34:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=64 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:34:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=65 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:34:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=66 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:34:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=67 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:34:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=68 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:34:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=69 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:34:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=70 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:34:07 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-10 10:34:09 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-10 10:34:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-10 10:34:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:34:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:34:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-10 10:34:10 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=54.987400, avg_loss=0.662499, seen=83, correct=50, accuracy=0.602410
2025-09-10 10:34:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:34:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:34:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:34:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 10:34:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:34:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:34:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:34:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:34:16 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.532255, avg_loss=0.638306, seen=40, correct=27, accuracy=0.675000
2025-09-10 10:34:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:34:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:34:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:34:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 10:34:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=71 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:34:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=72 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:34:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=73 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:34:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=74 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:34:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=75 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:34:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=76 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:34:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=77 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:34:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=78 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:34:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=79 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:34:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=80 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:34:26 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-10 10:34:27 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-10 10:34:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-10 10:34:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:34:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:34:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-10 10:34:28 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=54.379051, avg_loss=0.655169, seen=83, correct=54, accuracy=0.650602
2025-09-10 10:34:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:34:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:34:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:34:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 10:34:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:34:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:34:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:34:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:34:33 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.529087, avg_loss=0.638227, seen=40, correct=28, accuracy=0.700000
2025-09-10 10:34:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:34:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:34:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:34:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 10:34:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=81 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:34:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=82 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:34:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=83 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:34:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=84 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:34:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=85 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:34:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=86 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:34:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=87 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:34:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=88 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:34:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=89 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:34:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=90 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:34:42 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-10 10:34:43 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-10 10:34:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-10 10:34:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:34:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:34:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-10 10:34:45 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=55.043949, avg_loss=0.663180, seen=83, correct=52, accuracy=0.626506
2025-09-10 10:34:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:34:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:34:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:34:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 10:34:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:34:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:34:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:34:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:34:49 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.804628, avg_loss=0.645116, seen=40, correct=29, accuracy=0.725000
2025-09-10 10:34:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:34:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:34:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:34:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 10:34:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=91 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:34:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=92 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:34:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=93 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:34:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=94 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:34:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=95 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:34:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=96 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:34:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=97 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:34:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=98 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:34:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=99 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:34:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=100 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:34:56 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-10 10:34:58 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-10 10:34:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-10 10:34:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:34:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:34:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-10 10:34:59 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=55.734787, avg_loss=0.671503, seen=83, correct=49, accuracy=0.590361
2025-09-10 10:34:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:34:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:35:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:35:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 10:35:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:35:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:35:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:35:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:35:06 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.771786, avg_loss=0.644295, seen=40, correct=28, accuracy=0.700000
2025-09-10 10:35:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:35:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:35:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:35:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 10:35:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-10 10:35:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-10 10:35:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:35:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:35:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 10:35:09 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #51', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-10 10:35:09 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #51', 'Round': 0, 'Results_raw': {}}
2025-09-10 10:35:09 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 10:35:09 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-10 10:35:10 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-10 10:35:10 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-10 10:35:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-09-10 10:35:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:35:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=27, num_train_batch_last_epoch=19, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:35:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-09-10 10:35:12 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=54, loss_sum=36.394073, avg_loss=0.673964, seen=54, correct=33, accuracy=0.611111
2025-09-10 10:35:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:35:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:35:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:35:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 10:35:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:35:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:35:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:35:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:35:14 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.162994, avg_loss=0.654075, seen=40, correct=22, accuracy=0.550000
2025-09-10 10:35:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:35:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:35:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:35:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 10:35:16 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-10 10:35:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=258, total=1030)
2025-09-10 10:35:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:35:17 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-10 10:35:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:35:17 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=129, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-10 10:35:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:35:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:35:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:35:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:35:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:35:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:35:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:35:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:35:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:35:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:35:25 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-10 10:35:25 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-10 10:35:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-09-10 10:35:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:35:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=27, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:35:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-09-10 10:35:26 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=54, loss_sum=37.360661, avg_loss=0.691864, seen=54, correct=29, accuracy=0.537037
2025-09-10 10:35:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:35:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:35:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:35:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:35:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:35:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:35:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:35:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:35:30 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.746164, avg_loss=0.643654, seen=40, correct=24, accuracy=0.600000
2025-09-10 10:35:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:35:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:35:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:35:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:35:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:35:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:35:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:35:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:35:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:35:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:35:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:35:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:35:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:35:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:35:40 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-10 10:35:42 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-10 10:35:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-09-10 10:35:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:35:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=27, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:35:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-09-10 10:35:43 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=54, loss_sum=36.786186, avg_loss=0.681226, seen=54, correct=33, accuracy=0.611111
2025-09-10 10:35:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:35:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:35:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:35:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:35:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:35:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:35:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:35:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:35:46 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.030499, avg_loss=0.650762, seen=40, correct=25, accuracy=0.625000
2025-09-10 10:35:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:35:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:35:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:35:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:35:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:35:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:35:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:35:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:35:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:35:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:35:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:35:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:35:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:35:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:35:54 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-10 10:35:54 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-10 10:35:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-09-10 10:35:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:35:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=27, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:35:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-09-10 10:35:55 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=54, loss_sum=36.033627, avg_loss=0.667289, seen=54, correct=32, accuracy=0.592593
2025-09-10 10:35:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:35:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:35:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:36:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:36:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:36:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:36:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:36:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:36:03 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.036201, avg_loss=0.650905, seen=40, correct=21, accuracy=0.525000
2025-09-10 10:36:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:36:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:36:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:36:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:36:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=31 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:36:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=32 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:36:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=33 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:36:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=34 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:36:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=35 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:36:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=36 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:36:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=37 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:36:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=38 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:36:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=39 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:36:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=40 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:36:11 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-10 10:36:12 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-10 10:36:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-09-10 10:36:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:36:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=27, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:36:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-09-10 10:36:13 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=54, loss_sum=36.191978, avg_loss=0.670222, seen=54, correct=29, accuracy=0.537037
2025-09-10 10:36:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:36:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:36:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:36:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:36:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:36:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:36:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:36:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:36:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.713320, avg_loss=0.642833, seen=40, correct=21, accuracy=0.525000
2025-09-10 10:36:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:36:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:36:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:36:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:36:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=41 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:36:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=42 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:36:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=43 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:36:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=44 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:36:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=45 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:36:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=46 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:36:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=47 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:36:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=48 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:36:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=49 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:36:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=50 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:36:31 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-10 10:36:31 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-10 10:36:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-09-10 10:36:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:36:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=27, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:36:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-09-10 10:36:32 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=54, loss_sum=36.544891, avg_loss=0.676757, seen=54, correct=28, accuracy=0.518519
2025-09-10 10:36:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:36:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:36:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:36:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:36:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:36:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:36:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:36:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:36:37 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.561506, avg_loss=0.639038, seen=40, correct=21, accuracy=0.525000
2025-09-10 10:36:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:36:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:36:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:36:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:36:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=51 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:36:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=52 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:36:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=53 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:36:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=54 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:36:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=55 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:36:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=56 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:36:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=57 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:36:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=58 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:36:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=59 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:36:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=60 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:36:47 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-10 10:36:48 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-10 10:36:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-09-10 10:36:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:36:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=27, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:36:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-09-10 10:36:49 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=54, loss_sum=36.127377, avg_loss=0.669025, seen=54, correct=30, accuracy=0.555556
2025-09-10 10:36:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:36:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:36:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:36:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:36:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:36:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:36:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:36:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:36:52 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.761467, avg_loss=0.619037, seen=40, correct=26, accuracy=0.650000
2025-09-10 10:36:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:36:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:36:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:36:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:36:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=61 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:36:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=62 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:36:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=63 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:36:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=64 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:36:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=65 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:36:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=66 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:36:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=67 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:36:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=68 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:37:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=69 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:37:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=70 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:37:01 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-10 10:37:01 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-10 10:37:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-09-10 10:37:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:37:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=27, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:37:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-09-10 10:37:03 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=54, loss_sum=36.376007, avg_loss=0.673630, seen=54, correct=28, accuracy=0.518519
2025-09-10 10:37:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:37:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:37:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:37:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:37:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:37:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:37:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:37:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:37:10 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.578121, avg_loss=0.639453, seen=40, correct=24, accuracy=0.600000
2025-09-10 10:37:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:37:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:37:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:37:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:37:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=71 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:37:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=72 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:37:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=73 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:37:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=74 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:37:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=75 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:37:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=76 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:37:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=77 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:37:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=78 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:37:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=79 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:37:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=80 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:37:19 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-10 10:37:20 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-10 10:37:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-09-10 10:37:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:37:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=27, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:37:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-09-10 10:37:21 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=54, loss_sum=36.431931, avg_loss=0.674665, seen=54, correct=34, accuracy=0.629630
2025-09-10 10:37:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:37:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:37:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:37:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:37:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:37:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:37:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:37:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:37:26 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.488068, avg_loss=0.662202, seen=40, correct=22, accuracy=0.550000
2025-09-10 10:37:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:37:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:37:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:37:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:37:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=81 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:37:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=82 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:37:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=83 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:37:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=84 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:37:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=85 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:37:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=86 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:37:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=87 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:37:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=88 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:37:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=89 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:37:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=90 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:37:37 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-10 10:37:37 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-10 10:37:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-09-10 10:37:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:37:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=27, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:37:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-09-10 10:37:38 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=54, loss_sum=36.232914, avg_loss=0.670980, seen=54, correct=36, accuracy=0.666667
2025-09-10 10:37:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:37:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:37:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:37:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:37:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:37:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:37:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:37:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:37:43 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.973475, avg_loss=0.649337, seen=40, correct=23, accuracy=0.575000
2025-09-10 10:37:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:37:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:37:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:37:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:37:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=91 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:37:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=92 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:37:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=93 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:37:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=94 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:37:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=95 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:37:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=96 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:37:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=97 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:37:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=98 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:37:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=99 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:37:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=100 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:37:52 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-10 10:37:53 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-10 10:37:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-09-10 10:37:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:37:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=27, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:37:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-09-10 10:37:54 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=54, loss_sum=35.965195, avg_loss=0.666022, seen=54, correct=32, accuracy=0.592593
2025-09-10 10:37:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:37:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:37:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:37:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:37:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:37:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:37:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:37:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:37:57 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.537704, avg_loss=0.638443, seen=40, correct=21, accuracy=0.525000
2025-09-10 10:37:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:37:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:37:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:37:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:37:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-10 10:37:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-10 10:37:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:37:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:38:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:38:00 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #36', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-10 10:38:00 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #36', 'Round': 0, 'Results_raw': {}}
2025-09-10 10:38:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 10:38:00 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-10 10:38:01 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-10 10:38:01 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-10 10:38:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-09-10 10:38:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:38:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=32, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:38:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 10:38:03 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=136, loss_sum=86.587326, avg_loss=0.636672, seen=136, correct=86, accuracy=0.632353
2025-09-10 10:38:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:38:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:38:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:38:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 10:38:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:38:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:38:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:38:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:38:08 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.224030, avg_loss=0.680601, seen=40, correct=22, accuracy=0.550000
2025-09-10 10:38:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:38:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:38:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:38:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 10:38:10 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-10 10:38:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=648, total=2589)
2025-09-10 10:38:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:38:10 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-10 10:38:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:38:10 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=324, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-10 10:38:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:38:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:38:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:38:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:38:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:38:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:38:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:38:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:38:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:38:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:38:19 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-10 10:38:20 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-10 10:38:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-09-10 10:38:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:38:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:38:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 10:38:22 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=136, loss_sum=86.142410, avg_loss=0.633400, seen=136, correct=86, accuracy=0.632353
2025-09-10 10:38:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:38:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:38:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:38:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2252MB allocated=2204MB
2025-09-10 10:38:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:38:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:38:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:38:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:38:27 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.629501, avg_loss=0.690738, seen=40, correct=21, accuracy=0.525000
2025-09-10 10:38:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:38:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:38:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:38:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2252MB allocated=2204MB
2025-09-10 10:38:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:38:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:38:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:38:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:38:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:38:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:38:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:38:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:38:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:38:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:38:37 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-10 10:38:37 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-10 10:38:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-09-10 10:38:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:38:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:38:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 10:38:39 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=136, loss_sum=86.792694, avg_loss=0.638182, seen=136, correct=82, accuracy=0.602941
2025-09-10 10:38:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:38:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:38:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:38:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2252MB allocated=2204MB
2025-09-10 10:38:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:38:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:38:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:38:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:38:43 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.585114, avg_loss=0.689628, seen=40, correct=21, accuracy=0.525000
2025-09-10 10:38:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:38:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:38:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:38:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2252MB allocated=2204MB
2025-09-10 10:38:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:38:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:38:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:38:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:38:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:38:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:38:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:38:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:38:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:38:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:38:52 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-10 10:38:53 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-10 10:38:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-09-10 10:38:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:38:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:38:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 10:38:55 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=136, loss_sum=86.710785, avg_loss=0.637579, seen=136, correct=86, accuracy=0.632353
2025-09-10 10:38:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:38:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:38:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:38:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2252MB allocated=2204MB
2025-09-10 10:38:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:38:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:38:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:39:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:39:00 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.467926, avg_loss=0.686698, seen=40, correct=21, accuracy=0.525000
2025-09-10 10:39:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:39:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:39:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:39:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2252MB allocated=2204MB
2025-09-10 10:39:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=31 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:39:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=32 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:39:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=33 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:39:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=34 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:39:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=35 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:39:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=36 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:39:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=37 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:39:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=38 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:39:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=39 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:39:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=40 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:39:10 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-10 10:39:10 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-10 10:39:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-09-10 10:39:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:39:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:39:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 10:39:12 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=136, loss_sum=87.039276, avg_loss=0.639995, seen=136, correct=81, accuracy=0.595588
2025-09-10 10:39:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:39:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:39:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:39:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2252MB allocated=2204MB
2025-09-10 10:39:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:39:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:39:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:39:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:39:16 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.030399, avg_loss=0.700760, seen=40, correct=22, accuracy=0.550000
2025-09-10 10:39:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:39:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:39:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:39:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2252MB allocated=2204MB
2025-09-10 10:39:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=41 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:39:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=42 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:39:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=43 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:39:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=44 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:39:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=45 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:39:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=46 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:39:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=47 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:39:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=48 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:39:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=49 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:39:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=50 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:39:25 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-10 10:39:26 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-10 10:39:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-09-10 10:39:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:39:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:39:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 10:39:28 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=136, loss_sum=86.947617, avg_loss=0.639321, seen=136, correct=80, accuracy=0.588235
2025-09-10 10:39:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:39:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:39:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:39:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2252MB allocated=2204MB
2025-09-10 10:39:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:39:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:39:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:39:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:39:32 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.428673, avg_loss=0.685717, seen=40, correct=22, accuracy=0.550000
2025-09-10 10:39:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:39:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:39:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:39:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2252MB allocated=2204MB
2025-09-10 10:39:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=51 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:39:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=52 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:39:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=53 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:39:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=54 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:39:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=55 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:39:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=56 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:39:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=57 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:39:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=58 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:39:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=59 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:39:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=60 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:39:43 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-10 10:39:45 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-10 10:39:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-09-10 10:39:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:39:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:39:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 10:39:47 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=136, loss_sum=85.502136, avg_loss=0.628692, seen=136, correct=88, accuracy=0.647059
2025-09-10 10:39:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:39:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:39:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:39:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2252MB allocated=2204MB
2025-09-10 10:39:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:39:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:39:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:39:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:39:51 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.653690, avg_loss=0.641342, seen=40, correct=25, accuracy=0.625000
2025-09-10 10:39:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:39:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:39:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:39:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2252MB allocated=2204MB
2025-09-10 10:39:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=61 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:39:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=62 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:39:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=63 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:39:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=64 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:39:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=65 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:39:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=66 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:39:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=67 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:39:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=68 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:39:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=69 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:40:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=70 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:40:00 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-10 10:40:01 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-10 10:40:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-09-10 10:40:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:40:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:40:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 10:40:03 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=136, loss_sum=85.332443, avg_loss=0.627444, seen=136, correct=82, accuracy=0.602941
2025-09-10 10:40:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:40:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:40:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:40:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2252MB allocated=2204MB
2025-09-10 10:40:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:40:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:40:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:40:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:40:07 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.338799, avg_loss=0.633470, seen=40, correct=29, accuracy=0.725000
2025-09-10 10:40:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:40:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:40:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:40:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2252MB allocated=2204MB
2025-09-10 10:40:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=71 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:40:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=72 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:40:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=73 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:40:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=74 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:40:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=75 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:40:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=76 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:40:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=77 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:40:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=78 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:40:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=79 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:40:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=80 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:40:14 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-10 10:40:15 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-10 10:40:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-09-10 10:40:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:40:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:40:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 10:40:18 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=136, loss_sum=85.997017, avg_loss=0.632331, seen=136, correct=85, accuracy=0.625000
2025-09-10 10:40:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:40:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:40:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:40:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2252MB allocated=2204MB
2025-09-10 10:40:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:40:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:40:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:40:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:40:23 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.425339, avg_loss=0.635633, seen=40, correct=26, accuracy=0.650000
2025-09-10 10:40:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:40:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:40:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:40:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2252MB allocated=2204MB
2025-09-10 10:40:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=81 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:40:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=82 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:40:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=83 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:40:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=84 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:40:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=85 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:40:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=86 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:40:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=87 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:40:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=88 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:40:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=89 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:40:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=90 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:40:35 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-10 10:40:35 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-10 10:40:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-09-10 10:40:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:40:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:40:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 10:40:37 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=136, loss_sum=86.505646, avg_loss=0.636071, seen=136, correct=85, accuracy=0.625000
2025-09-10 10:40:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:40:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:40:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:40:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2252MB allocated=2204MB
2025-09-10 10:40:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:40:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:40:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:40:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:40:43 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.197418, avg_loss=0.629935, seen=40, correct=27, accuracy=0.675000
2025-09-10 10:40:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:40:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:40:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:40:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2252MB allocated=2204MB
2025-09-10 10:40:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=91 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:40:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=92 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:40:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=93 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:40:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=94 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:40:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=95 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:40:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=96 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:40:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=97 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:40:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=98 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:40:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=99 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:40:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=100 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:40:52 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-10 10:40:54 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-10 10:40:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-09-10 10:40:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:40:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:40:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 10:40:57 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=136, loss_sum=85.619698, avg_loss=0.629557, seen=136, correct=83, accuracy=0.610294
2025-09-10 10:40:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:40:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:40:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:40:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2252MB allocated=2204MB
2025-09-10 10:40:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:40:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:40:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:41:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:41:01 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.861753, avg_loss=0.646544, seen=40, correct=23, accuracy=0.575000
2025-09-10 10:41:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:41:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:41:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:41:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2252MB allocated=2204MB
2025-09-10 10:41:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-10 10:41:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-10 10:41:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:41:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:41:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2252MB allocated=2204MB
2025-09-10 10:41:04 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #16', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-10 10:41:04 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #16', 'Round': 0, 'Results_raw': {}}
2025-09-10 10:41:04 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 10:41:04 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-10 10:41:05 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-10 10:41:05 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-10 10:41:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-09-10 10:41:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:41:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=33, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:41:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 10:41:08 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=134, loss_sum=86.489471, avg_loss=0.645444, seen=134, correct=85, accuracy=0.634328
2025-09-10 10:41:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:41:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:41:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:41:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 10:41:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:41:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:41:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:41:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:41:12 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.746651, avg_loss=0.768666, seen=40, correct=18, accuracy=0.450000
2025-09-10 10:41:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:41:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:41:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:41:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 10:41:14 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-10 10:41:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=637, total=2547)
2025-09-10 10:41:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:41:15 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-10 10:41:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:41:15 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=319, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-10 10:41:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:41:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:41:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:41:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:41:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:41:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:41:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:41:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:41:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:41:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:41:24 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-10 10:41:24 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-10 10:41:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-09-10 10:41:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:41:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:41:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 10:41:27 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=134, loss_sum=88.819099, avg_loss=0.662829, seen=134, correct=82, accuracy=0.611940
2025-09-10 10:41:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:41:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:41:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:41:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2240MB allocated=2204MB
2025-09-10 10:41:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:41:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:41:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:41:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:41:30 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.239687, avg_loss=0.755992, seen=40, correct=21, accuracy=0.525000
2025-09-10 10:41:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:41:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:41:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:41:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2240MB allocated=2204MB
2025-09-10 10:41:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:41:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:41:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:41:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:41:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:41:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:41:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:41:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:41:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:41:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:41:42 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-10 10:41:43 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-10 10:41:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-09-10 10:41:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:41:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:41:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 10:41:45 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=134, loss_sum=88.834480, avg_loss=0.662944, seen=134, correct=88, accuracy=0.656716
2025-09-10 10:41:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:41:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:41:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:41:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2240MB allocated=2204MB
2025-09-10 10:41:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:41:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:41:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:41:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:41:48 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.294964, avg_loss=0.757374, seen=40, correct=23, accuracy=0.575000
2025-09-10 10:41:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:41:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:41:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:41:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2240MB allocated=2204MB
2025-09-10 10:41:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:41:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:41:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:41:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:41:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:41:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:41:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:41:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:41:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:41:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:41:58 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-10 10:42:00 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-10 10:42:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-09-10 10:42:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:42:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:42:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 10:42:02 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=134, loss_sum=89.055710, avg_loss=0.664595, seen=134, correct=82, accuracy=0.611940
2025-09-10 10:42:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:42:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:42:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:42:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2240MB allocated=2204MB
2025-09-10 10:42:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:42:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:42:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:42:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:42:09 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.895950, avg_loss=0.747399, seen=40, correct=21, accuracy=0.525000
2025-09-10 10:42:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:42:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:42:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:42:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2240MB allocated=2204MB
2025-09-10 10:42:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=31 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:42:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=32 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:42:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=33 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:42:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=34 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:42:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=35 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:42:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=36 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:42:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=37 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:42:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=38 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:42:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=39 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:42:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=40 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:42:18 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-10 10:42:18 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-10 10:42:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-09-10 10:42:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:42:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:42:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 10:42:20 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=134, loss_sum=88.008347, avg_loss=0.656779, seen=134, correct=79, accuracy=0.589552
2025-09-10 10:42:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:42:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:42:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:42:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2240MB allocated=2204MB
2025-09-10 10:42:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:42:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:42:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:42:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:42:28 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.153442, avg_loss=0.753836, seen=40, correct=18, accuracy=0.450000
2025-09-10 10:42:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:42:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:42:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:42:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2240MB allocated=2204MB
2025-09-10 10:42:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=41 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:42:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=42 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:42:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=43 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:42:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=44 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:42:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=45 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:42:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=46 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:42:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=47 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:42:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=48 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:42:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=49 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:42:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=50 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:42:37 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-10 10:42:38 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-10 10:42:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-09-10 10:42:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:42:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:42:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 10:42:40 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=134, loss_sum=88.486801, avg_loss=0.660349, seen=134, correct=76, accuracy=0.567164
2025-09-10 10:42:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:42:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:42:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:42:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2240MB allocated=2204MB
2025-09-10 10:42:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:42:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:42:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:42:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:42:45 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.041119, avg_loss=0.751028, seen=40, correct=20, accuracy=0.500000
2025-09-10 10:42:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:42:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:42:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:42:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2240MB allocated=2204MB
2025-09-10 10:42:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=51 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:42:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=52 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:42:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=53 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:42:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=54 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:42:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=55 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:42:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=56 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:42:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=57 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:42:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=58 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:42:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=59 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:42:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=60 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:42:53 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-10 10:42:55 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-10 10:42:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-09-10 10:42:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:42:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:42:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 10:42:57 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=134, loss_sum=86.811157, avg_loss=0.647844, seen=134, correct=83, accuracy=0.619403
2025-09-10 10:42:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:42:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:42:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:42:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2240MB allocated=2204MB
2025-09-10 10:42:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:42:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:42:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:43:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:43:00 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.762110, avg_loss=0.744053, seen=40, correct=18, accuracy=0.450000
2025-09-10 10:43:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:43:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:43:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:43:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2240MB allocated=2204MB
2025-09-10 10:43:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=61 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:43:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=62 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:43:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=63 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:43:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=64 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:43:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=65 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:43:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=66 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:43:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=67 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:43:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=68 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:43:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=69 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:43:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=70 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:43:09 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-10 10:43:09 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-10 10:43:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-09-10 10:43:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:43:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:43:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 10:43:11 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=134, loss_sum=85.962479, avg_loss=0.641511, seen=134, correct=84, accuracy=0.626866
2025-09-10 10:43:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:43:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:43:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:43:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2240MB allocated=2204MB
2025-09-10 10:43:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:43:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:43:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:43:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:43:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.030888, avg_loss=0.750772, seen=40, correct=22, accuracy=0.550000
2025-09-10 10:43:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:43:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:43:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:43:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2240MB allocated=2204MB
2025-09-10 10:43:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=71 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:43:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=72 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:43:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=73 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:43:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=74 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:43:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=75 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:43:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=76 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:43:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=77 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:43:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=78 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:43:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=79 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:43:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=80 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:43:27 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-10 10:43:28 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-10 10:43:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-09-10 10:43:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:43:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:43:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 10:43:30 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=134, loss_sum=84.800446, avg_loss=0.632839, seen=134, correct=89, accuracy=0.664179
2025-09-10 10:43:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:43:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:43:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:43:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2240MB allocated=2204MB
2025-09-10 10:43:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:43:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:43:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:43:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:43:32 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.298702, avg_loss=0.757468, seen=40, correct=19, accuracy=0.475000
2025-09-10 10:43:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:43:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:43:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:43:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2240MB allocated=2204MB
2025-09-10 10:43:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=81 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:43:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=82 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:43:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=83 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:43:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=84 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:43:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=85 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:43:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=86 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:43:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=87 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:43:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=88 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:43:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=89 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:43:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=90 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:43:40 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-10 10:43:42 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-10 10:43:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-09-10 10:43:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:43:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:43:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 10:43:44 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=134, loss_sum=84.677673, avg_loss=0.631923, seen=134, correct=89, accuracy=0.664179
2025-09-10 10:43:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:43:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:43:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:43:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2240MB allocated=2204MB
2025-09-10 10:43:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:43:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:43:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:43:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:43:51 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.481670, avg_loss=0.762042, seen=40, correct=17, accuracy=0.425000
2025-09-10 10:43:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:43:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:43:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:43:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2240MB allocated=2204MB
2025-09-10 10:43:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=91 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:43:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=92 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:43:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=93 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:43:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=94 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:43:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=95 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:43:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=96 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:43:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=97 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:43:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=98 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:43:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=99 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:43:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=100 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:43:59 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-10 10:43:59 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-10 10:44:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-09-10 10:44:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:44:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:44:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 10:44:02 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=134, loss_sum=85.951302, avg_loss=0.641428, seen=134, correct=90, accuracy=0.671642
2025-09-10 10:44:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:44:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:44:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:44:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2240MB allocated=2204MB
2025-09-10 10:44:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:44:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:44:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:44:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:44:08 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.115475, avg_loss=0.752887, seen=40, correct=20, accuracy=0.500000
2025-09-10 10:44:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:44:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:44:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:44:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2240MB allocated=2204MB
2025-09-10 10:44:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-10 10:44:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-10 10:44:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:44:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:44:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2240MB allocated=2204MB
2025-09-10 10:44:10 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #6', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-10 10:44:10 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #6', 'Round': 0, 'Results_raw': {}}
2025-09-10 10:44:10 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 10:44:10 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 2 for training...
2025-09-10 10:44:11 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-10 10:44:11 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-10 10:44:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:44:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:44:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:44:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:44:16 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.849258, avg_loss=0.679246, seen=200, correct=107, accuracy=0.535000
2025-09-10 10:44:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:44:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:44:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:44:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 10:44:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:44:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:44:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:44:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:44:19 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.110645, avg_loss=0.602766, seen=40, correct=27, accuracy=0.675000
2025-09-10 10:44:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:44:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:44:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:44:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 10:44:20 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-10 10:44:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1548, total=6191)
2025-09-10 10:44:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:44:21 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-10 10:44:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:44:21 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=774, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-10 10:44:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:44:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:44:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:44:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:44:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:44:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:44:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:44:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:44:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:44:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:44:27 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-10 10:44:27 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-10 10:44:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:44:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:44:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:44:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:44:30 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=136.289444, avg_loss=0.681447, seen=200, correct=115, accuracy=0.575000
2025-09-10 10:44:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:44:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:44:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:44:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 10:44:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:44:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:44:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:44:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:44:35 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.574535, avg_loss=0.689363, seen=40, correct=23, accuracy=0.575000
2025-09-10 10:44:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:44:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:44:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:44:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 10:44:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:44:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:44:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:44:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:44:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:44:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:44:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:44:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:44:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:44:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:44:48 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-10 10:44:48 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-10 10:44:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:44:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:44:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:44:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:44:51 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=134.876083, avg_loss=0.674380, seen=200, correct=116, accuracy=0.580000
2025-09-10 10:44:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:44:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:44:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:44:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 10:44:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:44:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:44:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:44:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:44:56 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.153843, avg_loss=0.678846, seen=40, correct=22, accuracy=0.550000
2025-09-10 10:44:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:44:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:44:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:44:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 10:45:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:45:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:45:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:45:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:45:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:45:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:45:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:45:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:45:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:45:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:45:07 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-10 10:45:08 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-10 10:45:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:45:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:45:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:45:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:45:11 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=133.853729, avg_loss=0.669269, seen=200, correct=116, accuracy=0.580000
2025-09-10 10:45:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:45:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:45:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:45:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 10:45:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:45:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:45:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:45:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:45:15 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.899971, avg_loss=0.622499, seen=40, correct=27, accuracy=0.675000
2025-09-10 10:45:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:45:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:45:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:45:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 10:45:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=31 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:45:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=32 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:45:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=33 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:45:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=34 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:45:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=35 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:45:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=36 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:45:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=37 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:45:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=38 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:45:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=39 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:45:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=40 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:45:26 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-10 10:45:26 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-10 10:45:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:45:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:45:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:45:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:45:29 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=134.999512, avg_loss=0.674998, seen=200, correct=114, accuracy=0.570000
2025-09-10 10:45:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:45:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:45:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:45:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 10:45:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:45:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:45:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:45:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:45:34 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.743526, avg_loss=0.593588, seen=40, correct=27, accuracy=0.675000
2025-09-10 10:45:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:45:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:45:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:45:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 10:45:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=41 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:45:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=42 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:45:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=43 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:45:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=44 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:45:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=45 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:45:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=46 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:45:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=47 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:45:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=48 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:45:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=49 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:45:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=50 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:45:42 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-10 10:45:44 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-10 10:45:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:45:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:45:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:45:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:45:47 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.020111, avg_loss=0.675101, seen=200, correct=115, accuracy=0.575000
2025-09-10 10:45:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:45:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:45:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:45:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 10:45:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:45:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:45:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:45:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:45:50 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.578169, avg_loss=0.589454, seen=40, correct=27, accuracy=0.675000
2025-09-10 10:45:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:45:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:45:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:45:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 10:45:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=51 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:45:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=52 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:45:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=53 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:45:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=54 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:45:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=55 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:46:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=56 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:46:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=57 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:46:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=58 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:46:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=59 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:46:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=60 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:46:02 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-10 10:46:03 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-10 10:46:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:46:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:46:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:46:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:46:06 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=134.745544, avg_loss=0.673728, seen=200, correct=117, accuracy=0.585000
2025-09-10 10:46:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:46:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:46:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:46:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 10:46:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:46:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:46:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:46:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:46:12 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.718662, avg_loss=0.617967, seen=40, correct=27, accuracy=0.675000
2025-09-10 10:46:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:46:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:46:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:46:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 10:46:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=61 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:46:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=62 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:46:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=63 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:46:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=64 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:46:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=65 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:46:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=66 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:46:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=67 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:46:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=68 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:46:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=69 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:46:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=70 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:46:24 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-10 10:46:25 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-10 10:46:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:46:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:46:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:46:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:46:28 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=134.482437, avg_loss=0.672412, seen=200, correct=115, accuracy=0.575000
2025-09-10 10:46:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:46:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:46:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:46:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 10:46:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:46:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:46:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:46:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:46:32 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.623766, avg_loss=0.640594, seen=40, correct=23, accuracy=0.575000
2025-09-10 10:46:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:46:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:46:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:46:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 10:46:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=71 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:46:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=72 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:46:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=73 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:46:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=74 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:46:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=75 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:46:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=76 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:46:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=77 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:46:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=78 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:46:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=79 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:46:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=80 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:46:41 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-10 10:46:43 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-10 10:46:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:46:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:46:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:46:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:46:46 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=133.831223, avg_loss=0.669156, seen=200, correct=118, accuracy=0.590000
2025-09-10 10:46:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:46:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:46:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:46:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 10:46:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:46:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:46:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:46:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:46:50 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.368111, avg_loss=0.609203, seen=40, correct=27, accuracy=0.675000
2025-09-10 10:46:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:46:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:46:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:46:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 10:46:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=81 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:46:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=82 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:46:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=83 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:46:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=84 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:46:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=85 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:46:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=86 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:46:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=87 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:46:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=88 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:46:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=89 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:46:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=90 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:46:58 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-10 10:46:59 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-10 10:46:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:46:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:46:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:47:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:47:02 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=134.538162, avg_loss=0.672691, seen=200, correct=119, accuracy=0.595000
2025-09-10 10:47:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:47:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:47:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:47:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 10:47:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:47:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:47:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:47:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:47:11 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.483398, avg_loss=0.612085, seen=40, correct=26, accuracy=0.650000
2025-09-10 10:47:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:47:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:47:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:47:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 10:47:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=91 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:47:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=92 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:47:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=93 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:47:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=94 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:47:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=95 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:47:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=96 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:47:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=97 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:47:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=98 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:47:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=99 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:47:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=100 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:47:19 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-10 10:47:21 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-10 10:47:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:47:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:47:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:47:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:47:24 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=133.763046, avg_loss=0.668815, seen=200, correct=118, accuracy=0.590000
2025-09-10 10:47:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:47:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:47:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:47:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 10:47:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:47:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:47:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:47:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:47:29 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.925011, avg_loss=0.623125, seen=40, correct=26, accuracy=0.650000
2025-09-10 10:47:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:47:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:47:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:47:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 10:47:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-10 10:47:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-10 10:47:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:47:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:47:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 10:47:33 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #29', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-10 10:47:33 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #29', 'Round': 0, 'Results_raw': {}}
2025-09-10 10:47:33 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 10:47:33 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-10 10:47:34 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-10 10:47:34 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-10 10:47:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:47:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:47:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:47:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:47:37 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=128.078110, avg_loss=0.640391, seen=200, correct=128, accuracy=0.640000
2025-09-10 10:47:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:47:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:47:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:47:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 10:47:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:47:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:47:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:47:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:47:41 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.435249, avg_loss=0.685881, seen=40, correct=23, accuracy=0.575000
2025-09-10 10:47:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:47:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:47:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:47:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 10:47:42 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-10 10:47:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1471, total=5883)
2025-09-10 10:47:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:47:43 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-10 10:47:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:47:43 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=736, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-10 10:47:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:47:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:47:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:47:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:47:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:47:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:47:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:47:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:47:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:47:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:47:49 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-10 10:47:50 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-10 10:47:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:47:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:47:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:47:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:47:53 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=131.389404, avg_loss=0.656947, seen=200, correct=127, accuracy=0.635000
2025-09-10 10:47:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:47:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:47:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:47:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2252MB allocated=2204MB
2025-09-10 10:47:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:47:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:47:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:47:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:47:56 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.676527, avg_loss=0.691913, seen=40, correct=20, accuracy=0.500000
2025-09-10 10:47:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:47:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:47:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:47:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2252MB allocated=2204MB
2025-09-10 10:48:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:48:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:48:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:48:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:48:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:48:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:48:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:48:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:48:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:48:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:48:06 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-10 10:48:06 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-10 10:48:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:48:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:48:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:48:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:48:09 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=129.579239, avg_loss=0.647896, seen=200, correct=124, accuracy=0.620000
2025-09-10 10:48:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:48:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:48:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:48:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2252MB allocated=2204MB
2025-09-10 10:48:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:48:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:48:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:48:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:48:12 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.699532, avg_loss=0.692488, seen=40, correct=20, accuracy=0.500000
2025-09-10 10:48:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:48:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:48:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:48:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2252MB allocated=2204MB
2025-09-10 10:48:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:48:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:48:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:48:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:48:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:48:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:48:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:48:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:48:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:48:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:48:20 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-10 10:48:23 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-10 10:48:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:48:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:48:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:48:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:48:26 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=127.491165, avg_loss=0.637456, seen=200, correct=127, accuracy=0.635000
2025-09-10 10:48:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:48:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:48:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:48:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2252MB allocated=2204MB
2025-09-10 10:48:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:48:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:48:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:48:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:48:32 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.047249, avg_loss=0.701181, seen=40, correct=24, accuracy=0.600000
2025-09-10 10:48:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:48:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:48:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:48:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2252MB allocated=2204MB
2025-09-10 10:48:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=31 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:48:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=32 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:48:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=33 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:48:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=34 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:48:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=35 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:48:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=36 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:48:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=37 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:48:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=38 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:48:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=39 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:48:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=40 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:48:39 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-10 10:48:41 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-10 10:48:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:48:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:48:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:48:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:48:44 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=126.962921, avg_loss=0.634815, seen=200, correct=121, accuracy=0.605000
2025-09-10 10:48:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:48:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:48:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:48:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2252MB allocated=2204MB
2025-09-10 10:48:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:48:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:48:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:48:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:48:48 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.439831, avg_loss=0.685996, seen=40, correct=22, accuracy=0.550000
2025-09-10 10:48:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:48:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:48:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:48:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2252MB allocated=2204MB
2025-09-10 10:48:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=41 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:48:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=42 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:48:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=43 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:48:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=44 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:48:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=45 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:48:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=46 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:48:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=47 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:48:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=48 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:48:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=49 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:48:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=50 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:48:57 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-10 10:48:57 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-10 10:48:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:48:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:48:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:49:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:49:00 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=127.572510, avg_loss=0.637863, seen=200, correct=126, accuracy=0.630000
2025-09-10 10:49:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:49:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:49:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:49:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2252MB allocated=2204MB
2025-09-10 10:49:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:49:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:49:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:49:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:49:07 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.252686, avg_loss=0.681317, seen=40, correct=23, accuracy=0.575000
2025-09-10 10:49:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:49:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:49:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:49:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2252MB allocated=2204MB
2025-09-10 10:49:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=51 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:49:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=52 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:49:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=53 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:49:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=54 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:49:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=55 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:49:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=56 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:49:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=57 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:49:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=58 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:49:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=59 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:49:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=60 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:49:16 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-10 10:49:16 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-10 10:49:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:49:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:49:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:49:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:49:19 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=126.876022, avg_loss=0.634380, seen=200, correct=124, accuracy=0.620000
2025-09-10 10:49:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:49:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:49:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:49:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2252MB allocated=2204MB
2025-09-10 10:49:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:49:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:49:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:49:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:49:24 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.848991, avg_loss=0.696225, seen=40, correct=24, accuracy=0.600000
2025-09-10 10:49:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:49:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:49:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:49:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2252MB allocated=2204MB
2025-09-10 10:49:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=61 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:49:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=62 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:49:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=63 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:49:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=64 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:49:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=65 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:49:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=66 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:49:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=67 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:49:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=68 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:49:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=69 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:49:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=70 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:49:34 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-10 10:49:35 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-10 10:49:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:49:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:49:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:49:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:49:38 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=127.943802, avg_loss=0.639719, seen=200, correct=121, accuracy=0.605000
2025-09-10 10:49:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:49:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:49:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:49:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2252MB allocated=2204MB
2025-09-10 10:49:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:49:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:49:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:49:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:49:42 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.846756, avg_loss=0.696169, seen=40, correct=24, accuracy=0.600000
2025-09-10 10:49:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:49:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:49:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:49:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2252MB allocated=2204MB
2025-09-10 10:49:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=71 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:49:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=72 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:49:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=73 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:49:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=74 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:49:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=75 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:49:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=76 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:49:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=77 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:49:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=78 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:49:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=79 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:49:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=80 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:49:51 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-10 10:49:52 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-10 10:49:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:49:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:49:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:49:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:49:55 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=126.982117, avg_loss=0.634911, seen=200, correct=126, accuracy=0.630000
2025-09-10 10:49:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:49:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:49:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:49:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2252MB allocated=2204MB
2025-09-10 10:49:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:49:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:49:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:49:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:49:58 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.064236, avg_loss=0.701606, seen=40, correct=24, accuracy=0.600000
2025-09-10 10:49:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:49:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:49:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:49:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2252MB allocated=2204MB
2025-09-10 10:50:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=81 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:50:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=82 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:50:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=83 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:50:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=84 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:50:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=85 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:50:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=86 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:50:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=87 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:50:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=88 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:50:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=89 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:50:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=90 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:50:06 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-10 10:50:08 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-10 10:50:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:50:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:50:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:50:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:50:11 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=127.677788, avg_loss=0.638389, seen=200, correct=121, accuracy=0.605000
2025-09-10 10:50:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:50:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:50:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:50:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2252MB allocated=2204MB
2025-09-10 10:50:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:50:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:50:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:50:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:50:14 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.391735, avg_loss=0.684793, seen=40, correct=24, accuracy=0.600000
2025-09-10 10:50:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:50:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:50:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:50:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2252MB allocated=2204MB
2025-09-10 10:50:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=91 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:50:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=92 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:50:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=93 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:50:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=94 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:50:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=95 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:50:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=96 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:50:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=97 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:50:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=98 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:50:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=99 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:50:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=100 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:50:23 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-10 10:50:23 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-10 10:50:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 10:50:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:50:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:50:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 10:50:27 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=128.051895, avg_loss=0.640259, seen=200, correct=123, accuracy=0.615000
2025-09-10 10:50:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:50:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:50:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:50:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2252MB allocated=2204MB
2025-09-10 10:50:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:50:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:50:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:50:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:50:32 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.763519, avg_loss=0.669088, seen=40, correct=27, accuracy=0.675000
2025-09-10 10:50:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:50:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:50:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:50:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2252MB allocated=2204MB
2025-09-10 10:50:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-10 10:50:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-10 10:50:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:50:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:50:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2252MB allocated=2204MB
2025-09-10 10:50:35 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #17', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-10 10:50:35 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #17', 'Round': 0, 'Results_raw': {}}
2025-09-10 10:50:35 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 10:50:35 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-10 10:50:36 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-10 10:50:36 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-10 10:50:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-10 10:50:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:50:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=45, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:50:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-10 10:50:38 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=75.264343, avg_loss=0.684221, seen=110, correct=65, accuracy=0.590909
2025-09-10 10:50:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:50:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:50:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:50:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 10:50:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:50:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:50:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:50:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:50:41 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.523226, avg_loss=0.638081, seen=40, correct=27, accuracy=0.675000
2025-09-10 10:50:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:50:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:50:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:50:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 10:50:43 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-10 10:50:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=525, total=2100)
2025-09-10 10:50:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:50:43 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-10 10:50:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:50:43 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-10 10:50:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:50:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:50:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:50:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:50:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:50:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:50:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:50:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:50:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:50:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:50:50 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-10 10:50:51 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-10 10:50:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-10 10:50:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:50:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:50:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-10 10:50:53 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=76.340218, avg_loss=0.694002, seen=110, correct=62, accuracy=0.563636
2025-09-10 10:50:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:50:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:50:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:50:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:50:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:50:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:50:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:50:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:50:56 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.464169, avg_loss=0.661604, seen=40, correct=22, accuracy=0.550000
2025-09-10 10:50:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:50:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:50:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:50:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:50:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:51:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:51:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:51:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:51:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:51:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:51:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:51:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:51:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:51:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:51:05 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-10 10:51:05 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-10 10:51:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-10 10:51:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:51:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:51:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-10 10:51:07 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=74.284515, avg_loss=0.675314, seen=110, correct=67, accuracy=0.609091
2025-09-10 10:51:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:51:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:51:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:51:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:51:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:51:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:51:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:51:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:51:11 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.994240, avg_loss=0.674856, seen=40, correct=23, accuracy=0.575000
2025-09-10 10:51:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:51:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:51:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:51:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:51:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:51:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:51:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:51:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:51:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:51:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:51:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:51:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:51:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:51:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:51:20 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-10 10:51:21 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-10 10:51:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-10 10:51:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:51:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:51:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-10 10:51:22 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=75.985611, avg_loss=0.690778, seen=110, correct=60, accuracy=0.545455
2025-09-10 10:51:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:51:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:51:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:51:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:51:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:51:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:51:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:51:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:51:27 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.021019, avg_loss=0.650525, seen=40, correct=24, accuracy=0.600000
2025-09-10 10:51:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:51:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:51:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:51:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:51:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=31 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:51:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=32 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:51:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=33 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:51:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=34 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:51:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=35 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:51:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=36 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:51:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=37 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:51:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=38 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:51:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=39 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:51:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=40 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:51:35 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-10 10:51:36 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-10 10:51:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-10 10:51:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:51:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:51:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-10 10:51:38 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=74.765442, avg_loss=0.679686, seen=110, correct=62, accuracy=0.563636
2025-09-10 10:51:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:51:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:51:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:51:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:51:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:51:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:51:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:51:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:51:46 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.940956, avg_loss=0.648524, seen=40, correct=25, accuracy=0.625000
2025-09-10 10:51:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:51:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:51:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:51:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:51:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=41 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:51:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=42 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:51:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=43 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:51:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=44 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:51:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=45 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:51:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=46 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:51:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=47 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:51:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=48 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:51:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=49 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:51:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=50 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:51:54 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-10 10:51:54 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-10 10:51:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-10 10:51:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:51:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:51:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-10 10:51:56 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=74.721336, avg_loss=0.679285, seen=110, correct=62, accuracy=0.563636
2025-09-10 10:51:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:51:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:51:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:51:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:52:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:52:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:52:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:52:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:52:02 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.371239, avg_loss=0.659281, seen=40, correct=22, accuracy=0.550000
2025-09-10 10:52:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:52:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:52:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:52:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:52:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=51 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:52:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=52 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:52:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=53 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:52:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=54 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:52:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=55 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:52:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=56 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:52:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=57 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:52:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=58 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:52:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=59 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:52:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=60 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:52:12 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-10 10:52:13 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-10 10:52:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-10 10:52:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:52:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:52:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-10 10:52:14 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=75.584564, avg_loss=0.687132, seen=110, correct=63, accuracy=0.572727
2025-09-10 10:52:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:52:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:52:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:52:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:52:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:52:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:52:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:52:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:52:19 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.973753, avg_loss=0.649344, seen=40, correct=21, accuracy=0.525000
2025-09-10 10:52:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:52:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:52:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:52:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:52:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=61 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:52:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=62 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:52:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=63 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:52:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=64 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:52:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=65 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:52:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=66 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:52:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=67 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:52:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=68 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:52:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=69 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:52:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=70 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:52:29 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-10 10:52:30 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-10 10:52:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-10 10:52:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:52:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:52:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-10 10:52:32 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=77.036911, avg_loss=0.700336, seen=110, correct=66, accuracy=0.600000
2025-09-10 10:52:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:52:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:52:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:52:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:52:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:52:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:52:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:52:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:52:35 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.313049, avg_loss=0.632826, seen=40, correct=26, accuracy=0.650000
2025-09-10 10:52:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:52:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:52:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:52:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:52:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=71 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:52:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=72 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:52:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=73 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:52:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=74 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:52:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=75 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:52:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=76 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:52:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=77 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:52:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=78 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:52:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=79 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:52:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=80 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:52:43 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-10 10:52:46 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-10 10:52:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-10 10:52:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:52:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:52:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-10 10:52:47 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=76.360573, avg_loss=0.694187, seen=110, correct=66, accuracy=0.600000
2025-09-10 10:52:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:52:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:52:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:52:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:52:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:52:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:52:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:52:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:52:54 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.899595, avg_loss=0.622490, seen=40, correct=30, accuracy=0.750000
2025-09-10 10:52:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:52:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:52:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:52:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:52:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=81 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:52:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=82 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:52:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=83 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:52:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=84 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:53:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=85 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:53:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=86 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:53:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=87 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:53:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=88 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:53:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=89 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:53:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=90 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:53:03 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-10 10:53:03 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-10 10:53:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-10 10:53:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:53:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:53:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-10 10:53:05 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=75.901810, avg_loss=0.690016, seen=110, correct=64, accuracy=0.581818
2025-09-10 10:53:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:53:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:53:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:53:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:53:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:53:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:53:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:53:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:53:11 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.112549, avg_loss=0.627814, seen=40, correct=28, accuracy=0.700000
2025-09-10 10:53:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:53:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:53:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:53:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:53:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=91 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:53:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=92 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:53:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=93 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:53:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=94 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:53:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=95 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:53:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=96 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:53:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=97 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:53:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=98 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:53:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=99 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:53:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=100 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:53:20 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-10 10:53:21 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-10 10:53:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-10 10:53:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:53:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:53:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-10 10:53:23 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=74.582413, avg_loss=0.678022, seen=110, correct=67, accuracy=0.609091
2025-09-10 10:53:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:53:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:53:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:53:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:53:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:53:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:53:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:53:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:53:27 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.480122, avg_loss=0.637003, seen=40, correct=24, accuracy=0.600000
2025-09-10 10:53:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:53:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:53:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:53:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:53:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-10 10:53:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-10 10:53:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:53:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:53:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2238MB allocated=2204MB
2025-09-10 10:53:31 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #46', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-10 10:53:31 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #46', 'Round': 0, 'Results_raw': {}}
2025-09-10 10:53:31 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 10:53:31 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-10 10:53:32 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-10 10:53:32 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-10 10:53:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-09-10 10:53:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:53:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=77, num_train_batch_last_epoch=23, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:53:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-09-10 10:53:36 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=153, loss_sum=101.798248, avg_loss=0.665348, seen=153, correct=94, accuracy=0.614379
2025-09-10 10:53:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:53:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:53:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:53:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 10:53:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:53:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:53:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:53:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:53:39 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.286346, avg_loss=0.757159, seen=40, correct=18, accuracy=0.450000
2025-09-10 10:53:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:53:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:53:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:53:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 10:53:41 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-10 10:53:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=729, total=2915)
2025-09-10 10:53:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:53:41 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-10 10:53:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:53:41 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=365, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-10 10:53:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:53:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:53:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:53:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:53:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:53:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:53:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:53:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:53:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:53:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:53:48 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-10 10:53:49 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-10 10:53:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-09-10 10:53:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:53:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=77, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:53:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-09-10 10:53:51 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=153, loss_sum=104.820030, avg_loss=0.685098, seen=153, correct=92, accuracy=0.601307
2025-09-10 10:53:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:53:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:53:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:53:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 10:53:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:53:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:53:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:53:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:53:56 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.688231, avg_loss=0.792206, seen=40, correct=19, accuracy=0.475000
2025-09-10 10:53:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:53:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:53:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:53:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 10:54:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:54:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:54:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:54:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:54:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:54:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:54:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:54:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:54:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:54:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:54:08 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-10 10:54:09 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-10 10:54:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-09-10 10:54:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:54:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=77, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:54:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-09-10 10:54:12 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=153, loss_sum=101.821930, avg_loss=0.665503, seen=153, correct=89, accuracy=0.581699
2025-09-10 10:54:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:54:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:54:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:54:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 10:54:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:54:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:54:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:54:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:54:17 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.764515, avg_loss=0.769113, seen=40, correct=17, accuracy=0.425000
2025-09-10 10:54:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:54:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:54:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:54:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 10:54:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:54:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:54:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:54:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:54:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:54:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:54:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:54:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:54:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:54:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:54:26 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-10 10:54:26 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-10 10:54:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-09-10 10:54:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:54:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=77, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:54:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-09-10 10:54:28 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=153, loss_sum=102.829041, avg_loss=0.672085, seen=153, correct=85, accuracy=0.555556
2025-09-10 10:54:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:54:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:54:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:54:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 10:54:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:54:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:54:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:54:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:54:32 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.117500, avg_loss=0.777938, seen=40, correct=18, accuracy=0.450000
2025-09-10 10:54:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:54:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:54:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:54:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 10:54:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=31 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:54:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=32 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:54:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=33 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:54:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=34 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:54:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=35 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:54:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=36 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:54:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=37 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:54:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=38 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:54:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=39 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:54:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=40 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:54:41 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-10 10:54:42 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-10 10:54:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-09-10 10:54:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:54:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=77, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:54:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-09-10 10:54:44 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=153, loss_sum=104.477646, avg_loss=0.682860, seen=153, correct=86, accuracy=0.562092
2025-09-10 10:54:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:54:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:54:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:54:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 10:54:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:54:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:54:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:54:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:54:50 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.895533, avg_loss=0.747388, seen=40, correct=20, accuracy=0.500000
2025-09-10 10:54:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:54:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:54:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:54:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 10:54:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=41 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:54:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=42 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:54:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=43 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:54:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=44 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:54:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=45 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:54:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=46 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:54:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=47 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:54:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=48 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:54:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=49 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:54:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=50 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:54:58 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-10 10:54:59 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-10 10:55:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-09-10 10:55:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:55:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=77, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:55:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-09-10 10:55:02 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=153, loss_sum=104.386276, avg_loss=0.682263, seen=153, correct=90, accuracy=0.588235
2025-09-10 10:55:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:55:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:55:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:55:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 10:55:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:55:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:55:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:55:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:55:10 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.487307, avg_loss=0.737183, seen=40, correct=23, accuracy=0.575000
2025-09-10 10:55:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:55:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:55:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:55:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 10:55:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=51 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:55:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=52 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:55:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=53 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:55:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=54 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:55:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=55 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:55:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=56 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:55:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=57 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:55:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=58 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:55:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=59 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:55:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=60 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:55:19 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-10 10:55:20 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-10 10:55:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-09-10 10:55:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:55:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=77, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:55:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-09-10 10:55:23 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=153, loss_sum=104.415604, avg_loss=0.682455, seen=153, correct=88, accuracy=0.575163
2025-09-10 10:55:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:55:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:55:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:55:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 10:55:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:55:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:55:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:55:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:55:28 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.966362, avg_loss=0.749159, seen=40, correct=22, accuracy=0.550000
2025-09-10 10:55:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:55:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:55:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:55:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 10:55:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=61 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:55:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=62 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:55:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=63 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:55:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=64 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:55:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=65 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:55:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=66 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:55:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=67 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:55:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=68 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:55:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=69 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:55:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=70 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:55:36 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-10 10:55:36 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-10 10:55:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-09-10 10:55:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:55:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=77, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:55:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-09-10 10:55:39 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=153, loss_sum=103.801147, avg_loss=0.678439, seen=153, correct=89, accuracy=0.581699
2025-09-10 10:55:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:55:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:55:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:55:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 10:55:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:55:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:55:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:55:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:55:46 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.964956, avg_loss=0.724124, seen=40, correct=23, accuracy=0.575000
2025-09-10 10:55:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:55:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:55:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:55:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 10:55:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=71 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:55:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=72 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:55:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=73 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:55:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=74 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:55:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=75 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:55:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=76 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:55:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=77 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:55:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=78 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:55:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=79 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:55:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=80 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:55:54 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-10 10:55:55 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-10 10:55:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-09-10 10:55:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:55:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=77, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:55:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-09-10 10:55:58 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=153, loss_sum=105.796165, avg_loss=0.691478, seen=153, correct=89, accuracy=0.581699
2025-09-10 10:55:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:55:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:55:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:56:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 10:56:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:56:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:56:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:56:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:56:01 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.212528, avg_loss=0.705313, seen=40, correct=25, accuracy=0.625000
2025-09-10 10:56:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:56:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:56:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:56:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 10:56:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=81 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:56:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=82 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:56:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=83 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:56:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=84 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:56:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=85 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:56:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=86 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:56:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=87 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:56:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=88 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:56:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=89 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:56:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=90 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:56:13 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-10 10:56:13 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-10 10:56:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-09-10 10:56:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:56:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=77, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:56:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-09-10 10:56:15 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=153, loss_sum=104.946930, avg_loss=0.685928, seen=153, correct=82, accuracy=0.535948
2025-09-10 10:56:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:56:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:56:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:56:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 10:56:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:56:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:56:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:56:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:56:21 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.896332, avg_loss=0.722408, seen=40, correct=21, accuracy=0.525000
2025-09-10 10:56:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:56:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:56:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:56:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 10:56:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=91 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:56:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=92 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:56:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=93 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:56:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=94 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:56:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=95 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:56:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=96 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:56:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=97 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:56:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=98 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:56:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=99 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:56:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=100 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:56:33 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-10 10:56:34 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-10 10:56:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-09-10 10:56:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:56:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=77, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:56:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-09-10 10:56:37 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=153, loss_sum=102.462097, avg_loss=0.669687, seen=153, correct=91, accuracy=0.594771
2025-09-10 10:56:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:56:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:56:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:56:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 10:56:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:56:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:56:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:56:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:56:42 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.798830, avg_loss=0.744971, seen=40, correct=18, accuracy=0.450000
2025-09-10 10:56:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:56:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:56:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:56:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 10:56:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-10 10:56:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-10 10:56:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:56:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:56:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 10:56:45 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #21', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-10 10:56:45 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #21', 'Round': 0, 'Results_raw': {}}
2025-09-10 10:56:45 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 10:56:45 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-10 10:56:45 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-10 10:56:45 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-10 10:56:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-09-10 10:56:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:56:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=74, num_train_batch_last_epoch=26, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:56:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-10 10:56:49 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=147, loss_sum=104.040764, avg_loss=0.707760, seen=147, correct=76, accuracy=0.517007
2025-09-10 10:56:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:56:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:56:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:56:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 10:56:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:56:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:56:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:56:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:56:52 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.023048, avg_loss=0.625576, seen=40, correct=26, accuracy=0.650000
2025-09-10 10:56:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:56:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:56:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:56:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 10:56:54 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-10 10:56:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=703, total=2812)
2025-09-10 10:56:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:56:54 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-10 10:56:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:56:54 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=352, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-10 10:56:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:56:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:56:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:56:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:56:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:56:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:56:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:56:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:57:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:57:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:57:01 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-10 10:57:01 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-10 10:57:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-09-10 10:57:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:57:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=74, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:57:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-10 10:57:03 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=147, loss_sum=106.741676, avg_loss=0.726134, seen=147, correct=80, accuracy=0.544218
2025-09-10 10:57:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:57:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:57:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:57:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2240MB allocated=2204MB
2025-09-10 10:57:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:57:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:57:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:57:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:57:08 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.097237, avg_loss=0.677431, seen=40, correct=23, accuracy=0.575000
2025-09-10 10:57:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:57:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:57:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:57:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2240MB allocated=2204MB
2025-09-10 10:57:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:57:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:57:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:57:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:57:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:57:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:57:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:57:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:57:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:57:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:57:19 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-10 10:57:19 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-10 10:57:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-09-10 10:57:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:57:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=74, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:57:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-10 10:57:22 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=147, loss_sum=104.603333, avg_loss=0.711587, seen=147, correct=72, accuracy=0.489796
2025-09-10 10:57:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:57:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:57:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:57:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2240MB allocated=2204MB
2025-09-10 10:57:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:57:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:57:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:57:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:57:27 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.183140, avg_loss=0.629578, seen=40, correct=26, accuracy=0.650000
2025-09-10 10:57:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:57:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:57:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:57:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2240MB allocated=2204MB
2025-09-10 10:57:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:57:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:57:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:57:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:57:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:57:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:57:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:57:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:57:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:57:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:57:36 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-10 10:57:37 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-10 10:57:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-09-10 10:57:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:57:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=74, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:57:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-10 10:57:40 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=147, loss_sum=103.497452, avg_loss=0.704064, seen=147, correct=74, accuracy=0.503401
2025-09-10 10:57:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:57:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:57:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:57:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2240MB allocated=2204MB
2025-09-10 10:57:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:57:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:57:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:57:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:57:44 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.255280, avg_loss=0.606382, seen=40, correct=25, accuracy=0.625000
2025-09-10 10:57:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:57:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:57:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:57:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2240MB allocated=2204MB
2025-09-10 10:57:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=31 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:57:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=32 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:57:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=33 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:57:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=34 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:57:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=35 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:57:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=36 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:57:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=37 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:57:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=38 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:57:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=39 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:57:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=40 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:57:54 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-10 10:57:54 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-10 10:57:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-09-10 10:57:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:57:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=74, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:57:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-10 10:57:57 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=147, loss_sum=102.115295, avg_loss=0.694662, seen=147, correct=81, accuracy=0.551020
2025-09-10 10:57:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:57:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:57:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:58:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2240MB allocated=2204MB
2025-09-10 10:58:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:58:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:58:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:58:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:58:02 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.188133, avg_loss=0.604703, seen=40, correct=26, accuracy=0.650000
2025-09-10 10:58:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:58:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:58:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:58:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2240MB allocated=2204MB
2025-09-10 10:58:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=41 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:58:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=42 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:58:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=43 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:58:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=44 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:58:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=45 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:58:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=46 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:58:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=47 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:58:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=48 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:58:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=49 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:58:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=50 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:58:11 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-10 10:58:13 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-10 10:58:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-09-10 10:58:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:58:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=74, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:58:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-10 10:58:16 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=147, loss_sum=102.070450, avg_loss=0.694357, seen=147, correct=77, accuracy=0.523810
2025-09-10 10:58:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:58:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:58:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:58:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2240MB allocated=2204MB
2025-09-10 10:58:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:58:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:58:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:58:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:58:23 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.524162, avg_loss=0.613104, seen=40, correct=27, accuracy=0.675000
2025-09-10 10:58:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:58:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:58:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:58:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2240MB allocated=2204MB
2025-09-10 10:58:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=51 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:58:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=52 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:58:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=53 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:58:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=54 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:58:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=55 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:58:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=56 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:58:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=57 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:58:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=58 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:58:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=59 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:58:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=60 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:58:32 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-10 10:58:32 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-10 10:58:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-09-10 10:58:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:58:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=74, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:58:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-10 10:58:35 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=147, loss_sum=102.692932, avg_loss=0.698591, seen=147, correct=82, accuracy=0.557823
2025-09-10 10:58:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:58:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:58:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:58:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2240MB allocated=2204MB
2025-09-10 10:58:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:58:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:58:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:58:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:58:39 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.321428, avg_loss=0.633036, seen=40, correct=21, accuracy=0.525000
2025-09-10 10:58:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:58:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:58:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:58:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2240MB allocated=2204MB
2025-09-10 10:58:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=61 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:58:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=62 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:58:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=63 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:58:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=64 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:58:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=65 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:58:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=66 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:58:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=67 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:58:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=68 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:58:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=69 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:58:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=70 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:58:50 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-10 10:58:50 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-10 10:58:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-09-10 10:58:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:58:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=74, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:58:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-10 10:58:53 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=147, loss_sum=101.863564, avg_loss=0.692949, seen=147, correct=86, accuracy=0.585034
2025-09-10 10:58:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:58:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:58:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:58:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2240MB allocated=2204MB
2025-09-10 10:58:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:58:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:58:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:58:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:58:59 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.744370, avg_loss=0.643609, seen=40, correct=23, accuracy=0.575000
2025-09-10 10:58:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:58:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:59:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:59:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2240MB allocated=2204MB
2025-09-10 10:59:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=71 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:59:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=72 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:59:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=73 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:59:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=74 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:59:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=75 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:59:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=76 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:59:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=77 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:59:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=78 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:59:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=79 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:59:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=80 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:59:13 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-10 10:59:13 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-10 10:59:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-09-10 10:59:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:59:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=74, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:59:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-10 10:59:15 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=147, loss_sum=102.048294, avg_loss=0.694206, seen=147, correct=80, accuracy=0.544218
2025-09-10 10:59:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:59:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:59:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:59:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2240MB allocated=2204MB
2025-09-10 10:59:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:59:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:59:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:59:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:59:19 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.811329, avg_loss=0.670283, seen=40, correct=22, accuracy=0.550000
2025-09-10 10:59:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:59:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:59:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:59:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2240MB allocated=2204MB
2025-09-10 10:59:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=81 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:59:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=82 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:59:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=83 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:59:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=84 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:59:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=85 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:59:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=86 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:59:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=87 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:59:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=88 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:59:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=89 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:59:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=90 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:59:27 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-10 10:59:28 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-10 10:59:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-09-10 10:59:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:59:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=74, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:59:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-10 10:59:30 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=147, loss_sum=101.829842, avg_loss=0.692720, seen=147, correct=79, accuracy=0.537415
2025-09-10 10:59:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:59:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:59:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:59:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2240MB allocated=2204MB
2025-09-10 10:59:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:59:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:59:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:59:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:59:37 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.480106, avg_loss=0.637003, seen=40, correct=21, accuracy=0.525000
2025-09-10 10:59:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:59:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:59:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:59:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2240MB allocated=2204MB
2025-09-10 10:59:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=91 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:59:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=92 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:59:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=93 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:59:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=94 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:59:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=95 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:59:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=96 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:59:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=97 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:59:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=98 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:59:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=99 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:59:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=100 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 10:59:49 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-10 10:59:50 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-10 10:59:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-09-10 10:59:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:59:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=74, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:59:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-10 10:59:52 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=147, loss_sum=100.512512, avg_loss=0.683759, seen=147, correct=80, accuracy=0.544218
2025-09-10 10:59:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:59:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:59:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:59:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2240MB allocated=2204MB
2025-09-10 10:59:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 10:59:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:59:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 10:59:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 10:59:57 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.463135, avg_loss=0.636578, seen=40, correct=24, accuracy=0.600000
2025-09-10 10:59:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 10:59:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:59:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:59:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2240MB allocated=2204MB
2025-09-10 10:59:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-10 10:59:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-10 10:59:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 10:59:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 10:59:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2240MB allocated=2204MB
2025-09-10 10:59:59 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #47', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-10 10:59:59 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #47', 'Round': 0, 'Results_raw': {}}
2025-09-10 11:00:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 11:00:00 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-10 11:00:00 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-10 11:00:00 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-10 11:00:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-10 11:00:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:00:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=6, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:00:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-10 11:00:04 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=126.516037, avg_loss=0.672958, seen=188, correct=110, accuracy=0.585106
2025-09-10 11:00:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:00:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:00:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:00:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 11:00:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:00:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:00:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:00:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:00:10 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.840139, avg_loss=0.671003, seen=40, correct=25, accuracy=0.625000
2025-09-10 11:00:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:00:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:00:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:00:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 11:00:11 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-10 11:00:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=893, total=3572)
2025-09-10 11:00:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:00:12 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-10 11:00:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:00:12 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=447, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-10 11:00:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:00:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:00:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:00:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:00:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:00:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:00:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:00:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:00:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:00:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:00:18 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-10 11:00:20 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-10 11:00:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-10 11:00:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:00:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:00:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-10 11:00:23 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=122.977577, avg_loss=0.654136, seen=188, correct=114, accuracy=0.606383
2025-09-10 11:00:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:00:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:00:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:00:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:00:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:00:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:00:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:00:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:00:27 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.820309, avg_loss=0.695508, seen=40, correct=21, accuracy=0.525000
2025-09-10 11:00:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:00:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:00:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:00:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:00:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:00:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:00:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:00:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:00:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:00:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:00:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:00:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:00:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:00:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:00:40 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-10 11:00:40 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-10 11:00:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-10 11:00:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:00:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:00:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-10 11:00:43 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=124.799995, avg_loss=0.663830, seen=188, correct=113, accuracy=0.601064
2025-09-10 11:00:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:00:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:00:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:00:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:00:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:00:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:00:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:00:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:00:49 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.180815, avg_loss=0.679520, seen=40, correct=22, accuracy=0.550000
2025-09-10 11:00:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:00:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:00:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:00:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:00:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:00:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:00:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:00:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:00:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:00:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:00:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:00:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:00:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:00:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:00:57 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-10 11:00:58 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-10 11:00:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-10 11:00:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:00:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:01:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-10 11:01:01 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=124.127167, avg_loss=0.660251, seen=188, correct=116, accuracy=0.617021
2025-09-10 11:01:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:01:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:01:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:01:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:01:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:01:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:01:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:01:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:01:04 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.810432, avg_loss=0.670261, seen=40, correct=23, accuracy=0.575000
2025-09-10 11:01:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:01:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:01:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:01:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:01:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=31 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:01:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=32 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:01:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=33 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:01:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=34 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:01:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=35 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:01:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=36 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:01:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=37 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:01:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=38 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:01:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=39 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:01:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=40 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:01:13 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-10 11:01:14 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-10 11:01:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-10 11:01:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:01:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:01:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-10 11:01:17 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=123.767731, avg_loss=0.658339, seen=188, correct=114, accuracy=0.606383
2025-09-10 11:01:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:01:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:01:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:01:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:01:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:01:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:01:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:01:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:01:24 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.694202, avg_loss=0.667355, seen=40, correct=23, accuracy=0.575000
2025-09-10 11:01:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:01:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:01:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:01:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:01:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=41 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:01:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=42 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:01:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=43 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:01:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=44 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:01:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=45 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:01:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=46 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:01:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=47 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:01:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=48 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:01:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=49 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:01:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=50 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:01:31 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-10 11:01:32 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-10 11:01:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-10 11:01:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:01:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:01:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-10 11:01:35 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=122.993828, avg_loss=0.654222, seen=188, correct=116, accuracy=0.617021
2025-09-10 11:01:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:01:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:01:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:01:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:01:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:01:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:01:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:01:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:01:39 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.855484, avg_loss=0.671387, seen=40, correct=24, accuracy=0.600000
2025-09-10 11:01:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:01:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:01:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:01:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:01:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=51 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:01:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=52 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:01:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=53 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:01:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=54 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:01:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=55 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:01:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=56 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:01:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=57 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:01:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=58 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:01:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=59 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:01:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=60 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:01:52 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-10 11:01:53 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-10 11:01:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-10 11:01:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:01:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:01:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-10 11:01:56 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=126.005356, avg_loss=0.670241, seen=188, correct=115, accuracy=0.611702
2025-09-10 11:01:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:01:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:01:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:02:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:02:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:02:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:02:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:02:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:02:02 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.773537, avg_loss=0.644338, seen=40, correct=25, accuracy=0.625000
2025-09-10 11:02:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:02:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:02:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:02:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:02:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=61 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:02:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=62 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:02:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=63 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:02:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=64 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:02:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=65 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:02:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=66 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:02:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=67 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:02:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=68 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:02:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=69 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:02:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=70 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:02:16 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-10 11:02:16 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-10 11:02:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-10 11:02:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:02:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:02:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-10 11:02:19 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=127.785454, avg_loss=0.679710, seen=188, correct=110, accuracy=0.585106
2025-09-10 11:02:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:02:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:02:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:02:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:02:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:02:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:02:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:02:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:02:24 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.347004, avg_loss=0.658675, seen=40, correct=25, accuracy=0.625000
2025-09-10 11:02:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:02:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:02:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:02:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:02:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=71 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:02:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=72 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:02:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=73 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:02:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=74 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:02:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=75 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:02:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=76 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:02:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=77 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:02:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=78 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:02:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=79 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:02:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=80 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:02:37 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-10 11:02:39 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-10 11:02:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-10 11:02:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:02:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:02:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-10 11:02:41 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=125.741783, avg_loss=0.668839, seen=188, correct=109, accuracy=0.579787
2025-09-10 11:02:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:02:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:02:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:02:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:02:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:02:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:02:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:02:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:02:47 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.922413, avg_loss=0.673060, seen=40, correct=23, accuracy=0.575000
2025-09-10 11:02:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:02:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:02:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:02:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:02:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=81 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:02:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=82 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:02:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=83 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:02:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=84 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:02:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=85 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:02:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=86 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:02:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=87 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:02:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=88 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:02:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=89 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:02:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=90 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:02:57 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-10 11:02:57 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-10 11:02:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-10 11:02:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:02:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:03:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-10 11:03:01 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=122.152672, avg_loss=0.649748, seen=188, correct=119, accuracy=0.632979
2025-09-10 11:03:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:03:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:03:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:03:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:03:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:03:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:03:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:03:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:03:05 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.513474, avg_loss=0.687837, seen=40, correct=23, accuracy=0.575000
2025-09-10 11:03:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:03:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:03:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:03:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:03:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=91 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:03:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=92 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:03:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=93 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:03:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=94 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:03:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=95 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:03:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=96 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:03:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=97 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:03:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=98 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:03:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=99 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:03:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=100 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:03:13 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-10 11:03:15 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-10 11:03:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-10 11:03:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:03:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:03:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-10 11:03:18 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=124.112610, avg_loss=0.660173, seen=188, correct=120, accuracy=0.638298
2025-09-10 11:03:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:03:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:03:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:03:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:03:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:03:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:03:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:03:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:03:27 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.985950, avg_loss=0.674649, seen=40, correct=25, accuracy=0.625000
2025-09-10 11:03:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:03:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:03:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:03:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:03:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-10 11:03:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-10 11:03:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:03:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:03:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:03:30 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #9', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-10 11:03:30 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #9', 'Round': 0, 'Results_raw': {}}
2025-09-10 11:03:30 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 11:03:30 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-10 11:03:31 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-10 11:03:31 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-10 11:03:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-09-10 11:03:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:03:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=80, num_train_batch_last_epoch=20, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:03:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-09-10 11:03:34 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=160, loss_sum=105.378448, avg_loss=0.658615, seen=160, correct=98, accuracy=0.612500
2025-09-10 11:03:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:03:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:03:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:03:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 11:03:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:03:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:03:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:03:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:03:38 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.685223, avg_loss=0.592131, seen=40, correct=29, accuracy=0.725000
2025-09-10 11:03:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:03:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:03:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:03:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 11:03:39 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-10 11:03:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=764, total=3055)
2025-09-10 11:03:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:03:40 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-10 11:03:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:03:40 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=382, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-10 11:03:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:03:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:03:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:03:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:03:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:03:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:03:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:03:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:03:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:03:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:03:46 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-10 11:03:47 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-10 11:03:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-09-10 11:03:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:03:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=80, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:03:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-09-10 11:03:49 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=160, loss_sum=109.482819, avg_loss=0.684268, seen=160, correct=94, accuracy=0.587500
2025-09-10 11:03:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:03:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:03:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:03:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:03:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:03:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:03:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:03:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:03:55 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.958677, avg_loss=0.648967, seen=40, correct=25, accuracy=0.625000
2025-09-10 11:03:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:03:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:03:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:03:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:04:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:04:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:04:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:04:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:04:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:04:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:04:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:04:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:04:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:04:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:04:07 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-10 11:04:07 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-10 11:04:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-09-10 11:04:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:04:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=80, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:04:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-09-10 11:04:10 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=160, loss_sum=106.267822, avg_loss=0.664174, seen=160, correct=100, accuracy=0.625000
2025-09-10 11:04:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:04:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:04:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:04:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:04:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:04:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:04:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:04:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:04:15 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.699865, avg_loss=0.592497, seen=40, correct=29, accuracy=0.725000
2025-09-10 11:04:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:04:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:04:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:04:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:04:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:04:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:04:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:04:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:04:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:04:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:04:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:04:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:04:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:04:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:04:26 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-10 11:04:26 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-10 11:04:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-09-10 11:04:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:04:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=80, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:04:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-09-10 11:04:29 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=160, loss_sum=110.557449, avg_loss=0.690984, seen=160, correct=100, accuracy=0.625000
2025-09-10 11:04:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:04:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:04:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:04:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:04:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:04:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:04:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:04:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:04:34 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.412628, avg_loss=0.610316, seen=40, correct=26, accuracy=0.650000
2025-09-10 11:04:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:04:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:04:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:04:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:04:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=31 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:04:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=32 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:04:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=33 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:04:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=34 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:04:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=35 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:04:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=36 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:04:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=37 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:04:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=38 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:04:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=39 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:04:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=40 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:04:45 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-10 11:04:46 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-10 11:04:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-09-10 11:04:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:04:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=80, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:04:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-09-10 11:04:49 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=160, loss_sum=106.625000, avg_loss=0.666406, seen=160, correct=105, accuracy=0.656250
2025-09-10 11:04:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:04:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:04:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:04:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:04:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:04:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:04:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:04:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:04:53 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.269772, avg_loss=0.631744, seen=40, correct=27, accuracy=0.675000
2025-09-10 11:04:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:04:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:04:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:04:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:04:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=41 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:04:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=42 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:04:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=43 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:04:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=44 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:05:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=45 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:05:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=46 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:05:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=47 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:05:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=48 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:05:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=49 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:05:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=50 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:05:04 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-10 11:05:05 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-10 11:05:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-09-10 11:05:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:05:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=80, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:05:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-09-10 11:05:08 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=160, loss_sum=104.000160, avg_loss=0.650001, seen=160, correct=102, accuracy=0.637500
2025-09-10 11:05:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:05:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:05:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:05:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:05:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:05:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:05:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:05:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:05:11 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.715086, avg_loss=0.617877, seen=40, correct=29, accuracy=0.725000
2025-09-10 11:05:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:05:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:05:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:05:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:05:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=51 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:05:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=52 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:05:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=53 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:05:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=54 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:05:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=55 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:05:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=56 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:05:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=57 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:05:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=58 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:05:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=59 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:05:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=60 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:05:20 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-10 11:05:22 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-10 11:05:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-09-10 11:05:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:05:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=80, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:05:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-09-10 11:05:24 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=160, loss_sum=106.033806, avg_loss=0.662711, seen=160, correct=93, accuracy=0.581250
2025-09-10 11:05:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:05:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:05:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:05:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:05:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:05:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:05:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:05:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:05:31 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.414179, avg_loss=0.610354, seen=40, correct=26, accuracy=0.650000
2025-09-10 11:05:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:05:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:05:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:05:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:05:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=61 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:05:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=62 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:05:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=63 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:05:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=64 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:05:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=65 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:05:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=66 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:05:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=67 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:05:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=68 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:05:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=69 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:05:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=70 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:05:40 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-10 11:05:40 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-10 11:05:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-09-10 11:05:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:05:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=80, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:05:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-09-10 11:05:43 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=160, loss_sum=107.601952, avg_loss=0.672512, seen=160, correct=89, accuracy=0.556250
2025-09-10 11:05:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:05:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:05:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:05:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:05:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:05:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:05:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:05:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:05:48 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.533688, avg_loss=0.613342, seen=40, correct=25, accuracy=0.625000
2025-09-10 11:05:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:05:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:05:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:05:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:05:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=71 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:05:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=72 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:05:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=73 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:05:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=74 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:05:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=75 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:05:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=76 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:05:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=77 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:05:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=78 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:05:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=79 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:05:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=80 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:05:59 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-10 11:05:59 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-10 11:06:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-09-10 11:06:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:06:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=80, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:06:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-09-10 11:06:02 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=160, loss_sum=104.786018, avg_loss=0.654913, seen=160, correct=92, accuracy=0.575000
2025-09-10 11:06:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:06:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:06:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:06:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:06:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:06:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:06:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:06:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:06:07 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.917614, avg_loss=0.597940, seen=40, correct=28, accuracy=0.700000
2025-09-10 11:06:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:06:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:06:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:06:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:06:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=81 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:06:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=82 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:06:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=83 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:06:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=84 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:06:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=85 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:06:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=86 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:06:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=87 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:06:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=88 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:06:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=89 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:06:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=90 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:06:19 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-10 11:06:19 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-10 11:06:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-09-10 11:06:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:06:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=80, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:06:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-09-10 11:06:22 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=160, loss_sum=103.087631, avg_loss=0.644298, seen=160, correct=97, accuracy=0.606250
2025-09-10 11:06:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:06:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:06:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:06:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:06:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:06:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:06:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:06:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:06:27 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.003637, avg_loss=0.625091, seen=40, correct=28, accuracy=0.700000
2025-09-10 11:06:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:06:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:06:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:06:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:06:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=91 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:06:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=92 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:06:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=93 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:06:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=94 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:06:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=95 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:06:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=96 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:06:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=97 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:06:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=98 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:06:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=99 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:06:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=100 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:06:38 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-10 11:06:40 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-10 11:06:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-09-10 11:06:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:06:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=80, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:06:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-09-10 11:06:42 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=160, loss_sum=102.900291, avg_loss=0.643127, seen=160, correct=97, accuracy=0.606250
2025-09-10 11:06:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:06:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:06:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:06:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:06:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:06:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:06:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:06:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:06:45 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.243416, avg_loss=0.631085, seen=40, correct=28, accuracy=0.700000
2025-09-10 11:06:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:06:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:06:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:06:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:06:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-10 11:06:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-10 11:06:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:06:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:06:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:06:48 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #14', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-10 11:06:48 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #14', 'Round': 0, 'Results_raw': {}}
2025-09-10 11:06:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 11:06:48 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-10 11:06:49 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-10 11:06:49 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-10 11:06:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-09-10 11:06:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:06:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=81, num_train_batch_last_epoch=19, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:06:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-09-10 11:06:52 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=161, loss_sum=100.699905, avg_loss=0.625465, seen=161, correct=107, accuracy=0.664596
2025-09-10 11:06:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:06:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:06:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:06:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 11:06:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:06:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:06:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:06:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:06:56 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.701393, avg_loss=0.617535, seen=40, correct=27, accuracy=0.675000
2025-09-10 11:06:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:06:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:06:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:06:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 11:06:58 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-10 11:06:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=766, total=3063)
2025-09-10 11:06:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:06:59 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-10 11:06:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:06:59 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=383, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-10 11:07:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:07:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:07:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:07:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:07:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:07:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:07:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:07:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:07:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:07:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:07:06 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-10 11:07:06 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-10 11:07:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-09-10 11:07:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:07:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=81, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:07:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-09-10 11:07:08 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=161, loss_sum=101.473618, avg_loss=0.630271, seen=161, correct=106, accuracy=0.658385
2025-09-10 11:07:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:07:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:07:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:07:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:07:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:07:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:07:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:07:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:07:14 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.842709, avg_loss=0.621068, seen=40, correct=28, accuracy=0.700000
2025-09-10 11:07:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:07:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:07:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:07:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:07:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:07:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:07:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:07:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:07:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:07:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:07:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:07:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:07:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:07:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:07:21 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-10 11:07:22 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-10 11:07:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-09-10 11:07:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:07:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=81, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:07:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-09-10 11:07:24 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=161, loss_sum=101.017288, avg_loss=0.627437, seen=161, correct=104, accuracy=0.645963
2025-09-10 11:07:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:07:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:07:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:07:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:07:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:07:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:07:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:07:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:07:28 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.199877, avg_loss=0.629997, seen=40, correct=26, accuracy=0.650000
2025-09-10 11:07:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:07:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:07:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:07:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:07:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:07:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:07:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:07:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:07:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:07:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:07:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:07:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:07:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:07:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:07:36 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-10 11:07:38 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-10 11:07:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-09-10 11:07:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:07:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=81, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:07:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-09-10 11:07:40 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=161, loss_sum=97.831772, avg_loss=0.607651, seen=161, correct=109, accuracy=0.677019
2025-09-10 11:07:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:07:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:07:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:07:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:07:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:07:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:07:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:07:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:07:45 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.099089, avg_loss=0.627477, seen=40, correct=28, accuracy=0.700000
2025-09-10 11:07:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:07:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:07:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:07:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:07:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=31 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:07:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=32 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:07:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=33 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:07:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=34 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:07:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=35 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:07:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=36 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:07:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=37 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:07:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=38 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:07:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=39 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:07:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=40 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:07:56 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-10 11:07:56 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-10 11:07:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-09-10 11:07:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:07:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=81, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:07:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-09-10 11:07:58 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=161, loss_sum=99.023697, avg_loss=0.615054, seen=161, correct=103, accuracy=0.639752
2025-09-10 11:07:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:07:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:08:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:08:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:08:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:08:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:08:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:08:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:08:02 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.170885, avg_loss=0.629272, seen=40, correct=28, accuracy=0.700000
2025-09-10 11:08:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:08:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:08:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:08:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:08:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=41 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:08:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=42 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:08:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=43 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:08:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=44 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:08:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=45 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:08:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=46 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:08:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=47 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:08:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=48 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:08:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=49 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:08:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=50 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:08:11 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-10 11:08:11 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-10 11:08:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-09-10 11:08:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:08:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=81, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:08:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-09-10 11:08:14 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=161, loss_sum=100.789551, avg_loss=0.626022, seen=161, correct=101, accuracy=0.627329
2025-09-10 11:08:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:08:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:08:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:08:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:08:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:08:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:08:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:08:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:08:21 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.280289, avg_loss=0.632007, seen=40, correct=26, accuracy=0.650000
2025-09-10 11:08:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:08:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:08:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:08:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:08:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=51 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:08:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=52 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:08:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=53 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:08:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=54 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:08:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=55 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:08:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=56 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:08:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=57 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:08:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=58 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:08:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=59 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:08:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=60 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:08:29 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-10 11:08:30 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-10 11:08:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-09-10 11:08:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:08:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=81, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:08:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-09-10 11:08:32 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=161, loss_sum=99.330360, avg_loss=0.616959, seen=161, correct=109, accuracy=0.677019
2025-09-10 11:08:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:08:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:08:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:08:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:08:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:08:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:08:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:08:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:08:39 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.434395, avg_loss=0.610860, seen=40, correct=28, accuracy=0.700000
2025-09-10 11:08:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:08:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:08:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:08:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:08:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=61 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:08:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=62 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:08:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=63 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:08:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=64 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:08:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=65 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:08:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=66 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:08:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=67 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:08:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=68 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:08:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=69 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:08:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=70 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:08:47 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-10 11:08:49 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-10 11:08:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-09-10 11:08:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:08:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=81, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:08:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-09-10 11:08:51 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=161, loss_sum=99.381958, avg_loss=0.617279, seen=161, correct=107, accuracy=0.664596
2025-09-10 11:08:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:08:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:08:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:08:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:08:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:08:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:08:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:08:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:08:58 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.335777, avg_loss=0.608394, seen=40, correct=28, accuracy=0.700000
2025-09-10 11:08:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:08:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:09:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:09:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:09:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=71 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:09:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=72 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:09:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=73 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:09:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=74 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:09:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=75 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:09:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=76 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:09:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=77 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:09:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=78 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:09:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=79 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:09:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=80 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:09:09 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-10 11:09:09 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-10 11:09:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-09-10 11:09:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:09:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=81, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:09:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-09-10 11:09:11 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=161, loss_sum=100.669373, avg_loss=0.625276, seen=161, correct=107, accuracy=0.664596
2025-09-10 11:09:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:09:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:09:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:09:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:09:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:09:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:09:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:09:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:09:16 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.695904, avg_loss=0.617398, seen=40, correct=27, accuracy=0.675000
2025-09-10 11:09:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:09:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:09:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:09:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:09:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=81 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:09:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=82 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:09:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=83 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:09:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=84 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:09:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=85 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:09:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=86 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:09:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=87 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:09:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=88 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:09:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=89 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:09:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=90 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:09:29 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-10 11:09:29 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-10 11:09:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-09-10 11:09:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:09:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=81, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:09:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-09-10 11:09:31 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=161, loss_sum=103.951752, avg_loss=0.645663, seen=161, correct=102, accuracy=0.633540
2025-09-10 11:09:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:09:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:09:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:09:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:09:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:09:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:09:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:09:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:09:37 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.641939, avg_loss=0.641048, seen=40, correct=26, accuracy=0.650000
2025-09-10 11:09:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:09:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:09:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:09:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:09:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=91 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:09:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=92 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:09:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=93 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:09:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=94 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:09:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=95 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:09:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=96 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:09:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=97 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:09:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=98 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:09:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=99 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:09:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=100 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:09:51 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-10 11:09:51 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-10 11:09:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-09-10 11:09:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:09:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=81, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:09:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-09-10 11:09:54 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=161, loss_sum=107.294388, avg_loss=0.666425, seen=161, correct=97, accuracy=0.602484
2025-09-10 11:09:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:09:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:09:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:09:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:09:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:09:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:09:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:10:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:10:00 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.428242, avg_loss=0.660706, seen=40, correct=24, accuracy=0.600000
2025-09-10 11:10:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:10:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:10:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:10:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:10:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-10 11:10:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-10 11:10:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:10:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:10:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:10:04 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #26', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-10 11:10:04 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #26', 'Round': 0, 'Results_raw': {}}
2025-09-10 11:10:04 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 11:10:04 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-10 11:10:05 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-10 11:10:05 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-10 11:10:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-09-10 11:10:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:10:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=32, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:10:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 11:10:09 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=135, loss_sum=93.260048, avg_loss=0.690815, seen=135, correct=84, accuracy=0.622222
2025-09-10 11:10:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:10:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:10:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:10:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 11:10:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:10:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:10:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:10:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:10:12 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.197376, avg_loss=0.604934, seen=40, correct=26, accuracy=0.650000
2025-09-10 11:10:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:10:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:10:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:10:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 11:10:14 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-10 11:10:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=644, total=2576)
2025-09-10 11:10:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:10:14 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-10 11:10:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:10:14 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=322, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-10 11:10:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:10:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:10:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:10:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:10:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:10:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:10:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:10:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:10:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:10:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:10:20 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-10 11:10:21 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-10 11:10:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-09-10 11:10:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:10:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:10:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 11:10:23 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=135, loss_sum=96.107506, avg_loss=0.711907, seen=135, correct=78, accuracy=0.577778
2025-09-10 11:10:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:10:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:10:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:10:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:10:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:10:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:10:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:10:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:10:31 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.591103, avg_loss=0.614778, seen=40, correct=26, accuracy=0.650000
2025-09-10 11:10:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:10:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:10:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:10:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:10:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:10:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:10:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:10:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:10:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:10:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:10:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:10:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:10:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:10:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:10:40 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-10 11:10:40 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-10 11:10:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-09-10 11:10:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:10:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:10:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 11:10:42 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=135, loss_sum=96.257904, avg_loss=0.713022, seen=135, correct=74, accuracy=0.548148
2025-09-10 11:10:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:10:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:10:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:10:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:10:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:10:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:10:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:10:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:10:46 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.766163, avg_loss=0.694154, seen=40, correct=24, accuracy=0.600000
2025-09-10 11:10:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:10:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:10:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:10:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:10:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:10:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:10:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:10:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:10:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:10:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:10:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:10:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:10:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:10:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:10:59 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-10 11:11:00 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-10 11:11:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-09-10 11:11:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:11:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:11:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 11:11:02 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=135, loss_sum=96.221367, avg_loss=0.712751, seen=135, correct=72, accuracy=0.533333
2025-09-10 11:11:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:11:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:11:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:11:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:11:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:11:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:11:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:11:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:11:06 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.936054, avg_loss=0.698401, seen=40, correct=20, accuracy=0.500000
2025-09-10 11:11:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:11:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:11:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:11:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:11:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=31 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:11:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=32 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:11:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=33 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:11:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=34 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:11:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=35 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:11:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=36 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:11:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=37 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:11:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=38 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:11:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=39 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:11:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=40 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:11:15 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-10 11:11:19 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-10 11:11:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-09-10 11:11:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:11:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:11:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 11:11:21 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=135, loss_sum=95.324692, avg_loss=0.706109, seen=135, correct=75, accuracy=0.555556
2025-09-10 11:11:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:11:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:11:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:11:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:11:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:11:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:11:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:11:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:11:24 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.759012, avg_loss=0.693975, seen=40, correct=19, accuracy=0.475000
2025-09-10 11:11:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:11:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:11:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:11:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:11:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=41 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:11:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=42 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:11:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=43 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:11:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=44 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:11:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=45 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:11:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=46 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:11:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=47 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:11:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=48 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:11:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=49 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:11:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=50 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:11:32 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-10 11:11:34 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-10 11:11:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-09-10 11:11:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:11:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:11:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 11:11:36 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=135, loss_sum=94.215652, avg_loss=0.697894, seen=135, correct=77, accuracy=0.570370
2025-09-10 11:11:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:11:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:11:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:11:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:11:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:11:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:11:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:11:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:11:39 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.629168, avg_loss=0.690729, seen=40, correct=21, accuracy=0.525000
2025-09-10 11:11:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:11:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:11:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:11:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:11:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=51 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:11:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=52 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:11:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=53 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:11:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=54 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:11:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=55 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:11:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=56 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:11:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=57 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:11:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=58 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:11:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=59 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:11:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=60 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:11:50 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-10 11:11:50 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-10 11:11:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-09-10 11:11:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:11:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:11:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 11:11:52 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=135, loss_sum=92.369728, avg_loss=0.684220, seen=135, correct=81, accuracy=0.600000
2025-09-10 11:11:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:11:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:11:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:11:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:11:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:11:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:11:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:11:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:11:57 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.358028, avg_loss=0.658951, seen=40, correct=24, accuracy=0.600000
2025-09-10 11:11:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:11:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:11:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:12:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:12:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=61 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:12:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=62 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:12:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=63 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:12:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=64 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:12:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=65 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:12:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=66 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:12:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=67 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:12:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=68 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:12:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=69 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:12:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=70 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:12:06 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-10 11:12:06 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-10 11:12:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-09-10 11:12:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:12:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:12:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 11:12:08 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=135, loss_sum=91.771477, avg_loss=0.679789, seen=135, correct=84, accuracy=0.622222
2025-09-10 11:12:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:12:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:12:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:12:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:12:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:12:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:12:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:12:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:12:16 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.288036, avg_loss=0.632201, seen=40, correct=27, accuracy=0.675000
2025-09-10 11:12:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:12:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:12:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:12:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:12:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=71 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:12:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=72 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:12:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=73 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:12:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=74 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:12:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=75 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:12:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=76 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:12:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=77 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:12:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=78 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:12:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=79 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:12:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=80 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:12:24 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-10 11:12:24 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-10 11:12:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-09-10 11:12:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:12:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:12:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 11:12:26 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=135, loss_sum=92.735168, avg_loss=0.686927, seen=135, correct=82, accuracy=0.607407
2025-09-10 11:12:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:12:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:12:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:12:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:12:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:12:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:12:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:12:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:12:30 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.411060, avg_loss=0.635277, seen=40, correct=28, accuracy=0.700000
2025-09-10 11:12:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:12:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:12:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:12:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:12:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=81 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:12:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=82 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:12:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=83 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:12:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=84 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:12:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=85 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:12:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=86 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:12:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=87 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:12:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=88 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:12:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=89 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:12:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=90 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:12:43 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-10 11:12:43 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-10 11:12:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-09-10 11:12:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:12:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:12:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 11:12:46 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=135, loss_sum=92.536613, avg_loss=0.685456, seen=135, correct=79, accuracy=0.585185
2025-09-10 11:12:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:12:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:12:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:12:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:12:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:12:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:12:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:12:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:12:50 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.608337, avg_loss=0.640208, seen=40, correct=25, accuracy=0.625000
2025-09-10 11:12:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:12:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:12:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:12:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:12:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=91 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:12:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=92 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:12:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=93 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:12:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=94 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:12:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=95 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:12:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=96 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:12:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=97 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:12:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=98 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:12:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=99 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:12:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=100 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:12:58 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-10 11:13:02 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-10 11:13:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-09-10 11:13:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:13:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:13:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 11:13:04 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=135, loss_sum=93.013809, avg_loss=0.688991, seen=135, correct=77, accuracy=0.570370
2025-09-10 11:13:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:13:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:13:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:13:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:13:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:13:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:13:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:13:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:13:09 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.344563, avg_loss=0.633614, seen=40, correct=24, accuracy=0.600000
2025-09-10 11:13:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:13:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:13:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:13:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:13:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-10 11:13:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-10 11:13:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:13:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:13:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:13:11 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #18', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-10 11:13:11 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #18', 'Round': 0, 'Results_raw': {}}
2025-09-10 11:13:11 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 11:13:11 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-10 11:13:12 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-10 11:13:12 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-10 11:13:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-10 11:13:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:13:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=6, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:13:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-10 11:13:16 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=126.428444, avg_loss=0.672492, seen=188, correct=109, accuracy=0.579787
2025-09-10 11:13:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:13:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:13:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:13:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 11:13:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:13:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:13:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:13:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:13:20 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.729603, avg_loss=0.643240, seen=40, correct=24, accuracy=0.600000
2025-09-10 11:13:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:13:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:13:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:13:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 11:13:22 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-10 11:13:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=898, total=3589)
2025-09-10 11:13:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:13:22 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-10 11:13:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:13:22 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=449, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-10 11:13:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:13:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:13:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:13:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:13:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:13:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:13:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:13:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:13:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:13:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:13:29 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-10 11:13:29 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-10 11:13:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-10 11:13:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:13:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:13:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-10 11:13:32 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=126.621414, avg_loss=0.673518, seen=188, correct=109, accuracy=0.579787
2025-09-10 11:13:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:13:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:13:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:13:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:13:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:13:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:13:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:13:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:13:36 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.245882, avg_loss=0.631147, seen=40, correct=25, accuracy=0.625000
2025-09-10 11:13:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:13:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:13:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:13:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:13:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:13:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:13:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:13:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:13:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:13:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:13:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:13:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:13:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:13:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:13:46 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-10 11:13:46 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-10 11:13:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-10 11:13:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:13:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:13:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-10 11:13:49 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=126.971138, avg_loss=0.675378, seen=188, correct=111, accuracy=0.590426
2025-09-10 11:13:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:13:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:13:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:13:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:13:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:13:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:13:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:13:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:13:52 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.018747, avg_loss=0.625469, seen=40, correct=26, accuracy=0.650000
2025-09-10 11:13:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:13:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:13:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:13:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:13:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:13:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:13:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:13:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:13:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:14:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:14:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:14:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:14:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:14:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:14:03 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-10 11:14:03 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-10 11:14:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-10 11:14:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:14:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:14:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-10 11:14:06 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=127.294098, avg_loss=0.677096, seen=188, correct=111, accuracy=0.590426
2025-09-10 11:14:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:14:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:14:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:14:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:14:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:14:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:14:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:14:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:14:10 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.151077, avg_loss=0.628777, seen=40, correct=26, accuracy=0.650000
2025-09-10 11:14:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:14:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:14:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:14:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:14:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=31 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:14:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=32 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:14:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=33 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:14:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=34 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:14:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=35 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:14:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=36 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:14:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=37 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:14:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=38 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:14:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=39 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:14:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=40 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:14:19 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-10 11:14:19 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-10 11:14:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-10 11:14:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:14:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:14:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-10 11:14:22 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=128.272980, avg_loss=0.682303, seen=188, correct=108, accuracy=0.574468
2025-09-10 11:14:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:14:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:14:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:14:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:14:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:14:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:14:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:14:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:14:28 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.481346, avg_loss=0.637034, seen=40, correct=26, accuracy=0.650000
2025-09-10 11:14:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:14:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:14:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:14:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:14:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=41 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:14:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=42 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:14:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=43 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:14:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=44 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:14:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=45 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:14:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=46 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:14:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=47 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:14:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=48 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:14:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=49 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:14:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=50 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:14:38 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-10 11:14:38 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-10 11:14:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-10 11:14:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:14:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:14:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-10 11:14:41 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=126.768585, avg_loss=0.674301, seen=188, correct=110, accuracy=0.585106
2025-09-10 11:14:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:14:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:14:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:14:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:14:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:14:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:14:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:14:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:14:44 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.561020, avg_loss=0.639025, seen=40, correct=26, accuracy=0.650000
2025-09-10 11:14:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:14:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:14:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:14:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:14:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=51 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:14:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=52 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:14:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=53 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:14:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=54 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:14:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=55 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:14:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=56 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:14:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=57 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:14:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=58 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:14:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=59 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:14:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=60 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:14:55 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-10 11:14:56 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-10 11:14:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-10 11:14:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:14:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:14:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-10 11:14:59 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=126.273033, avg_loss=0.671665, seen=188, correct=115, accuracy=0.611702
2025-09-10 11:14:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:14:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:15:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:15:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:15:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:15:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:15:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:15:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:15:02 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.579805, avg_loss=0.639495, seen=40, correct=25, accuracy=0.625000
2025-09-10 11:15:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:15:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:15:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:15:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:15:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=61 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:15:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=62 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:15:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=63 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:15:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=64 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:15:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=65 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:15:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=66 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:15:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=67 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:15:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=68 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:15:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=69 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:15:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=70 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:15:10 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-10 11:15:12 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-10 11:15:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-10 11:15:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:15:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:15:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-10 11:15:15 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=126.056114, avg_loss=0.670511, seen=188, correct=112, accuracy=0.595745
2025-09-10 11:15:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:15:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:15:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:15:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:15:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:15:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:15:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:15:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:15:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.360355, avg_loss=0.634009, seen=40, correct=25, accuracy=0.625000
2025-09-10 11:15:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:15:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:15:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:15:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:15:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=71 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:15:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=72 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:15:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=73 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:15:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=74 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:15:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=75 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:15:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=76 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:15:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=77 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:15:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=78 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:15:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=79 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:15:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=80 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:15:28 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-10 11:15:29 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-10 11:15:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-10 11:15:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:15:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:15:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-10 11:15:32 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=126.562737, avg_loss=0.673206, seen=188, correct=110, accuracy=0.585106
2025-09-10 11:15:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:15:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:15:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:15:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:15:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:15:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:15:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:15:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:15:36 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.624487, avg_loss=0.640612, seen=40, correct=27, accuracy=0.675000
2025-09-10 11:15:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:15:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:15:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:15:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:15:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=81 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:15:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=82 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:15:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=83 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:15:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=84 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:15:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=85 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:15:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=86 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:15:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=87 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:15:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=88 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:15:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=89 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:15:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=90 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:15:46 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-10 11:15:46 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-10 11:15:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-10 11:15:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:15:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:15:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-10 11:15:49 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=126.031540, avg_loss=0.670381, seen=188, correct=114, accuracy=0.606383
2025-09-10 11:15:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:15:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:15:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:15:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:15:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:15:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:15:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:15:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:15:53 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.574369, avg_loss=0.614359, seen=40, correct=26, accuracy=0.650000
2025-09-10 11:15:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:15:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:15:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:15:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:15:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=91 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:15:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=92 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:15:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=93 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:15:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=94 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:15:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=95 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:15:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=96 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:15:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=97 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:16:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=98 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:16:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=99 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:16:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=100 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:16:01 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-10 11:16:03 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-10 11:16:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-10 11:16:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:16:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:16:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-10 11:16:06 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=123.995567, avg_loss=0.659551, seen=188, correct=112, accuracy=0.595745
2025-09-10 11:16:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:16:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:16:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:16:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:16:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:16:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:16:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:16:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:16:14 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.363234, avg_loss=0.634081, seen=40, correct=27, accuracy=0.675000
2025-09-10 11:16:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:16:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:16:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:16:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:16:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-10 11:16:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-10 11:16:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:16:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:16:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:16:17 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #52', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-10 11:16:17 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #52', 'Round': 0, 'Results_raw': {}}
2025-09-10 11:16:17 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 11:16:17 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-10 11:16:18 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-10 11:16:18 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-10 11:16:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-09-10 11:16:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:16:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=45, num_train_batch_last_epoch=10, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:16:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-09-10 11:16:20 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=89, loss_sum=58.462402, avg_loss=0.656881, seen=89, correct=56, accuracy=0.629213
2025-09-10 11:16:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:16:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:16:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:16:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 11:16:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:16:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:16:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:16:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:16:24 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.518379, avg_loss=0.662959, seen=40, correct=25, accuracy=0.625000
2025-09-10 11:16:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:16:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:16:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:16:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 11:16:25 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-10 11:16:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=424, total=1694)
2025-09-10 11:16:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:16:26 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-10 11:16:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:16:26 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=212, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-10 11:16:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:16:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:16:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:16:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:16:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:16:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:16:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:16:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:16:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:16:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:16:34 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-10 11:16:37 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-10 11:16:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-09-10 11:16:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:16:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=45, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:16:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-09-10 11:16:38 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=89, loss_sum=59.730740, avg_loss=0.671132, seen=89, correct=54, accuracy=0.606742
2025-09-10 11:16:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:16:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:16:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:16:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:16:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:16:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:16:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:16:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:16:43 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.409052, avg_loss=0.660226, seen=40, correct=23, accuracy=0.575000
2025-09-10 11:16:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:16:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:16:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:16:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:16:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:16:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:16:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:16:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:16:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:16:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:16:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:16:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:16:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:16:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:16:51 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-10 11:16:51 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-10 11:16:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-09-10 11:16:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:16:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=45, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:16:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-09-10 11:16:52 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=89, loss_sum=59.336365, avg_loss=0.666701, seen=89, correct=51, accuracy=0.573034
2025-09-10 11:16:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:16:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:16:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:16:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:16:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:16:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:16:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:16:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:16:58 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.722099, avg_loss=0.643052, seen=40, correct=24, accuracy=0.600000
2025-09-10 11:16:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:16:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:17:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:17:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:17:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:17:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:17:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:17:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:17:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:17:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:17:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:17:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:17:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:17:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:17:08 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-10 11:17:08 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-10 11:17:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-09-10 11:17:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:17:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=45, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:17:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-09-10 11:17:10 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=89, loss_sum=58.786343, avg_loss=0.660521, seen=89, correct=53, accuracy=0.595506
2025-09-10 11:17:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:17:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:17:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:17:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:17:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:17:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:17:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:17:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:17:13 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.689661, avg_loss=0.642242, seen=40, correct=25, accuracy=0.625000
2025-09-10 11:17:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:17:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:17:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:17:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:17:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=31 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:17:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=32 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:17:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=33 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:17:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=34 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:17:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=35 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:17:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=36 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:17:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=37 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:17:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=38 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:17:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=39 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:17:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=40 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:17:23 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-10 11:17:23 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-10 11:17:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-09-10 11:17:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:17:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=45, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:17:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-09-10 11:17:25 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=89, loss_sum=59.178864, avg_loss=0.664931, seen=89, correct=50, accuracy=0.561798
2025-09-10 11:17:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:17:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:17:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:17:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:17:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:17:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:17:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:17:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:17:29 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.249746, avg_loss=0.631244, seen=40, correct=25, accuracy=0.625000
2025-09-10 11:17:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:17:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:17:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:17:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:17:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=41 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:17:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=42 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:17:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=43 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:17:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=44 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:17:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=45 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:17:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=46 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:17:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=47 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:17:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=48 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:17:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=49 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:17:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=50 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:17:37 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-10 11:17:37 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-10 11:17:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-09-10 11:17:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:17:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=45, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:17:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-09-10 11:17:39 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=89, loss_sum=60.156975, avg_loss=0.675921, seen=89, correct=47, accuracy=0.528090
2025-09-10 11:17:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:17:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:17:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:17:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:17:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:17:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:17:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:17:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:17:44 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.191412, avg_loss=0.629785, seen=40, correct=28, accuracy=0.700000
2025-09-10 11:17:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:17:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:17:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:17:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:17:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=51 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:17:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=52 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:17:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=53 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:17:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=54 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:17:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=55 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:17:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=56 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:17:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=57 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:17:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=58 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:17:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=59 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:17:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=60 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:17:55 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-10 11:17:56 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-10 11:17:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-09-10 11:17:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:17:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=45, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:17:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-09-10 11:17:57 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=89, loss_sum=60.240402, avg_loss=0.676858, seen=89, correct=51, accuracy=0.573034
2025-09-10 11:17:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:17:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:18:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:18:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:18:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:18:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:18:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:18:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:18:03 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.213415, avg_loss=0.630335, seen=40, correct=26, accuracy=0.650000
2025-09-10 11:18:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:18:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:18:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:18:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:18:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=61 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:18:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=62 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:18:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=63 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:18:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=64 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:18:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=65 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:18:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=66 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:18:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=67 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:18:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=68 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:18:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=69 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:18:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=70 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:18:14 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-10 11:18:14 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-10 11:18:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-09-10 11:18:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:18:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=45, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:18:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-09-10 11:18:16 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=89, loss_sum=60.212540, avg_loss=0.676545, seen=89, correct=50, accuracy=0.561798
2025-09-10 11:18:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:18:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:18:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:18:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:18:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:18:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:18:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:18:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:18:19 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.335722, avg_loss=0.633393, seen=40, correct=29, accuracy=0.725000
2025-09-10 11:18:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:18:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:18:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:18:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:18:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=71 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:18:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=72 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:18:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=73 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:18:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=74 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:18:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=75 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:18:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=76 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:18:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=77 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:18:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=78 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:18:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=79 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:18:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=80 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:18:27 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-10 11:18:27 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-10 11:18:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-09-10 11:18:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:18:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=45, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:18:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-09-10 11:18:28 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=89, loss_sum=59.738426, avg_loss=0.671218, seen=89, correct=51, accuracy=0.573034
2025-09-10 11:18:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:18:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:18:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:18:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:18:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:18:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:18:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:18:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:18:36 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.621826, avg_loss=0.640546, seen=40, correct=25, accuracy=0.625000
2025-09-10 11:18:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:18:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:18:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:18:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:18:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=81 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:18:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=82 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:18:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=83 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:18:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=84 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:18:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=85 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:18:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=86 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:18:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=87 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:18:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=88 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:18:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=89 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:18:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=90 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:18:45 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-10 11:18:45 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-10 11:18:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-09-10 11:18:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:18:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=45, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:18:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-09-10 11:18:47 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=89, loss_sum=60.615005, avg_loss=0.681067, seen=89, correct=50, accuracy=0.561798
2025-09-10 11:18:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:18:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:18:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:18:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:18:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:18:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:18:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:18:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:18:51 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.344576, avg_loss=0.658614, seen=40, correct=24, accuracy=0.600000
2025-09-10 11:18:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:18:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:18:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:18:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:18:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=91 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:18:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=92 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:18:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=93 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:18:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=94 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:18:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=95 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:18:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=96 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:18:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=97 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:19:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=98 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:19:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=99 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:19:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=100 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:19:01 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-10 11:19:03 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-10 11:19:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-09-10 11:19:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:19:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=45, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:19:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-09-10 11:19:05 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=89, loss_sum=61.479397, avg_loss=0.690780, seen=89, correct=48, accuracy=0.539326
2025-09-10 11:19:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:19:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:19:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:19:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:19:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:19:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:19:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:19:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:19:08 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.423721, avg_loss=0.660593, seen=40, correct=24, accuracy=0.600000
2025-09-10 11:19:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:19:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:19:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:19:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:19:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-10 11:19:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-10 11:19:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:19:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:19:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:19:12 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #43', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-10 11:19:12 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #43', 'Round': 0, 'Results_raw': {}}
2025-09-10 11:19:12 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 11:19:12 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-10 11:19:13 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-10 11:19:13 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-10 11:19:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-10 11:19:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:19:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=4, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:19:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-10 11:19:14 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.854023, avg_loss=0.714002, seen=11, correct=7, accuracy=0.636364
2025-09-10 11:19:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:19:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:19:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:19:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 11:19:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:19:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:19:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:19:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:19:17 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.402802, avg_loss=0.635070, seen=40, correct=27, accuracy=0.675000
2025-09-10 11:19:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:19:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:19:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:19:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 11:19:18 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-10 11:19:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=54, total=214)
2025-09-10 11:19:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:19:19 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-10 11:19:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:19:19 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=27, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-10 11:19:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:19:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:19:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:19:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:19:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:19:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:19:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:19:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:19:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:19:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:19:28 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-10 11:19:30 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-10 11:19:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-10 11:19:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:19:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:19:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-10 11:19:30 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=9.112813, avg_loss=0.828438, seen=11, correct=4, accuracy=0.363636
2025-09-10 11:19:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:19:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:19:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:19:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:19:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:19:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:19:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:19:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:19:35 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.694490, avg_loss=0.692362, seen=40, correct=22, accuracy=0.550000
2025-09-10 11:19:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:19:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:19:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:19:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:19:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:19:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:19:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:19:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:19:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:19:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:19:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:19:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:19:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:19:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:19:42 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-10 11:19:43 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-10 11:19:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-10 11:19:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:19:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:19:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-10 11:19:44 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=10.423768, avg_loss=0.947615, seen=11, correct=3, accuracy=0.272727
2025-09-10 11:19:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:19:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:19:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:19:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:19:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:19:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:19:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:19:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:19:47 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.741070, avg_loss=0.743527, seen=40, correct=22, accuracy=0.550000
2025-09-10 11:19:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:19:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:19:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:19:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:19:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:19:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:19:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:19:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:19:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:19:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:19:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:19:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:19:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:19:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:19:57 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-10 11:19:58 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-10 11:19:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-10 11:19:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:19:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:19:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-10 11:19:59 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=10.769564, avg_loss=0.979051, seen=11, correct=3, accuracy=0.272727
2025-09-10 11:19:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:19:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:20:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:20:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:20:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:20:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:20:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:20:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:20:03 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.210091, avg_loss=0.755252, seen=40, correct=20, accuracy=0.500000
2025-09-10 11:20:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:20:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:20:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:20:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:20:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=31 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:20:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=32 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:20:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=33 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:20:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=34 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:20:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=35 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:20:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=36 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:20:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=37 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:20:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=38 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:20:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=39 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:20:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=40 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:20:14 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-10 11:20:14 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-10 11:20:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-10 11:20:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:20:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:20:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-10 11:20:15 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=10.617748, avg_loss=0.965250, seen=11, correct=4, accuracy=0.363636
2025-09-10 11:20:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:20:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:20:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:20:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:20:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:20:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:20:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:20:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:20:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.601185, avg_loss=0.740030, seen=40, correct=22, accuracy=0.550000
2025-09-10 11:20:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:20:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:20:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:20:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:20:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=41 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:20:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=42 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:20:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=43 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:20:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=44 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:20:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=45 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:20:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=46 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:20:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=47 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:20:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=48 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:20:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=49 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:20:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=50 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:20:26 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-10 11:20:28 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-10 11:20:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-10 11:20:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:20:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:20:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-10 11:20:28 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=10.413733, avg_loss=0.946703, seen=11, correct=4, accuracy=0.363636
2025-09-10 11:20:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:20:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:20:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:20:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:20:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:20:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:20:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:20:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:20:33 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.300282, avg_loss=0.732507, seen=40, correct=22, accuracy=0.550000
2025-09-10 11:20:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:20:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:20:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:20:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:20:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=51 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:20:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=52 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:20:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=53 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:20:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=54 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:20:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=55 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:20:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=56 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:20:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=57 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:20:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=58 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:20:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=59 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:20:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=60 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:20:41 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-10 11:20:41 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-10 11:20:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-10 11:20:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:20:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:20:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-10 11:20:42 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=10.796784, avg_loss=0.981526, seen=11, correct=4, accuracy=0.363636
2025-09-10 11:20:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:20:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:20:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:20:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:20:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:20:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:20:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:20:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:20:47 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.426867, avg_loss=0.735672, seen=40, correct=21, accuracy=0.525000
2025-09-10 11:20:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:20:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:20:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:20:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:20:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=61 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:20:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=62 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:20:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=63 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:20:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=64 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:20:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=65 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:20:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=66 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:20:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=67 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:20:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=68 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:20:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=69 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:20:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=70 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:20:55 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-10 11:20:58 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-10 11:20:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-10 11:20:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:20:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:20:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-10 11:20:58 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=10.973354, avg_loss=0.997578, seen=11, correct=4, accuracy=0.363636
2025-09-10 11:20:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:20:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:21:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:21:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:21:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:21:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:21:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:21:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:21:01 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.339790, avg_loss=0.733495, seen=40, correct=24, accuracy=0.600000
2025-09-10 11:21:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:21:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:21:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:21:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:21:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=71 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:21:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=72 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:21:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=73 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:21:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=74 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:21:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=75 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:21:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=76 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:21:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=77 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:21:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=78 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:21:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=79 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:21:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=80 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:21:11 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-10 11:21:11 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-10 11:21:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-10 11:21:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:21:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:21:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-10 11:21:11 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=11.092632, avg_loss=1.008421, seen=11, correct=5, accuracy=0.454545
2025-09-10 11:21:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:21:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:21:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:21:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:21:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:21:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:21:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:21:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:21:16 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.707539, avg_loss=0.742688, seen=40, correct=24, accuracy=0.600000
2025-09-10 11:21:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:21:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:21:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:21:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:21:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=81 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:21:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=82 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:21:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=83 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:21:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=84 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:21:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=85 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:21:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=86 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:21:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=87 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:21:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=88 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:21:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=89 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:21:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=90 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:21:26 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-10 11:21:27 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-10 11:21:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-10 11:21:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:21:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:21:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-10 11:21:27 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=11.444098, avg_loss=1.040373, seen=11, correct=5, accuracy=0.454545
2025-09-10 11:21:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:21:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:21:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:21:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:21:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:21:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:21:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:21:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:21:31 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.265186, avg_loss=0.756630, seen=40, correct=25, accuracy=0.625000
2025-09-10 11:21:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:21:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:21:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:21:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:21:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=91 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:21:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=92 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:21:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=93 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:21:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=94 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:21:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=95 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:21:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=96 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:21:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=97 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:21:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=98 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:21:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=99 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:21:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=100 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:21:39 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-10 11:21:40 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-10 11:21:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-10 11:21:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:21:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:21:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-10 11:21:40 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=11.663412, avg_loss=1.060310, seen=11, correct=5, accuracy=0.454545
2025-09-10 11:21:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:21:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:21:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:21:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:21:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:21:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:21:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:21:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:21:43 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.192173, avg_loss=0.754804, seen=40, correct=25, accuracy=0.625000
2025-09-10 11:21:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:21:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:21:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:21:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:21:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-10 11:21:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-10 11:21:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:21:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:21:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:21:46 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #2', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-10 11:21:46 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #2', 'Round': 0, 'Results_raw': {}}
2025-09-10 11:21:46 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 11:21:46 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-10 11:21:47 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-10 11:21:47 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-10 11:21:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-09-10 11:21:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:21:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=36, num_train_batch_last_epoch=28, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:21:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-10 11:21:49 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=72, loss_sum=47.917168, avg_loss=0.665516, seen=72, correct=43, accuracy=0.597222
2025-09-10 11:21:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:21:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:21:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:21:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 11:21:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:21:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:21:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:21:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:21:53 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.426258, avg_loss=0.610656, seen=40, correct=24, accuracy=0.600000
2025-09-10 11:21:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:21:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:21:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:21:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 11:21:54 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-10 11:21:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=343, total=1372)
2025-09-10 11:21:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:21:55 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-10 11:21:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:21:55 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=172, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-10 11:21:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:21:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:21:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:21:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:21:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:22:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:22:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:22:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:22:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:22:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:22:03 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-10 11:22:03 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-10 11:22:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-09-10 11:22:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:22:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=36, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:22:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-10 11:22:04 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=72, loss_sum=47.290653, avg_loss=0.656815, seen=72, correct=44, accuracy=0.611111
2025-09-10 11:22:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:22:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:22:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:22:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:22:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:22:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:22:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:22:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:22:08 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.449924, avg_loss=0.661248, seen=40, correct=23, accuracy=0.575000
2025-09-10 11:22:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:22:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:22:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:22:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:22:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:22:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:22:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:22:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:22:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:22:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:22:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:22:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:22:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:22:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:22:16 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-10 11:22:16 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-10 11:22:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-09-10 11:22:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:22:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=36, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:22:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-10 11:22:17 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=72, loss_sum=48.539997, avg_loss=0.674167, seen=72, correct=41, accuracy=0.569444
2025-09-10 11:22:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:22:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:22:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:22:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:22:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:22:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:22:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:22:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:22:22 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.388885, avg_loss=0.684722, seen=40, correct=20, accuracy=0.500000
2025-09-10 11:22:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:22:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:22:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:22:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:22:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:22:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:22:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:22:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:22:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:22:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:22:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:22:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:22:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:22:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:22:32 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-10 11:22:32 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-10 11:22:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-09-10 11:22:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:22:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=36, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:22:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-10 11:22:33 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=72, loss_sum=49.178238, avg_loss=0.683031, seen=72, correct=40, accuracy=0.555556
2025-09-10 11:22:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:22:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:22:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:22:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:22:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:22:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:22:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:22:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:22:37 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.260103, avg_loss=0.656503, seen=40, correct=24, accuracy=0.600000
2025-09-10 11:22:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:22:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:22:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:22:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:22:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=31 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:22:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=32 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:22:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=33 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:22:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=34 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:22:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=35 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:22:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=36 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:22:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=37 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:22:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=38 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:22:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=39 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:22:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=40 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:22:46 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-10 11:22:46 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-10 11:22:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-09-10 11:22:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:22:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=36, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:22:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-10 11:22:48 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=72, loss_sum=47.667904, avg_loss=0.662054, seen=72, correct=42, accuracy=0.583333
2025-09-10 11:22:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:22:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:22:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:22:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:22:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:22:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:22:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:22:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:22:53 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.394966, avg_loss=0.634874, seen=40, correct=27, accuracy=0.675000
2025-09-10 11:22:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:22:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:22:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:22:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:22:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=41 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:22:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=42 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:22:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=43 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:22:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=44 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:23:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=45 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:23:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=46 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:23:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=47 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:23:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=48 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:23:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=49 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:23:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=50 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:23:03 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-10 11:23:04 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-10 11:23:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-09-10 11:23:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:23:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=36, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:23:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-10 11:23:05 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=72, loss_sum=47.919441, avg_loss=0.665548, seen=72, correct=43, accuracy=0.597222
2025-09-10 11:23:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:23:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:23:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:23:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:23:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:23:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:23:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:23:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:23:09 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.862244, avg_loss=0.621556, seen=40, correct=29, accuracy=0.725000
2025-09-10 11:23:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:23:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:23:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:23:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:23:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=51 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:23:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=52 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:23:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=53 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:23:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=54 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:23:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=55 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:23:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=56 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:23:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=57 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:23:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=58 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:23:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=59 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:23:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=60 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:23:17 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-10 11:23:18 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-10 11:23:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-09-10 11:23:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:23:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=36, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:23:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-10 11:23:19 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=72, loss_sum=46.690353, avg_loss=0.648477, seen=72, correct=41, accuracy=0.569444
2025-09-10 11:23:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:23:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:23:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:23:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:23:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:23:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:23:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:23:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:23:24 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.881559, avg_loss=0.622039, seen=40, correct=29, accuracy=0.725000
2025-09-10 11:23:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:23:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:23:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:23:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:23:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=61 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:23:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=62 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:23:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=63 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:23:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=64 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:23:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=65 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:23:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=66 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:23:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=67 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:23:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=68 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:23:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=69 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:23:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=70 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:23:34 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-10 11:23:34 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-10 11:23:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-09-10 11:23:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:23:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=36, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:23:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-10 11:23:35 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=72, loss_sum=46.514751, avg_loss=0.646038, seen=72, correct=44, accuracy=0.611111
2025-09-10 11:23:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:23:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:23:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:23:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:23:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:23:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:23:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:23:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:23:41 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.618105, avg_loss=0.615453, seen=40, correct=30, accuracy=0.750000
2025-09-10 11:23:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:23:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:23:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:23:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:23:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=71 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:23:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=72 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:23:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=73 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:23:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=74 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:23:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=75 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:23:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=76 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:23:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=77 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:23:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=78 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:23:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=79 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:23:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=80 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:23:49 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-10 11:23:49 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-10 11:23:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-09-10 11:23:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:23:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=36, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:23:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-10 11:23:51 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=72, loss_sum=47.051407, avg_loss=0.653492, seen=72, correct=43, accuracy=0.597222
2025-09-10 11:23:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:23:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:23:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:23:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:23:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:23:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:23:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:23:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:23:55 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.842894, avg_loss=0.621072, seen=40, correct=27, accuracy=0.675000
2025-09-10 11:23:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:23:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:23:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:23:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:23:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=81 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:23:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=82 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:23:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=83 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:24:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=84 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:24:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=85 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:24:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=86 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:24:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=87 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:24:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=88 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:24:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=89 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:24:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=90 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:24:04 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-10 11:24:04 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-10 11:24:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-09-10 11:24:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:24:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=36, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:24:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-10 11:24:06 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=72, loss_sum=47.483299, avg_loss=0.659490, seen=72, correct=42, accuracy=0.583333
2025-09-10 11:24:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:24:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:24:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:24:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:24:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:24:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:24:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:24:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:24:11 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.182856, avg_loss=0.629571, seen=40, correct=27, accuracy=0.675000
2025-09-10 11:24:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:24:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:24:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:24:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:24:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=91 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:24:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=92 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:24:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=93 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:24:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=94 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:24:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=95 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:24:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=96 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:24:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=97 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:24:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=98 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:24:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=99 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:24:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=100 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:24:21 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-10 11:24:21 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-10 11:24:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-09-10 11:24:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:24:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=36, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:24:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-10 11:24:22 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=72, loss_sum=47.200371, avg_loss=0.655561, seen=72, correct=41, accuracy=0.569444
2025-09-10 11:24:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:24:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:24:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:24:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:24:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:24:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:24:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:24:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:24:27 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.356754, avg_loss=0.658919, seen=40, correct=23, accuracy=0.575000
2025-09-10 11:24:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:24:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:24:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:24:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:24:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-10 11:24:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-10 11:24:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:24:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:24:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2242MB allocated=2204MB
2025-09-10 11:24:29 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #13', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-10 11:24:29 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #13', 'Round': 0, 'Results_raw': {}}
2025-09-10 11:24:29 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 11:24:29 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-10 11:24:29 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-10 11:24:30 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-10 11:24:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-09-10 11:24:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:24:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=60, num_train_batch_last_epoch=40, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:24:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-09-10 11:24:32 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=119, loss_sum=83.242493, avg_loss=0.699517, seen=119, correct=69, accuracy=0.579832
2025-09-10 11:24:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:24:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:24:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:24:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 11:24:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:24:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:24:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:24:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:24:34 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.807516, avg_loss=0.695188, seen=40, correct=24, accuracy=0.600000
2025-09-10 11:24:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:24:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:24:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:24:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 11:24:35 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-10 11:24:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=569, total=2275)
2025-09-10 11:24:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:24:36 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-10 11:24:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:24:36 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=285, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-10 11:24:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:24:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:24:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:24:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:24:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:24:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:24:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:24:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:24:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:24:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:24:44 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-10 11:24:46 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-10 11:24:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-09-10 11:24:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:24:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=60, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:24:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-09-10 11:24:48 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=119, loss_sum=84.013367, avg_loss=0.705995, seen=119, correct=70, accuracy=0.588235
2025-09-10 11:24:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:24:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:24:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:24:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:24:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:24:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:24:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:24:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:24:52 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.583794, avg_loss=0.714595, seen=40, correct=19, accuracy=0.475000
2025-09-10 11:24:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:24:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:24:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:24:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:24:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:24:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:24:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:24:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:24:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:24:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:24:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:24:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:24:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:25:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:25:00 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-10 11:25:02 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-10 11:25:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-09-10 11:25:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:25:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=60, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:25:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-09-10 11:25:04 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=119, loss_sum=81.892944, avg_loss=0.688176, seen=119, correct=66, accuracy=0.554622
2025-09-10 11:25:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:25:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:25:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:25:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:25:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:25:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:25:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:25:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:25:07 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.038391, avg_loss=0.675960, seen=40, correct=24, accuracy=0.600000
2025-09-10 11:25:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:25:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:25:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:25:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:25:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:25:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:25:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:25:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:25:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:25:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:25:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:25:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:25:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:25:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:25:17 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-10 11:25:17 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-10 11:25:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-09-10 11:25:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:25:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=60, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:25:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-09-10 11:25:19 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=119, loss_sum=81.776978, avg_loss=0.687201, seen=119, correct=67, accuracy=0.563025
2025-09-10 11:25:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:25:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:25:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:25:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:25:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:25:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:25:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:25:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:25:22 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.455563, avg_loss=0.661389, seen=40, correct=21, accuracy=0.525000
2025-09-10 11:25:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:25:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:25:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:25:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:25:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=31 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:25:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=32 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:25:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=33 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:25:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=34 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:25:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=35 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:25:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=36 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:25:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=37 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:25:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=38 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:25:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=39 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:25:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=40 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:25:31 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-10 11:25:31 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-10 11:25:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-09-10 11:25:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:25:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=60, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:25:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-09-10 11:25:33 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=119, loss_sum=82.730263, avg_loss=0.695212, seen=119, correct=68, accuracy=0.571429
2025-09-10 11:25:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:25:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:25:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:25:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:25:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:25:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:25:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:25:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:25:36 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.992611, avg_loss=0.674815, seen=40, correct=22, accuracy=0.550000
2025-09-10 11:25:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:25:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:25:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:25:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:25:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=41 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:25:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=42 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:25:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=43 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:25:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=44 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:25:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=45 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:25:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=46 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:25:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=47 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:25:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=48 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:25:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=49 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:25:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=50 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:25:44 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-10 11:25:45 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-10 11:25:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-09-10 11:25:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:25:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=60, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:25:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-09-10 11:25:47 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=119, loss_sum=83.018616, avg_loss=0.697635, seen=119, correct=64, accuracy=0.537815
2025-09-10 11:25:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:25:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:25:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:25:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:25:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:25:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:25:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:25:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:25:52 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.107718, avg_loss=0.677693, seen=40, correct=22, accuracy=0.550000
2025-09-10 11:25:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:25:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:25:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:25:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:25:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=51 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:25:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=52 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:25:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=53 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:25:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=54 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:25:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=55 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:25:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=56 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:25:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=57 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:25:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=58 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:26:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=59 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:26:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=60 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:26:00 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-10 11:26:00 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-10 11:26:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-09-10 11:26:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:26:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=60, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:26:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-09-10 11:26:02 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=119, loss_sum=83.463135, avg_loss=0.701371, seen=119, correct=62, accuracy=0.521008
2025-09-10 11:26:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:26:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:26:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:26:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:26:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:26:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:26:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:26:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:26:06 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.923033, avg_loss=0.673076, seen=40, correct=23, accuracy=0.575000
2025-09-10 11:26:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:26:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:26:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:26:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:26:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=61 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:26:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=62 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:26:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=63 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:26:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=64 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:26:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=65 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:26:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=66 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:26:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=67 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:26:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=68 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:26:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=69 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:26:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=70 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:26:15 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-10 11:26:15 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-10 11:26:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-09-10 11:26:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:26:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=60, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:26:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-09-10 11:26:17 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=119, loss_sum=83.674141, avg_loss=0.703144, seen=119, correct=60, accuracy=0.504202
2025-09-10 11:26:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:26:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:26:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:26:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:26:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:26:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:26:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:26:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:26:22 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.551624, avg_loss=0.663791, seen=40, correct=22, accuracy=0.550000
2025-09-10 11:26:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:26:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:26:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:26:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:26:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=71 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:26:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=72 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:26:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=73 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:26:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=74 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:26:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=75 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:26:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=76 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:26:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=77 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:26:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=78 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:26:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=79 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:26:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=80 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:26:31 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-10 11:26:31 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-10 11:26:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-09-10 11:26:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:26:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=60, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:26:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-09-10 11:26:33 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=119, loss_sum=82.967606, avg_loss=0.697207, seen=119, correct=65, accuracy=0.546218
2025-09-10 11:26:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:26:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:26:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:26:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:26:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:26:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:26:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:26:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:26:38 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.997913, avg_loss=0.674948, seen=40, correct=24, accuracy=0.600000
2025-09-10 11:26:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:26:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:26:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:26:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:26:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=81 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:26:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=82 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:26:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=83 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:26:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=84 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:26:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=85 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:26:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=86 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:26:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=87 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:26:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=88 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:26:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=89 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:26:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=90 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:26:50 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-10 11:26:50 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-10 11:26:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-09-10 11:26:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:26:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=60, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:26:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-09-10 11:26:52 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=119, loss_sum=82.969559, avg_loss=0.697223, seen=119, correct=66, accuracy=0.554622
2025-09-10 11:26:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:26:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:26:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:26:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:26:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:26:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:26:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:26:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:26:57 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.943275, avg_loss=0.698582, seen=40, correct=24, accuracy=0.600000
2025-09-10 11:26:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:26:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:26:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:26:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:27:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=91 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:27:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=92 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:27:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=93 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:27:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=94 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:27:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=95 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:27:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=96 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:27:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=97 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:27:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=98 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:27:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=99 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:27:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=100 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:27:08 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-10 11:27:08 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-10 11:27:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-09-10 11:27:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:27:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=60, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:27:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-09-10 11:27:10 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=119, loss_sum=82.369972, avg_loss=0.692185, seen=119, correct=71, accuracy=0.596639
2025-09-10 11:27:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:27:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:27:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:27:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:27:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:27:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:27:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:27:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:27:14 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.698729, avg_loss=0.692468, seen=40, correct=25, accuracy=0.625000
2025-09-10 11:27:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:27:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:27:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:27:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:27:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-10 11:27:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-10 11:27:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:27:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:27:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:27:16 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #41', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-10 11:27:16 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #41', 'Round': 0, 'Results_raw': {}}
2025-09-10 11:27:16 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 11:27:16 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 2 for training...
2025-09-10 11:27:17 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-10 11:27:17 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-10 11:27:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 11:27:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:27:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:27:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 11:27:22 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=142.585159, avg_loss=0.712926, seen=200, correct=107, accuracy=0.535000
2025-09-10 11:27:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:27:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:27:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:27:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 11:27:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:27:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:27:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:27:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:27:25 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=32.601845, avg_loss=0.815046, seen=40, correct=21, accuracy=0.525000
2025-09-10 11:27:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:27:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:27:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:27:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 11:27:26 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-10 11:27:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1162, total=4647)
2025-09-10 11:27:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:27:26 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-10 11:27:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:27:26 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=581, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-10 11:27:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:27:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:27:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:27:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:27:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:27:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:27:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:27:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:27:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:27:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:27:34 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-10 11:27:35 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-10 11:27:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 11:27:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:27:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:27:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 11:27:38 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=141.674347, avg_loss=0.708372, seen=200, correct=102, accuracy=0.510000
2025-09-10 11:27:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:27:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:27:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:27:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:27:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:27:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:27:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:27:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:27:41 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.569885, avg_loss=0.764247, seen=40, correct=22, accuracy=0.550000
2025-09-10 11:27:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:27:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:27:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:27:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:27:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:27:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:27:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:27:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:27:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:27:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:27:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:27:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:27:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:27:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:27:51 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-10 11:27:52 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-10 11:27:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 11:27:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:27:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:27:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 11:27:55 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=143.427933, avg_loss=0.717140, seen=200, correct=101, accuracy=0.505000
2025-09-10 11:27:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:27:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:27:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:27:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:27:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:27:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:27:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:28:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:28:01 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.137182, avg_loss=0.703430, seen=40, correct=22, accuracy=0.550000
2025-09-10 11:28:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:28:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:28:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:28:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:28:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:28:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:28:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:28:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:28:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:28:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:28:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:28:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:28:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:28:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:28:10 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-10 11:28:10 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-10 11:28:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 11:28:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:28:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:28:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 11:28:13 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=143.897827, avg_loss=0.719489, seen=200, correct=102, accuracy=0.510000
2025-09-10 11:28:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:28:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:28:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:28:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:28:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:28:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:28:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:28:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:28:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.033516, avg_loss=0.700838, seen=40, correct=22, accuracy=0.550000
2025-09-10 11:28:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:28:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:28:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:28:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:28:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=31 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:28:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=32 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:28:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=33 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:28:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=34 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:28:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=35 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:28:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=36 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:28:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=37 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:28:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=38 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:28:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=39 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:28:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=40 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:28:27 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-10 11:28:28 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-10 11:28:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 11:28:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:28:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:28:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 11:28:31 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=143.722061, avg_loss=0.718610, seen=200, correct=107, accuracy=0.535000
2025-09-10 11:28:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:28:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:28:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:28:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:28:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:28:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:28:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:28:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:28:37 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.360680, avg_loss=0.709017, seen=40, correct=21, accuracy=0.525000
2025-09-10 11:28:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:28:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:28:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:28:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:28:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=41 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:28:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=42 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:28:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=43 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:28:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=44 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:28:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=45 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:28:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=46 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:28:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=47 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:28:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=48 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:28:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=49 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:28:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=50 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:28:44 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-10 11:28:45 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-10 11:28:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 11:28:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:28:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:28:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 11:28:48 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=144.959015, avg_loss=0.724795, seen=200, correct=100, accuracy=0.500000
2025-09-10 11:28:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:28:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:28:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:28:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:28:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:28:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:28:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:28:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:28:53 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.666031, avg_loss=0.716651, seen=40, correct=20, accuracy=0.500000
2025-09-10 11:28:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:28:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:28:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:28:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:28:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=51 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:28:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=52 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:28:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=53 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:28:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=54 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:28:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=55 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:28:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=56 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:29:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=57 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:29:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=58 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:29:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=59 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:29:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=60 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:29:02 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-10 11:29:02 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-10 11:29:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 11:29:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:29:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:29:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 11:29:05 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=145.569138, avg_loss=0.727846, seen=200, correct=103, accuracy=0.515000
2025-09-10 11:29:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:29:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:29:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:29:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:29:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:29:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:29:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:29:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:29:10 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.362825, avg_loss=0.709071, seen=40, correct=19, accuracy=0.475000
2025-09-10 11:29:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:29:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:29:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:29:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:29:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=61 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:29:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=62 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:29:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=63 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:29:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=64 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:29:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=65 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:29:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=66 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:29:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=67 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:29:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=68 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:29:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=69 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:29:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=70 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:29:19 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-10 11:29:19 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-10 11:29:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 11:29:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:29:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:29:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 11:29:22 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=142.880920, avg_loss=0.714405, seen=200, correct=104, accuracy=0.520000
2025-09-10 11:29:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:29:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:29:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:29:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:29:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:29:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:29:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:29:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:29:25 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.144863, avg_loss=0.703622, seen=40, correct=19, accuracy=0.475000
2025-09-10 11:29:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:29:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:29:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:29:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:29:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=71 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:29:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=72 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:29:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=73 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:29:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=74 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:29:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=75 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:29:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=76 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:29:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=77 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:29:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=78 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:29:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=79 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:29:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=80 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:29:37 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-10 11:29:37 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-10 11:29:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 11:29:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:29:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:29:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 11:29:40 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=141.745102, avg_loss=0.708726, seen=200, correct=107, accuracy=0.535000
2025-09-10 11:29:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:29:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:29:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:29:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:29:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:29:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:29:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:29:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:29:44 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.584044, avg_loss=0.714601, seen=40, correct=20, accuracy=0.500000
2025-09-10 11:29:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:29:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:29:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:29:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:29:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=81 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:29:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=82 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:29:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=83 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:29:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=84 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:29:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=85 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:29:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=86 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:29:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=87 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:29:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=88 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:29:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=89 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:29:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=90 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:29:52 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-10 11:29:53 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-10 11:29:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 11:29:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:29:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:29:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 11:29:56 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=143.148117, avg_loss=0.715741, seen=200, correct=99, accuracy=0.495000
2025-09-10 11:29:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:29:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:29:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:30:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:30:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:30:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:30:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:30:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:30:02 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.416000, avg_loss=0.735400, seen=40, correct=21, accuracy=0.525000
2025-09-10 11:30:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:30:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:30:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:30:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:30:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=91 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:30:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=92 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:30:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=93 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:30:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=94 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:30:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=95 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:30:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=96 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:30:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=97 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:30:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=98 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:30:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=99 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:30:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=100 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:30:12 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-10 11:30:13 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-10 11:30:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 11:30:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:30:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:30:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 11:30:16 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=142.214752, avg_loss=0.711074, seen=200, correct=103, accuracy=0.515000
2025-09-10 11:30:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:30:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:30:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:30:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:30:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:30:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:30:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:30:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:30:23 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.087439, avg_loss=0.727186, seen=40, correct=20, accuracy=0.500000
2025-09-10 11:30:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:30:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:30:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:30:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:30:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-10 11:30:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-10 11:30:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:30:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:30:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:30:27 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #25', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-10 11:30:27 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #25', 'Round': 0, 'Results_raw': {}}
2025-09-10 11:30:27 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 11:30:27 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-10 11:30:27 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-10 11:30:27 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-10 11:30:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-09-10 11:30:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:30:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=29, num_train_batch_last_epoch=13, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:30:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-09-10 11:30:28 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=57, loss_sum=37.267483, avg_loss=0.653815, seen=57, correct=34, accuracy=0.596491
2025-09-10 11:30:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:30:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:30:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:30:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 11:30:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:30:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:30:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:30:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:30:31 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.277843, avg_loss=0.606946, seen=40, correct=27, accuracy=0.675000
2025-09-10 11:30:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:30:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:30:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:30:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 11:30:33 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-10 11:30:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=272, total=1088)
2025-09-10 11:30:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:30:34 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-10 11:30:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:30:34 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=136, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-10 11:30:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:30:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:30:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:30:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:30:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:30:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:30:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:30:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:30:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:30:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:30:40 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-10 11:30:41 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-10 11:30:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-09-10 11:30:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:30:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=29, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:30:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-09-10 11:30:42 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=57, loss_sum=39.855381, avg_loss=0.699217, seen=57, correct=32, accuracy=0.561404
2025-09-10 11:30:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:30:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:30:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:30:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:30:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:30:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:30:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:30:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:30:49 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.570669, avg_loss=0.614267, seen=40, correct=27, accuracy=0.675000
2025-09-10 11:30:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:30:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:30:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:30:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:30:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:30:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:30:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:30:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:30:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:30:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:30:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:30:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:30:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:30:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:30:57 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-10 11:30:58 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-10 11:30:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-09-10 11:30:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:30:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=29, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:30:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-09-10 11:30:59 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=57, loss_sum=37.252811, avg_loss=0.653558, seen=57, correct=32, accuracy=0.561404
2025-09-10 11:30:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:30:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:31:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:31:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:31:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:31:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:31:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:31:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:31:03 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.961329, avg_loss=0.599033, seen=40, correct=29, accuracy=0.725000
2025-09-10 11:31:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:31:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:31:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:31:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:31:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:31:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:31:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:31:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:31:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:31:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:31:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:31:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:31:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:31:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:31:14 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-10 11:31:14 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-10 11:31:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-09-10 11:31:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:31:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=29, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:31:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-09-10 11:31:15 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=57, loss_sum=36.709236, avg_loss=0.644022, seen=57, correct=32, accuracy=0.561404
2025-09-10 11:31:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:31:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:31:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:31:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:31:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:31:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:31:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:31:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:31:20 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.217625, avg_loss=0.605441, seen=40, correct=29, accuracy=0.725000
2025-09-10 11:31:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:31:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:31:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:31:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:31:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=31 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:31:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=32 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:31:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=33 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:31:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=34 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:31:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=35 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:31:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=36 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:31:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=37 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:31:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=38 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:31:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=39 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:31:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=40 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:31:33 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-10 11:31:33 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-10 11:31:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-09-10 11:31:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:31:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=29, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:31:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-09-10 11:31:34 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=57, loss_sum=35.798443, avg_loss=0.628043, seen=57, correct=36, accuracy=0.631579
2025-09-10 11:31:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:31:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:31:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:31:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:31:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:31:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:31:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:31:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:31:37 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.308956, avg_loss=0.607724, seen=40, correct=24, accuracy=0.600000
2025-09-10 11:31:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:31:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:31:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:31:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:31:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=41 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:31:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=42 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:31:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=43 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:31:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=44 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:31:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=45 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:31:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=46 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:31:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=47 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:31:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=48 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:31:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=49 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:31:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=50 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:31:48 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-10 11:31:49 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-10 11:31:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-09-10 11:31:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:31:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=29, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:31:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-09-10 11:31:50 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=57, loss_sum=36.206451, avg_loss=0.635201, seen=57, correct=35, accuracy=0.614035
2025-09-10 11:31:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:31:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:31:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:31:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:31:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:31:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:31:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:31:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:31:54 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.768093, avg_loss=0.594202, seen=40, correct=26, accuracy=0.650000
2025-09-10 11:31:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:31:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:31:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:31:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:31:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=51 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:31:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=52 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:31:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=53 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:32:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=54 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:32:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=55 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:32:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=56 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:32:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=57 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:32:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=58 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:32:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=59 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:32:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=60 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:32:03 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-10 11:32:05 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-10 11:32:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-09-10 11:32:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:32:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=29, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:32:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-09-10 11:32:06 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=57, loss_sum=38.528393, avg_loss=0.675937, seen=57, correct=32, accuracy=0.561404
2025-09-10 11:32:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:32:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:32:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:32:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:32:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:32:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:32:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:32:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:32:09 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.420511, avg_loss=0.610513, seen=40, correct=28, accuracy=0.700000
2025-09-10 11:32:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:32:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:32:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:32:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:32:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=61 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:32:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=62 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:32:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=63 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:32:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=64 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:32:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=65 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:32:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=66 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:32:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=67 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:32:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=68 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:32:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=69 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:32:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=70 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:32:18 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-10 11:32:18 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-10 11:32:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-09-10 11:32:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:32:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=29, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:32:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-09-10 11:32:19 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=57, loss_sum=40.009029, avg_loss=0.701913, seen=57, correct=36, accuracy=0.631579
2025-09-10 11:32:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:32:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:32:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:32:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:32:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:32:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:32:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:32:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:32:22 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.847237, avg_loss=0.621181, seen=40, correct=28, accuracy=0.700000
2025-09-10 11:32:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:32:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:32:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:32:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:32:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=71 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:32:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=72 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:32:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=73 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:32:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=74 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:32:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=75 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:32:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=76 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:32:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=77 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:32:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=78 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:32:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=79 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:32:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=80 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:32:33 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-10 11:32:33 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-10 11:32:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-09-10 11:32:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:32:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=29, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:32:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-09-10 11:32:35 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=57, loss_sum=39.212406, avg_loss=0.687937, seen=57, correct=35, accuracy=0.614035
2025-09-10 11:32:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:32:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:32:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:32:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:32:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:32:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:32:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:32:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:32:39 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.643147, avg_loss=0.616079, seen=40, correct=28, accuracy=0.700000
2025-09-10 11:32:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:32:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:32:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:32:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:32:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=81 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:32:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=82 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:32:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=83 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:32:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=84 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:32:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=85 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:32:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=86 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:32:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=87 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:32:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=88 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:32:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=89 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:32:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=90 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:32:50 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-10 11:32:50 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-10 11:32:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-09-10 11:32:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:32:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=29, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:32:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-09-10 11:32:52 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=57, loss_sum=37.302551, avg_loss=0.654431, seen=57, correct=36, accuracy=0.631579
2025-09-10 11:32:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:32:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:32:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:32:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:32:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:32:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:32:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:32:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:32:55 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.840443, avg_loss=0.596011, seen=40, correct=29, accuracy=0.725000
2025-09-10 11:32:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:32:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:32:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:32:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:32:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=91 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:32:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=92 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:32:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=93 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:32:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=94 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:33:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=95 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:33:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=96 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:33:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=97 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:33:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=98 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:33:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=99 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:33:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=100 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:33:03 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-10 11:33:04 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-10 11:33:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-09-10 11:33:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:33:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=29, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:33:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-09-10 11:33:06 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=57, loss_sum=36.049789, avg_loss=0.632452, seen=57, correct=35, accuracy=0.614035
2025-09-10 11:33:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:33:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:33:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:33:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:33:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:33:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:33:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:33:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:33:08 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.912464, avg_loss=0.597812, seen=40, correct=31, accuracy=0.775000
2025-09-10 11:33:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:33:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:33:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:33:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:33:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-10 11:33:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-10 11:33:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:33:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:33:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:33:10 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #7', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-10 11:33:10 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #7', 'Round': 0, 'Results_raw': {}}
2025-09-10 11:33:10 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 11:33:10 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-10 11:33:11 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-10 11:33:11 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-10 11:33:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 11:33:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:33:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:33:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 11:33:15 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=134.620392, avg_loss=0.673102, seen=200, correct=120, accuracy=0.600000
2025-09-10 11:33:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:33:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:33:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:33:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 11:33:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:33:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:33:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:33:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:33:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.749962, avg_loss=0.768749, seen=40, correct=21, accuracy=0.525000
2025-09-10 11:33:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:33:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:33:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:33:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 11:33:19 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-10 11:33:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1236, total=4944)
2025-09-10 11:33:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:33:20 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-10 11:33:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:33:20 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=618, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-10 11:33:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:33:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:33:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:33:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:33:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:33:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:33:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:33:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:33:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:33:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:33:27 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-10 11:33:28 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-10 11:33:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 11:33:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:33:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:33:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 11:33:31 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=132.388718, avg_loss=0.661944, seen=200, correct=118, accuracy=0.590000
2025-09-10 11:33:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:33:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:33:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:33:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:33:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:33:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:33:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:33:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:33:34 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.472595, avg_loss=0.761815, seen=40, correct=19, accuracy=0.475000
2025-09-10 11:33:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:33:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:33:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:33:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:33:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:33:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:33:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:33:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:33:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:33:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:33:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:33:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:33:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:33:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:33:42 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-10 11:33:43 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-10 11:33:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 11:33:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:33:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:33:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 11:33:46 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=132.158325, avg_loss=0.660792, seen=200, correct=116, accuracy=0.580000
2025-09-10 11:33:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:33:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:33:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:33:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:33:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:33:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:33:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:33:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:33:54 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.228098, avg_loss=0.755702, seen=40, correct=19, accuracy=0.475000
2025-09-10 11:33:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:33:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:33:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:33:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:33:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:33:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:33:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:33:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:34:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:34:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:34:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:34:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:34:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:34:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:34:03 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-10 11:34:05 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-10 11:34:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 11:34:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:34:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:34:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 11:34:08 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=131.835663, avg_loss=0.659178, seen=200, correct=118, accuracy=0.590000
2025-09-10 11:34:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:34:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:34:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:34:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:34:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:34:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:34:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:34:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:34:17 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.229599, avg_loss=0.755740, seen=40, correct=18, accuracy=0.450000
2025-09-10 11:34:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:34:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:34:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:34:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:34:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=31 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:34:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=32 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:34:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=33 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:34:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=34 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:34:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=35 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:34:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=36 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:34:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=37 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:34:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=38 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:34:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=39 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:34:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=40 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:34:27 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-10 11:34:27 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-10 11:34:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 11:34:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:34:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:34:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 11:34:31 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=131.451172, avg_loss=0.657256, seen=200, correct=116, accuracy=0.580000
2025-09-10 11:34:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:34:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:34:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:34:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:34:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:34:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:34:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:34:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:34:36 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.511776, avg_loss=0.762794, seen=40, correct=18, accuracy=0.450000
2025-09-10 11:34:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:34:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:34:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:34:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:34:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=41 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:34:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=42 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:34:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=43 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:34:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=44 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:34:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=45 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:34:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=46 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:34:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=47 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:34:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=48 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:34:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=49 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:34:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=50 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:34:48 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-10 11:34:48 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-10 11:34:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 11:34:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:34:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:34:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 11:34:51 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=131.624207, avg_loss=0.658121, seen=200, correct=113, accuracy=0.565000
2025-09-10 11:34:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:34:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:34:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:34:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:34:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:34:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:34:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:34:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:34:56 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.266230, avg_loss=0.756656, seen=40, correct=18, accuracy=0.450000
2025-09-10 11:34:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:34:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:34:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:34:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:35:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=51 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:35:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=52 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:35:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=53 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:35:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=54 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:35:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=55 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:35:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=56 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:35:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=57 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:35:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=58 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:35:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=59 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:35:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=60 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:35:04 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-10 11:35:07 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-10 11:35:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 11:35:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:35:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:35:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 11:35:10 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=132.371063, avg_loss=0.661855, seen=200, correct=113, accuracy=0.565000
2025-09-10 11:35:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:35:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:35:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:35:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:35:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:35:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:35:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:35:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:35:13 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.134689, avg_loss=0.753367, seen=40, correct=17, accuracy=0.425000
2025-09-10 11:35:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:35:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:35:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:35:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:35:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=61 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:35:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=62 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:35:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=63 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:35:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=64 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:35:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=65 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:35:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=66 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:35:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=67 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:35:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=68 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:35:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=69 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:35:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=70 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:35:20 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-10 11:35:22 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-10 11:35:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 11:35:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:35:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:35:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 11:35:25 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=132.685730, avg_loss=0.663429, seen=200, correct=109, accuracy=0.545000
2025-09-10 11:35:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:35:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:35:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:35:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:35:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:35:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:35:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:35:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:35:33 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.911676, avg_loss=0.747792, seen=40, correct=16, accuracy=0.400000
2025-09-10 11:35:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:35:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:35:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:35:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:35:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=71 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:35:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=72 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:35:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=73 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:35:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=74 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:35:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=75 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:35:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=76 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:35:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=77 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:35:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=78 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:35:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=79 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:35:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=80 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:35:42 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-10 11:35:43 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-10 11:35:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 11:35:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:35:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:35:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 11:35:46 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=132.961548, avg_loss=0.664808, seen=200, correct=109, accuracy=0.545000
2025-09-10 11:35:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:35:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:35:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:35:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:35:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:35:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:35:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:35:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:35:51 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.547554, avg_loss=0.738689, seen=40, correct=19, accuracy=0.475000
2025-09-10 11:35:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:35:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:35:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:35:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:35:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=81 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:35:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=82 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:35:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=83 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:36:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=84 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:36:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=85 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:36:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=86 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:36:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=87 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:36:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=88 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:36:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=89 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:36:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=90 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:36:03 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-10 11:36:04 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-10 11:36:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 11:36:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:36:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:36:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 11:36:07 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=133.195496, avg_loss=0.665977, seen=200, correct=120, accuracy=0.600000
2025-09-10 11:36:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:36:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:36:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:36:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:36:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:36:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:36:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:36:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:36:13 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.685539, avg_loss=0.742138, seen=40, correct=20, accuracy=0.500000
2025-09-10 11:36:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:36:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:36:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:36:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:36:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=91 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:36:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=92 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:36:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=93 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:36:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=94 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:36:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=95 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:36:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=96 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:36:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=97 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:36:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=98 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:36:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=99 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:36:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=100 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:36:25 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-10 11:36:25 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-10 11:36:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 11:36:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:36:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:36:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 11:36:28 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=133.470596, avg_loss=0.667353, seen=200, correct=115, accuracy=0.575000
2025-09-10 11:36:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:36:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:36:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:36:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:36:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:36:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:36:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:36:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:36:32 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.633337, avg_loss=0.740833, seen=40, correct=21, accuracy=0.525000
2025-09-10 11:36:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:36:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:36:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:36:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:36:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-10 11:36:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-10 11:36:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:36:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:36:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:36:36 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #24', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-10 11:36:36 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #24', 'Round': 0, 'Results_raw': {}}
2025-09-10 11:36:36 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 11:36:36 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-10 11:36:37 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-10 11:36:37 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-10 11:36:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 11:36:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:36:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:36:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 11:36:41 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=137.101715, avg_loss=0.685509, seen=200, correct=110, accuracy=0.550000
2025-09-10 11:36:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:36:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:36:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:36:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 11:36:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:36:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:36:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:36:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:36:46 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.103912, avg_loss=0.652598, seen=40, correct=25, accuracy=0.625000
2025-09-10 11:36:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:36:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:36:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:36:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 11:36:48 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-10 11:36:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1069, total=4273)
2025-09-10 11:36:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:36:48 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-10 11:36:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:36:48 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=535, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-10 11:36:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:36:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:36:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:36:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:36:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:36:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:36:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:36:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:36:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:36:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:36:55 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-10 11:36:55 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-10 11:36:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 11:36:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:36:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:36:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 11:36:58 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=136.669952, avg_loss=0.683350, seen=200, correct=111, accuracy=0.555000
2025-09-10 11:36:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:36:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:37:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:37:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:37:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:37:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:37:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:37:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:37:03 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.499357, avg_loss=0.662484, seen=40, correct=26, accuracy=0.650000
2025-09-10 11:37:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:37:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:37:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:37:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:37:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:37:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:37:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:37:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:37:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:37:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:37:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:37:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:37:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:37:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:37:16 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-10 11:37:18 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-10 11:37:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 11:37:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:37:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:37:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 11:37:21 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=139.251755, avg_loss=0.696259, seen=200, correct=113, accuracy=0.565000
2025-09-10 11:37:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:37:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:37:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:37:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:37:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:37:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:37:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:37:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:37:25 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.751917, avg_loss=0.693798, seen=40, correct=25, accuracy=0.625000
2025-09-10 11:37:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:37:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:37:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:37:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:37:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:37:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:37:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:37:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:37:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:37:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:37:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:37:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:37:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:37:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:37:35 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-10 11:37:35 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-10 11:37:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 11:37:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:37:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:37:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 11:37:38 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=140.138794, avg_loss=0.700694, seen=200, correct=104, accuracy=0.520000
2025-09-10 11:37:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:37:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:37:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:37:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:37:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:37:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:37:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:37:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:37:43 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.567970, avg_loss=0.689199, seen=40, correct=24, accuracy=0.600000
2025-09-10 11:37:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:37:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:37:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:37:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:37:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=31 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:37:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=32 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:37:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=33 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:37:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=34 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:37:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=35 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:37:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=36 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:37:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=37 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:37:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=38 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:37:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=39 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:37:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=40 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:37:53 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-10 11:37:53 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-10 11:37:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 11:37:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:37:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:37:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 11:37:56 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=137.989426, avg_loss=0.689947, seen=200, correct=112, accuracy=0.560000
2025-09-10 11:37:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:37:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:37:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:38:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:38:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:38:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:38:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:38:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:38:01 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.604794, avg_loss=0.665120, seen=40, correct=27, accuracy=0.675000
2025-09-10 11:38:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:38:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:38:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:38:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:38:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=41 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:38:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=42 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:38:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=43 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:38:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=44 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:38:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=45 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:38:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=46 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:38:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=47 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:38:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=48 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:38:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=49 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:38:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=50 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:38:09 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-10 11:38:10 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-10 11:38:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 11:38:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:38:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:38:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 11:38:13 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=136.443970, avg_loss=0.682220, seen=200, correct=119, accuracy=0.595000
2025-09-10 11:38:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:38:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:38:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:38:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:38:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:38:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:38:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:38:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:38:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.227390, avg_loss=0.655685, seen=40, correct=26, accuracy=0.650000
2025-09-10 11:38:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:38:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:38:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:38:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:38:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=51 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:38:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=52 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:38:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=53 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:38:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=54 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:38:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=55 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:38:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=56 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:38:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=57 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:38:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=58 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:38:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=59 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:38:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=60 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:38:28 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-10 11:38:28 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-10 11:38:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 11:38:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:38:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:38:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 11:38:31 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.932373, avg_loss=0.679662, seen=200, correct=110, accuracy=0.550000
2025-09-10 11:38:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:38:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:38:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:38:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:38:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:38:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:38:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:38:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:38:37 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.634920, avg_loss=0.640873, seen=40, correct=27, accuracy=0.675000
2025-09-10 11:38:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:38:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:38:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:38:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:38:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=61 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:38:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=62 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:38:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=63 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:38:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=64 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:38:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=65 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:38:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=66 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:38:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=67 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:38:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=68 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:38:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=69 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:38:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=70 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:38:46 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-10 11:38:46 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-10 11:38:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 11:38:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:38:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:38:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 11:38:49 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=136.241730, avg_loss=0.681209, seen=200, correct=112, accuracy=0.560000
2025-09-10 11:38:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:38:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:38:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:38:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:38:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:38:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:38:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:38:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:38:53 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.690489, avg_loss=0.642262, seen=40, correct=23, accuracy=0.575000
2025-09-10 11:38:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:38:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:38:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:38:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:38:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=71 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:38:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=72 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:39:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=73 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:39:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=74 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:39:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=75 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:39:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=76 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:39:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=77 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:39:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=78 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:39:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=79 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:39:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=80 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:39:04 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-10 11:39:04 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-10 11:39:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 11:39:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:39:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:39:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 11:39:07 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.827011, avg_loss=0.679135, seen=200, correct=115, accuracy=0.575000
2025-09-10 11:39:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:39:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:39:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:39:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:39:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:39:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:39:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:39:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:39:13 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.352526, avg_loss=0.633813, seen=40, correct=24, accuracy=0.600000
2025-09-10 11:39:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:39:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:39:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:39:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:39:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=81 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:39:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=82 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:39:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=83 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:39:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=84 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:39:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=85 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:39:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=86 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:39:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=87 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:39:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=88 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:39:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=89 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:39:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=90 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:39:22 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-10 11:39:23 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-10 11:39:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 11:39:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:39:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:39:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 11:39:27 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=133.360382, avg_loss=0.666802, seen=200, correct=121, accuracy=0.605000
2025-09-10 11:39:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:39:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:39:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:39:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:39:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:39:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:39:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:39:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:39:34 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.295177, avg_loss=0.657379, seen=40, correct=25, accuracy=0.625000
2025-09-10 11:39:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:39:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:39:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:39:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:39:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=91 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:39:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=92 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:39:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=93 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:39:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=94 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:39:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=95 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:39:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=96 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:39:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=97 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:39:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=98 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:39:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=99 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:39:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=100 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:39:42 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-10 11:39:43 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-10 11:39:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 11:39:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:39:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:39:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 11:39:46 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=134.237518, avg_loss=0.671188, seen=200, correct=121, accuracy=0.605000
2025-09-10 11:39:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:39:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:39:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:39:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:39:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:39:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:39:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:39:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:39:50 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.285133, avg_loss=0.682128, seen=40, correct=25, accuracy=0.625000
2025-09-10 11:39:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:39:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:39:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:39:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:39:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-10 11:39:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-10 11:39:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:39:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:39:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:39:53 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #37', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-10 11:39:53 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #37', 'Round': 0, 'Results_raw': {}}
2025-09-10 11:39:53 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 11:39:53 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 2 for training...
2025-09-10 11:39:53 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-10 11:39:53 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-10 11:39:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-10 11:39:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:39:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=4, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:39:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-10 11:39:54 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.633342, avg_loss=0.693940, seen=11, correct=9, accuracy=0.818182
2025-09-10 11:39:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:39:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:39:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:39:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 11:39:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:39:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:39:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:39:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:39:57 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.549261, avg_loss=0.738732, seen=40, correct=22, accuracy=0.550000
2025-09-10 11:39:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:39:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:39:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:39:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 11:39:58 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-10 11:39:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=56, total=224)
2025-09-10 11:39:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:39:58 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-10 11:39:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:39:58 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=28, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-10 11:39:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:40:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:40:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:40:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:40:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:40:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:40:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:40:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:40:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:40:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:40:05 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-10 11:40:07 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-10 11:40:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-10 11:40:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:40:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:40:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-10 11:40:07 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.382908, avg_loss=0.671173, seen=11, correct=8, accuracy=0.727273
2025-09-10 11:40:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:40:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:40:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:40:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:40:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:40:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:40:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:40:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:40:12 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.045460, avg_loss=0.726136, seen=40, correct=19, accuracy=0.475000
2025-09-10 11:40:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:40:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:40:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:40:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:40:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:40:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:40:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:40:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:40:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:40:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:40:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:40:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:40:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:40:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:40:20 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-10 11:40:21 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-10 11:40:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-10 11:40:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:40:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:40:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-10 11:40:21 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.976068, avg_loss=0.725097, seen=11, correct=7, accuracy=0.636364
2025-09-10 11:40:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:40:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:40:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:40:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:40:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:40:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:40:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:40:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:40:26 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.012381, avg_loss=0.700310, seen=40, correct=23, accuracy=0.575000
2025-09-10 11:40:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:40:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:40:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:40:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:40:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:40:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:40:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:40:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:40:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:40:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:40:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:40:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:40:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:40:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:40:37 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-10 11:40:37 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-10 11:40:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-10 11:40:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:40:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:40:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-10 11:40:38 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=8.080959, avg_loss=0.734633, seen=11, correct=6, accuracy=0.545455
2025-09-10 11:40:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:40:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:40:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:40:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:40:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:40:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:40:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:40:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:40:42 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.419210, avg_loss=0.685480, seen=40, correct=24, accuracy=0.600000
2025-09-10 11:40:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:40:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:40:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:40:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:40:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=31 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:40:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=32 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:40:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=33 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:40:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=34 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:40:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=35 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:40:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=36 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:40:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=37 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:40:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=38 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:40:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=39 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:40:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=40 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:40:52 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-10 11:40:53 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-10 11:40:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-10 11:40:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:40:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:40:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-10 11:40:54 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=8.066652, avg_loss=0.733332, seen=11, correct=6, accuracy=0.545455
2025-09-10 11:40:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:40:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:40:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:40:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:40:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:40:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:40:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:40:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:40:58 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.607857, avg_loss=0.690196, seen=40, correct=24, accuracy=0.600000
2025-09-10 11:40:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:40:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:41:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:41:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:41:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=41 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:41:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=42 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:41:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=43 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:41:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=44 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:41:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=45 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:41:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=46 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:41:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=47 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:41:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=48 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:41:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=49 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:41:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=50 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:41:13 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-10 11:41:13 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-10 11:41:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-10 11:41:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:41:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:41:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-10 11:41:14 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.720234, avg_loss=0.701839, seen=11, correct=6, accuracy=0.545455
2025-09-10 11:41:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:41:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:41:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:41:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:41:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:41:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:41:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:41:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:41:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.636389, avg_loss=0.690910, seen=40, correct=24, accuracy=0.600000
2025-09-10 11:41:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:41:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:41:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:41:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:41:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=51 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:41:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=52 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:41:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=53 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:41:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=54 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:41:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=55 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:41:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=56 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:41:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=57 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:41:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=58 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:41:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=59 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:41:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=60 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:41:27 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-10 11:41:30 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-10 11:41:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-10 11:41:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:41:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:41:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-10 11:41:31 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.703335, avg_loss=0.700303, seen=11, correct=6, accuracy=0.545455
2025-09-10 11:41:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:41:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:41:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:41:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:41:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:41:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:41:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:41:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:41:36 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.360140, avg_loss=0.684003, seen=40, correct=24, accuracy=0.600000
2025-09-10 11:41:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:41:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:41:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:41:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:41:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=61 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:41:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=62 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:41:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=63 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:41:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=64 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:41:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=65 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:41:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=66 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:41:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=67 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:41:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=68 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:41:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=69 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:41:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=70 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:41:46 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-10 11:41:46 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-10 11:41:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-10 11:41:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:41:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:41:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-10 11:41:47 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.856474, avg_loss=0.714225, seen=11, correct=6, accuracy=0.545455
2025-09-10 11:41:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:41:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:41:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:41:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:41:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:41:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:41:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:41:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:41:55 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.571091, avg_loss=0.689277, seen=40, correct=24, accuracy=0.600000
2025-09-10 11:41:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:41:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:41:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:41:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:41:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=71 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:42:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=72 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:42:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=73 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:42:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=74 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:42:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=75 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:42:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=76 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:42:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=77 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:42:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=78 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:42:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=79 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:42:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=80 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:42:05 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-10 11:42:06 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-10 11:42:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-10 11:42:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:42:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:42:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-10 11:42:06 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.756496, avg_loss=0.705136, seen=11, correct=7, accuracy=0.636364
2025-09-10 11:42:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:42:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:42:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:42:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:42:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:42:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:42:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:42:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:42:13 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.758524, avg_loss=0.693963, seen=40, correct=24, accuracy=0.600000
2025-09-10 11:42:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:42:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:42:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:42:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:42:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=81 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:42:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=82 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:42:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=83 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:42:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=84 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:42:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=85 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:42:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=86 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:42:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=87 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:42:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=88 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:42:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=89 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:42:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=90 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:42:24 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-10 11:42:25 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-10 11:42:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-10 11:42:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:42:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:42:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-10 11:42:26 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.783849, avg_loss=0.707623, seen=11, correct=6, accuracy=0.545455
2025-09-10 11:42:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:42:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:42:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:42:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:42:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:42:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:42:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:42:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:42:30 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.552364, avg_loss=0.688809, seen=40, correct=25, accuracy=0.625000
2025-09-10 11:42:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:42:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:42:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:42:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:42:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=91 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:42:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=92 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:42:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=93 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:42:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=94 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:42:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=95 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:42:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=96 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:42:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=97 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:42:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=98 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:42:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=99 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:42:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=100 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:42:40 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-10 11:42:42 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-10 11:42:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-10 11:42:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:42:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:42:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-10 11:42:43 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.925584, avg_loss=0.720508, seen=11, correct=7, accuracy=0.636364
2025-09-10 11:42:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:42:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:42:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:42:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:42:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:42:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:42:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:42:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:42:45 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.929808, avg_loss=0.698245, seen=40, correct=23, accuracy=0.575000
2025-09-10 11:42:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:42:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:42:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:42:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:42:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-10 11:42:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-10 11:42:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:42:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:42:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:42:48 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #22', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-10 11:42:48 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #22', 'Round': 0, 'Results_raw': {}}
2025-09-10 11:42:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 11:42:48 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-10 11:42:49 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-10 11:42:49 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-10 11:42:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-09-10 11:42:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:42:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=63, num_train_batch_last_epoch=37, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:42:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-09-10 11:42:52 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=126, loss_sum=86.976700, avg_loss=0.690291, seen=126, correct=65, accuracy=0.515873
2025-09-10 11:42:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:42:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:42:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:42:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 11:42:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:42:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:42:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:42:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:42:56 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.053574, avg_loss=0.651339, seen=40, correct=23, accuracy=0.575000
2025-09-10 11:42:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:42:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:42:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:42:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 11:42:57 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-10 11:42:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=600, total=2399)
2025-09-10 11:42:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:42:58 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-10 11:42:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:42:58 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=300, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-10 11:43:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:43:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:43:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:43:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:43:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:43:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:43:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:43:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:43:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:43:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:43:09 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-10 11:43:09 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-10 11:43:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-09-10 11:43:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:43:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=63, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:43:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-09-10 11:43:12 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=126, loss_sum=85.561623, avg_loss=0.679060, seen=126, correct=76, accuracy=0.603175
2025-09-10 11:43:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:43:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:43:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:43:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:43:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:43:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:43:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:43:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:43:17 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.710941, avg_loss=0.617774, seen=40, correct=27, accuracy=0.675000
2025-09-10 11:43:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:43:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:43:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:43:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:43:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:43:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:43:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:43:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:43:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:43:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:43:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:43:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:43:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:43:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:43:29 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-10 11:43:29 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-10 11:43:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-09-10 11:43:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:43:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=63, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:43:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-09-10 11:43:31 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=126, loss_sum=88.514427, avg_loss=0.702495, seen=126, correct=76, accuracy=0.603175
2025-09-10 11:43:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:43:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:43:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:43:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:43:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:43:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:43:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:43:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:43:35 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.616718, avg_loss=0.615418, seen=40, correct=26, accuracy=0.650000
2025-09-10 11:43:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:43:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:43:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:43:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:43:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:43:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:43:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:43:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:43:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:43:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:43:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:43:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:43:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:43:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:43:42 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-10 11:43:43 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-10 11:43:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-09-10 11:43:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:43:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=63, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:43:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-09-10 11:43:45 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=126, loss_sum=87.418488, avg_loss=0.693798, seen=126, correct=70, accuracy=0.555556
2025-09-10 11:43:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:43:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:43:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:43:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:43:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:43:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:43:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:43:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:43:52 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.157488, avg_loss=0.603937, seen=40, correct=28, accuracy=0.700000
2025-09-10 11:43:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:43:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:43:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:43:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:43:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=31 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:43:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=32 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:43:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=33 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:43:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=34 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:43:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=35 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:43:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=36 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:43:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=37 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:43:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=38 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:43:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=39 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:44:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=40 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:44:00 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-10 11:44:00 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-10 11:44:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-09-10 11:44:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:44:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=63, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:44:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-09-10 11:44:02 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=126, loss_sum=85.416382, avg_loss=0.677908, seen=126, correct=78, accuracy=0.619048
2025-09-10 11:44:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:44:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:44:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:44:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:44:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:44:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:44:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:44:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:44:07 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.604441, avg_loss=0.615111, seen=40, correct=32, accuracy=0.800000
2025-09-10 11:44:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:44:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:44:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:44:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:44:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=41 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:44:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=42 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:44:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=43 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:44:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=44 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:44:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=45 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:44:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=46 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:44:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=47 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:44:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=48 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:44:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=49 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:44:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=50 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:44:20 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-10 11:44:20 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-10 11:44:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-09-10 11:44:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:44:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=63, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:44:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-09-10 11:44:22 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=126, loss_sum=87.507187, avg_loss=0.694501, seen=126, correct=69, accuracy=0.547619
2025-09-10 11:44:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:44:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:44:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:44:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:44:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:44:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:44:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:44:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:44:26 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.066921, avg_loss=0.626673, seen=40, correct=27, accuracy=0.675000
2025-09-10 11:44:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:44:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:44:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:44:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:44:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=51 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:44:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=52 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:44:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=53 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:44:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=54 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:44:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=55 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:44:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=56 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:44:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=57 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:44:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=58 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:44:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=59 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:44:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=60 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:44:38 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-10 11:44:38 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-10 11:44:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-09-10 11:44:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:44:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=63, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:44:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-09-10 11:44:40 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=126, loss_sum=88.677666, avg_loss=0.703791, seen=126, correct=62, accuracy=0.492063
2025-09-10 11:44:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:44:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:44:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:44:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:44:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:44:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:44:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:44:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:44:43 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.352316, avg_loss=0.633808, seen=40, correct=24, accuracy=0.600000
2025-09-10 11:44:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:44:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:44:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:44:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:44:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=61 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:44:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=62 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:44:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=63 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:44:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=64 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:44:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=65 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:44:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=66 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:44:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=67 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:44:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=68 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:44:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=69 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:44:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=70 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:44:53 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-10 11:44:54 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-10 11:44:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-09-10 11:44:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:44:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=63, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:44:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-09-10 11:44:56 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=126, loss_sum=88.300819, avg_loss=0.700800, seen=126, correct=66, accuracy=0.523810
2025-09-10 11:44:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:44:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:44:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:44:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:44:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:44:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:44:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:45:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:45:00 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.171978, avg_loss=0.629299, seen=40, correct=27, accuracy=0.675000
2025-09-10 11:45:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:45:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:45:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:45:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:45:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=71 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:45:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=72 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:45:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=73 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:45:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=74 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:45:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=75 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:45:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=76 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:45:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=77 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:45:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=78 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:45:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=79 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:45:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=80 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:45:09 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-10 11:45:09 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-10 11:45:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-09-10 11:45:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:45:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=63, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:45:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-09-10 11:45:11 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=126, loss_sum=88.420677, avg_loss=0.701751, seen=126, correct=68, accuracy=0.539683
2025-09-10 11:45:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:45:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:45:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:45:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:45:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:45:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:45:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:45:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:45:21 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.279556, avg_loss=0.631989, seen=40, correct=24, accuracy=0.600000
2025-09-10 11:45:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:45:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:45:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:45:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:45:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=81 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:45:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=82 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:45:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=83 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:45:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=84 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:45:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=85 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:45:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=86 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:45:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=87 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:45:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=88 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:45:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=89 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:45:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=90 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:45:29 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-10 11:45:29 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-10 11:45:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-09-10 11:45:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:45:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=63, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:45:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-09-10 11:45:31 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=126, loss_sum=87.826515, avg_loss=0.697036, seen=126, correct=74, accuracy=0.587302
2025-09-10 11:45:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:45:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:45:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:45:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:45:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:45:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:45:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:45:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:45:35 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.380590, avg_loss=0.609515, seen=40, correct=27, accuracy=0.675000
2025-09-10 11:45:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:45:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:45:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:45:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:45:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=91 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:45:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=92 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:45:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=93 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:45:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=94 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:45:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=95 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:45:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=96 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:45:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=97 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:45:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=98 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:45:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=99 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:45:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=100 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:45:48 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-10 11:45:48 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-10 11:45:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-09-10 11:45:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:45:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=63, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:45:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-09-10 11:45:50 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=126, loss_sum=86.571747, avg_loss=0.687077, seen=126, correct=70, accuracy=0.555556
2025-09-10 11:45:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:45:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:45:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:45:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:45:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:45:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:45:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:45:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:45:55 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.083614, avg_loss=0.602090, seen=40, correct=30, accuracy=0.750000
2025-09-10 11:45:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:45:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:45:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:45:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:45:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-10 11:45:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-10 11:45:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:45:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:45:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:45:58 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #20', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-10 11:45:58 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #20', 'Round': 0, 'Results_raw': {}}
2025-09-10 11:45:58 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 11:45:58 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-10 11:45:59 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-10 11:45:59 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-10 11:45:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-09-10 11:45:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:45:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=32, num_train_batch_last_epoch=4, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:46:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-09-10 11:46:01 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=63, loss_sum=41.009930, avg_loss=0.650951, seen=63, correct=42, accuracy=0.666667
2025-09-10 11:46:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:46:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:46:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:46:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 11:46:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:46:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:46:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:46:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:46:06 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.689796, avg_loss=0.617245, seen=40, correct=28, accuracy=0.700000
2025-09-10 11:46:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:46:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:46:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:46:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 11:46:09 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-10 11:46:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=303, total=1209)
2025-09-10 11:46:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:46:09 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-10 11:46:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:46:09 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=152, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-10 11:46:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:46:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:46:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:46:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:46:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:46:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:46:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:46:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:46:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:46:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:46:16 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-10 11:46:16 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-10 11:46:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-09-10 11:46:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:46:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=32, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:46:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-09-10 11:46:17 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=63, loss_sum=43.383530, avg_loss=0.688627, seen=63, correct=40, accuracy=0.634921
2025-09-10 11:46:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:46:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:46:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:46:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:46:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:46:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:46:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:46:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:46:22 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.911093, avg_loss=0.622777, seen=40, correct=28, accuracy=0.700000
2025-09-10 11:46:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:46:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:46:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:46:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:46:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:46:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:46:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:46:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:46:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:46:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:46:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:46:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:46:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:46:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:46:35 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-10 11:46:36 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-10 11:46:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-09-10 11:46:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:46:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=32, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:46:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-09-10 11:46:37 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=63, loss_sum=43.694702, avg_loss=0.693567, seen=63, correct=40, accuracy=0.634921
2025-09-10 11:46:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:46:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:46:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:46:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:46:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:46:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:46:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:46:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:46:41 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.456987, avg_loss=0.636425, seen=40, correct=25, accuracy=0.625000
2025-09-10 11:46:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:46:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:46:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:46:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:46:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:46:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:46:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:46:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:46:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:46:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:46:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:46:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:46:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:46:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:46:51 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-10 11:46:51 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-10 11:46:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-09-10 11:46:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:46:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=32, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:46:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-09-10 11:46:52 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=63, loss_sum=44.259850, avg_loss=0.702537, seen=63, correct=38, accuracy=0.603175
2025-09-10 11:46:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:46:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:46:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:46:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:46:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:46:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:46:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:46:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:46:57 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.198648, avg_loss=0.629966, seen=40, correct=26, accuracy=0.650000
2025-09-10 11:46:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:46:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:46:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:46:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:46:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=31 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:47:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=32 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:47:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=33 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:47:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=34 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:47:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=35 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:47:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=36 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:47:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=37 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:47:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=38 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:47:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=39 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:47:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=40 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:47:05 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-10 11:47:06 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-10 11:47:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-09-10 11:47:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:47:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=32, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:47:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-09-10 11:47:07 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=63, loss_sum=44.518089, avg_loss=0.706636, seen=63, correct=37, accuracy=0.587302
2025-09-10 11:47:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:47:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:47:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:47:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:47:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:47:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:47:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:47:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:47:11 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.190571, avg_loss=0.629764, seen=40, correct=26, accuracy=0.650000
2025-09-10 11:47:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:47:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:47:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:47:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:47:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=41 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:47:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=42 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:47:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=43 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:47:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=44 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:47:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=45 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:47:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=46 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:47:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=47 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:47:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=48 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:47:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=49 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:47:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=50 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:47:24 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-10 11:47:24 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-10 11:47:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-09-10 11:47:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:47:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=32, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:47:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-09-10 11:47:25 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=63, loss_sum=43.981163, avg_loss=0.698114, seen=63, correct=35, accuracy=0.555556
2025-09-10 11:47:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:47:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:47:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:47:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:47:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:47:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:47:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:47:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:47:30 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.158478, avg_loss=0.628962, seen=40, correct=27, accuracy=0.675000
2025-09-10 11:47:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:47:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:47:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:47:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:47:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=51 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:47:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=52 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:47:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=53 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:47:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=54 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:47:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=55 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:47:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=56 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:47:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=57 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:47:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=58 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:47:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=59 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:47:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=60 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:47:41 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-10 11:47:43 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-10 11:47:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-09-10 11:47:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:47:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=32, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:47:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-09-10 11:47:44 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=63, loss_sum=42.747215, avg_loss=0.678527, seen=63, correct=38, accuracy=0.603175
2025-09-10 11:47:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:47:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:47:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:47:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:47:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:47:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:47:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:47:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:47:48 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.189747, avg_loss=0.604744, seen=40, correct=27, accuracy=0.675000
2025-09-10 11:47:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:47:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:47:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:47:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:47:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=61 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:47:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=62 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:47:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=63 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:47:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=64 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:47:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=65 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:47:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=66 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:47:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=67 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:47:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=68 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:47:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=69 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:47:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=70 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:47:57 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-10 11:47:59 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-10 11:47:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-09-10 11:47:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:47:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=32, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:48:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-09-10 11:48:00 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=63, loss_sum=41.766239, avg_loss=0.662956, seen=63, correct=39, accuracy=0.619048
2025-09-10 11:48:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:48:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:48:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:48:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:48:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:48:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:48:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:48:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:48:10 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.824610, avg_loss=0.595615, seen=40, correct=27, accuracy=0.675000
2025-09-10 11:48:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:48:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:48:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:48:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:48:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=71 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:48:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=72 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:48:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=73 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:48:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=74 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:48:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=75 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:48:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=76 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:48:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=77 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:48:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=78 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:48:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=79 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:48:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=80 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:48:19 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-10 11:48:19 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-10 11:48:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-09-10 11:48:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:48:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=32, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:48:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-09-10 11:48:21 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=63, loss_sum=41.336868, avg_loss=0.656141, seen=63, correct=40, accuracy=0.634921
2025-09-10 11:48:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:48:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:48:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:48:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:48:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:48:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:48:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:48:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:48:27 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.708000, avg_loss=0.592700, seen=40, correct=27, accuracy=0.675000
2025-09-10 11:48:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:48:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:48:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:48:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:48:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=81 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:48:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=82 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:48:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=83 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:48:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=84 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:48:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=85 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:48:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=86 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:48:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=87 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:48:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=88 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:48:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=89 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:48:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=90 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:48:40 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-10 11:48:40 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-10 11:48:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-09-10 11:48:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:48:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=32, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:48:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-09-10 11:48:41 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=63, loss_sum=41.702427, avg_loss=0.661943, seen=63, correct=40, accuracy=0.634921
2025-09-10 11:48:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:48:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:48:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:48:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:48:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:48:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:48:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:48:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:48:44 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.651857, avg_loss=0.591296, seen=40, correct=30, accuracy=0.750000
2025-09-10 11:48:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:48:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:48:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:48:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:48:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=91 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:48:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=92 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:48:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=93 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:48:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=94 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:48:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=95 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:48:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=96 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:48:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=97 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:48:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=98 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:48:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=99 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:48:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=100 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:48:53 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-10 11:48:56 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-10 11:48:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-09-10 11:48:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:48:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=32, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:48:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-09-10 11:48:57 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=63, loss_sum=42.941418, avg_loss=0.681610, seen=63, correct=40, accuracy=0.634921
2025-09-10 11:48:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:48:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:48:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:48:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:48:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:48:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:48:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:49:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:49:01 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.092134, avg_loss=0.602303, seen=40, correct=27, accuracy=0.675000
2025-09-10 11:49:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:49:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:49:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:49:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:49:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-10 11:49:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-10 11:49:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:49:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:49:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2246MB allocated=2204MB
2025-09-10 11:49:04 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #10', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-10 11:49:04 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #10', 'Round': 0, 'Results_raw': {}}
2025-09-10 11:49:04 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 11:49:04 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 2 for training...
2025-09-10 11:49:05 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-10 11:49:05 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-10 11:49:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 11:49:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:49:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:49:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 11:49:09 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=129.173080, avg_loss=0.645865, seen=200, correct=124, accuracy=0.620000
2025-09-10 11:49:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:49:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:49:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:49:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 11:49:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:49:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:49:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:49:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:49:11 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.898457, avg_loss=0.672461, seen=40, correct=26, accuracy=0.650000
2025-09-10 11:49:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:49:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:49:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:49:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 11:49:13 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-10 11:49:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1002, total=4005)
2025-09-10 11:49:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:49:14 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-10 11:49:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:49:14 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=501, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-10 11:49:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:49:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:49:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:49:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:49:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:49:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:49:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:49:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:49:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:49:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:49:21 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-10 11:49:23 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-10 11:49:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 11:49:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:49:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:49:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 11:49:26 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=129.789551, avg_loss=0.648948, seen=200, correct=120, accuracy=0.600000
2025-09-10 11:49:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:49:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:49:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:49:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:49:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:49:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:49:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:49:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:49:29 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.478737, avg_loss=0.661968, seen=40, correct=23, accuracy=0.575000
2025-09-10 11:49:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:49:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:49:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:49:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:49:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:49:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:49:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:49:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:49:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:49:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:49:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:49:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:49:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:49:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:49:39 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-10 11:49:39 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-10 11:49:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 11:49:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:49:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:49:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 11:49:43 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=131.119522, avg_loss=0.655598, seen=200, correct=118, accuracy=0.590000
2025-09-10 11:49:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:49:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:49:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:49:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:49:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:49:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:49:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:49:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:49:47 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.277493, avg_loss=0.656937, seen=40, correct=23, accuracy=0.575000
2025-09-10 11:49:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:49:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:49:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:49:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:49:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:49:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:49:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:49:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:49:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:49:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:49:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:49:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:49:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:49:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:49:56 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-10 11:49:56 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-10 11:49:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 11:49:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:49:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:49:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 11:49:59 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=131.104263, avg_loss=0.655521, seen=200, correct=114, accuracy=0.570000
2025-09-10 11:49:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:49:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:50:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:50:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:50:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:50:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:50:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:50:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:50:02 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.541430, avg_loss=0.638536, seen=40, correct=23, accuracy=0.575000
2025-09-10 11:50:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:50:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:50:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:50:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:50:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=31 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:50:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=32 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:50:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=33 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:50:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=34 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:50:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=35 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:50:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=36 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:50:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=37 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:50:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=38 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:50:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=39 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:50:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=40 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:50:11 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-10 11:50:13 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-10 11:50:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 11:50:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:50:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:50:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 11:50:16 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=132.022690, avg_loss=0.660113, seen=200, correct=116, accuracy=0.580000
2025-09-10 11:50:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:50:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:50:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:50:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:50:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:50:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:50:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:50:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:50:20 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.357067, avg_loss=0.658927, seen=40, correct=22, accuracy=0.550000
2025-09-10 11:50:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:50:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:50:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:50:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:50:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=41 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:50:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=42 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:50:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=43 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:50:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=44 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:50:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=45 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:50:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=46 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:50:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=47 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:50:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=48 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:50:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=49 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:50:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=50 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:50:29 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-10 11:50:31 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-10 11:50:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 11:50:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:50:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:50:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 11:50:34 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=131.017899, avg_loss=0.655089, seen=200, correct=123, accuracy=0.615000
2025-09-10 11:50:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:50:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:50:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:50:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:50:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:50:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:50:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:50:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:50:38 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.585112, avg_loss=0.664628, seen=40, correct=24, accuracy=0.600000
2025-09-10 11:50:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:50:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:50:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:50:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:50:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=51 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:50:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=52 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:50:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=53 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:50:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=54 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:50:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=55 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:50:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=56 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:50:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=57 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:50:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=58 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:50:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=59 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:50:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=60 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:50:46 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-10 11:50:46 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-10 11:50:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 11:50:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:50:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:50:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 11:50:49 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=129.486664, avg_loss=0.647433, seen=200, correct=124, accuracy=0.620000
2025-09-10 11:50:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:50:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:50:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:50:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:50:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:50:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:50:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:50:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:50:54 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.233122, avg_loss=0.655828, seen=40, correct=26, accuracy=0.650000
2025-09-10 11:50:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:50:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:50:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:50:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:50:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=61 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:50:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=62 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:50:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=63 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:50:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=64 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:50:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=65 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:50:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=66 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:51:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=67 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:51:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=68 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:51:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=69 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:51:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=70 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:51:02 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-10 11:51:02 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-10 11:51:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 11:51:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:51:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:51:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 11:51:05 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=129.710739, avg_loss=0.648554, seen=200, correct=117, accuracy=0.585000
2025-09-10 11:51:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:51:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:51:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:51:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:51:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:51:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:51:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:51:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:51:09 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.282345, avg_loss=0.657059, seen=40, correct=24, accuracy=0.600000
2025-09-10 11:51:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:51:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:51:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:51:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:51:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=71 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:51:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=72 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:51:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=73 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:51:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=74 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:51:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=75 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:51:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=76 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:51:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=77 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:51:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=78 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:51:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=79 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:51:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=80 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:51:17 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-10 11:51:17 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-10 11:51:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 11:51:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:51:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:51:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 11:51:21 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=130.308472, avg_loss=0.651542, seen=200, correct=122, accuracy=0.610000
2025-09-10 11:51:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:51:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:51:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:51:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:51:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:51:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:51:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:51:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:51:26 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.625036, avg_loss=0.665626, seen=40, correct=22, accuracy=0.550000
2025-09-10 11:51:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:51:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:51:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:51:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:51:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=81 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:51:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=82 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:51:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=83 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:51:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=84 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:51:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=85 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:51:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=86 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:51:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=87 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:51:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=88 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:51:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=89 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:51:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=90 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:51:35 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-10 11:51:36 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-10 11:51:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 11:51:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:51:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:51:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 11:51:39 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=130.075134, avg_loss=0.650376, seen=200, correct=120, accuracy=0.600000
2025-09-10 11:51:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:51:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:51:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:51:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:51:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:51:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:51:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:51:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:51:44 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.549431, avg_loss=0.663736, seen=40, correct=23, accuracy=0.575000
2025-09-10 11:51:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:51:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:51:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:51:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:51:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=91 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:51:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=92 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:51:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=93 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:51:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=94 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:51:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=95 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:51:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=96 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:51:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=97 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:51:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=98 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:51:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=99 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:51:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=100 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:51:52 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-10 11:51:54 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-10 11:51:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 11:51:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:51:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:51:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 11:51:57 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=133.126938, avg_loss=0.665635, seen=200, correct=116, accuracy=0.580000
2025-09-10 11:51:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:51:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:51:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:52:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:52:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:52:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:52:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:52:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:52:02 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.833534, avg_loss=0.670838, seen=40, correct=21, accuracy=0.525000
2025-09-10 11:52:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:52:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:52:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:52:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:52:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-10 11:52:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-10 11:52:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:52:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:52:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:52:05 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #40', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-10 11:52:05 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #40', 'Round': 0, 'Results_raw': {}}
2025-09-10 11:52:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 11:52:05 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-10 11:52:06 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-10 11:52:06 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-10 11:52:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-09-10 11:52:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:52:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=33, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:52:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 11:52:08 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=133, loss_sum=90.515244, avg_loss=0.680566, seen=133, correct=73, accuracy=0.548872
2025-09-10 11:52:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:52:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:52:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:52:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 11:52:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:52:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:52:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:52:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:52:11 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.987129, avg_loss=0.774678, seen=40, correct=17, accuracy=0.425000
2025-09-10 11:52:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:52:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:52:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:52:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 11:52:12 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-10 11:52:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=632, total=2527)
2025-09-10 11:52:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:52:12 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-10 11:52:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:52:12 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=316, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-10 11:52:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:52:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:52:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:52:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:52:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:52:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:52:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:52:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:52:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:52:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:52:19 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-10 11:52:21 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-10 11:52:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-09-10 11:52:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:52:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:52:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 11:52:23 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=133, loss_sum=95.922684, avg_loss=0.721223, seen=133, correct=71, accuracy=0.533835
2025-09-10 11:52:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:52:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:52:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:52:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:52:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:52:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:52:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:52:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:52:27 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.754082, avg_loss=0.793852, seen=40, correct=18, accuracy=0.450000
2025-09-10 11:52:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:52:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:52:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:52:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:52:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:52:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:52:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:52:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:52:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:52:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:52:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:52:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:52:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:52:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:52:38 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-10 11:52:38 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-10 11:52:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-09-10 11:52:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:52:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:52:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 11:52:40 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=133, loss_sum=94.344124, avg_loss=0.709354, seen=133, correct=71, accuracy=0.533835
2025-09-10 11:52:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:52:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:52:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:52:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:52:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:52:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:52:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:52:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:52:44 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.388958, avg_loss=0.784724, seen=40, correct=18, accuracy=0.450000
2025-09-10 11:52:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:52:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:52:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:52:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:52:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:52:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:52:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:52:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:52:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:52:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:52:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:52:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:52:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:52:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:52:54 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-10 11:52:55 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-10 11:52:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-09-10 11:52:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:52:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:52:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 11:52:57 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=133, loss_sum=91.620720, avg_loss=0.688878, seen=133, correct=70, accuracy=0.526316
2025-09-10 11:52:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:52:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:53:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:53:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:53:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:53:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:53:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:53:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:53:03 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.796034, avg_loss=0.769901, seen=40, correct=18, accuracy=0.450000
2025-09-10 11:53:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:53:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:53:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:53:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:53:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=31 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:53:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=32 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:53:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=33 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:53:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=34 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:53:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=35 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:53:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=36 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:53:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=37 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:53:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=38 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:53:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=39 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:53:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=40 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:53:10 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-10 11:53:11 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-10 11:53:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-09-10 11:53:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:53:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:53:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 11:53:13 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=133, loss_sum=90.791199, avg_loss=0.682641, seen=133, correct=74, accuracy=0.556391
2025-09-10 11:53:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:53:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:53:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:53:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:53:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:53:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:53:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:53:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:53:17 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.187620, avg_loss=0.729691, seen=40, correct=18, accuracy=0.450000
2025-09-10 11:53:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:53:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:53:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:53:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:53:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=41 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:53:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=42 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:53:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=43 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:53:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=44 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:53:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=45 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:53:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=46 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:53:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=47 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:53:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=48 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:53:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=49 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:53:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=50 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:53:26 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-10 11:53:26 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-10 11:53:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-09-10 11:53:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:53:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:53:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 11:53:28 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=133, loss_sum=89.250931, avg_loss=0.671060, seen=133, correct=80, accuracy=0.601504
2025-09-10 11:53:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:53:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:53:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:53:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:53:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:53:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:53:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:53:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:53:34 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.601244, avg_loss=0.765031, seen=40, correct=16, accuracy=0.400000
2025-09-10 11:53:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:53:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:53:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:53:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:53:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=51 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:53:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=52 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:53:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=53 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:53:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=54 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:53:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=55 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:53:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=56 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:53:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=57 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:53:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=58 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:53:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=59 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:53:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=60 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:53:46 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-10 11:53:46 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-10 11:53:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-09-10 11:53:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:53:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:53:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 11:53:48 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=133, loss_sum=90.847488, avg_loss=0.683064, seen=133, correct=75, accuracy=0.563910
2025-09-10 11:53:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:53:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:53:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:53:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:53:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:53:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:53:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:53:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:53:53 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.607300, avg_loss=0.740182, seen=40, correct=18, accuracy=0.450000
2025-09-10 11:53:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:53:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:53:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:53:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:53:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=61 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:53:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=62 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:53:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=63 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:54:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=64 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:54:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=65 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:54:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=66 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:54:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=67 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:54:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=68 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:54:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=69 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:54:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=70 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:54:03 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-10 11:54:04 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-10 11:54:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-09-10 11:54:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:54:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:54:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 11:54:06 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=133, loss_sum=93.439499, avg_loss=0.702553, seen=133, correct=74, accuracy=0.556391
2025-09-10 11:54:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:54:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:54:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:54:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:54:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:54:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:54:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:54:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:54:10 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.694260, avg_loss=0.717356, seen=40, correct=17, accuracy=0.425000
2025-09-10 11:54:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:54:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:54:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:54:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:54:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=71 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:54:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=72 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:54:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=73 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:54:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=74 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:54:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=75 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:54:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=76 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:54:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=77 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:54:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=78 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:54:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=79 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:54:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=80 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:54:21 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-10 11:54:21 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-10 11:54:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-09-10 11:54:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:54:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:54:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 11:54:23 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=133, loss_sum=94.031937, avg_loss=0.707007, seen=133, correct=73, accuracy=0.548872
2025-09-10 11:54:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:54:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:54:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:54:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:54:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:54:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:54:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:54:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:54:26 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.701550, avg_loss=0.717539, seen=40, correct=17, accuracy=0.425000
2025-09-10 11:54:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:54:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:54:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:54:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:54:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=81 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:54:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=82 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:54:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=83 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:54:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=84 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:54:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=85 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:54:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=86 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:54:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=87 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:54:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=88 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:54:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=89 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:54:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=90 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:54:35 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-10 11:54:36 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-10 11:54:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-09-10 11:54:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:54:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:54:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 11:54:38 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=133, loss_sum=91.204544, avg_loss=0.685748, seen=133, correct=79, accuracy=0.593985
2025-09-10 11:54:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:54:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:54:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:54:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:54:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:54:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:54:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:54:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:54:42 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.934698, avg_loss=0.773367, seen=40, correct=18, accuracy=0.450000
2025-09-10 11:54:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:54:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:54:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:54:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:54:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=91 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:54:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=92 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:54:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=93 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:54:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=94 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:54:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=95 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:54:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=96 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:54:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=97 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:54:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=98 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:54:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=99 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:54:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=100 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:54:52 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-10 11:54:53 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-10 11:54:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-09-10 11:54:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:54:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:54:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 11:54:55 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=133, loss_sum=88.757187, avg_loss=0.667347, seen=133, correct=82, accuracy=0.616541
2025-09-10 11:54:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:54:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:54:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:55:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:55:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:55:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:55:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:55:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:55:02 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.548183, avg_loss=0.788705, seen=40, correct=16, accuracy=0.400000
2025-09-10 11:55:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:55:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:55:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:55:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:55:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-10 11:55:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-10 11:55:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:55:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:55:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 11:55:05 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #50', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-10 11:55:05 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #50', 'Round': 0, 'Results_raw': {}}
2025-09-10 11:55:06 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 11:55:06 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 2 for training...
2025-09-10 11:55:06 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-10 11:55:06 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-10 11:55:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-10 11:55:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:55:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=4, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:55:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-10 11:55:07 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.073946, avg_loss=0.643086, seen=11, correct=7, accuracy=0.636364
2025-09-10 11:55:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:55:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:55:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:55:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 11:55:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:55:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:55:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:55:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:55:10 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.096897, avg_loss=0.777422, seen=40, correct=18, accuracy=0.450000
2025-09-10 11:55:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:55:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:55:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:55:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 11:55:11 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-10 11:55:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=54, total=213)
2025-09-10 11:55:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:55:11 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-10 11:55:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:55:11 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=27, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-10 11:55:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:55:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:55:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:55:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:55:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:55:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:55:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:55:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:55:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:55:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:55:17 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-10 11:55:17 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-10 11:55:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-10 11:55:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:55:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:55:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-10 11:55:18 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.816020, avg_loss=0.710547, seen=11, correct=6, accuracy=0.545455
2025-09-10 11:55:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:55:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:55:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:55:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:55:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:55:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:55:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:55:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:55:24 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.973886, avg_loss=0.724347, seen=40, correct=20, accuracy=0.500000
2025-09-10 11:55:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:55:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:55:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:55:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:55:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:55:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:55:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:55:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:55:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:55:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:55:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:55:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:55:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:55:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:55:33 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-10 11:55:34 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-10 11:55:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-10 11:55:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:55:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:55:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-10 11:55:35 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=8.039105, avg_loss=0.730828, seen=11, correct=4, accuracy=0.363636
2025-09-10 11:55:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:55:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:55:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:55:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:55:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:55:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:55:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:55:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:55:39 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.124556, avg_loss=0.728114, seen=40, correct=20, accuracy=0.500000
2025-09-10 11:55:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:55:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:55:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:55:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:55:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:55:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:55:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:55:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:55:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:55:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:55:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:55:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:55:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:55:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:55:49 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-10 11:55:49 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-10 11:55:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-10 11:55:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:55:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:55:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-10 11:55:50 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.579351, avg_loss=0.689032, seen=11, correct=5, accuracy=0.454545
2025-09-10 11:55:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:55:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:55:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:55:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:55:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:55:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:55:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:55:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:55:53 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.635338, avg_loss=0.715883, seen=40, correct=21, accuracy=0.525000
2025-09-10 11:55:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:55:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:55:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:55:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:55:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=31 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:55:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=32 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:55:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=33 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:55:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=34 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:55:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=35 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:55:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=36 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:56:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=37 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:56:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=38 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:56:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=39 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:56:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=40 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:56:02 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-10 11:56:02 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-10 11:56:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-10 11:56:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:56:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:56:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-10 11:56:02 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.341827, avg_loss=0.667439, seen=11, correct=6, accuracy=0.545455
2025-09-10 11:56:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:56:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:56:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:56:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:56:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:56:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:56:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:56:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:56:06 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.536602, avg_loss=0.738415, seen=40, correct=22, accuracy=0.550000
2025-09-10 11:56:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:56:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:56:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:56:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:56:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=41 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:56:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=42 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:56:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=43 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:56:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=44 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:56:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=45 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:56:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=46 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:56:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=47 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:56:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=48 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:56:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=49 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:56:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=50 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:56:15 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-10 11:56:16 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-10 11:56:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-10 11:56:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:56:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:56:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-10 11:56:16 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.175715, avg_loss=0.652338, seen=11, correct=5, accuracy=0.454545
2025-09-10 11:56:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:56:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:56:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:56:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:56:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:56:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:56:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:56:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:56:21 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.984715, avg_loss=0.724618, seen=40, correct=22, accuracy=0.550000
2025-09-10 11:56:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:56:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:56:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:56:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:56:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=51 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:56:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=52 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:56:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=53 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:56:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=54 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:56:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=55 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:56:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=56 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:56:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=57 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:56:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=58 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:56:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=59 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:56:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=60 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:56:30 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-10 11:56:32 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-10 11:56:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-10 11:56:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:56:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:56:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-10 11:56:33 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=6.971740, avg_loss=0.633795, seen=11, correct=5, accuracy=0.454545
2025-09-10 11:56:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:56:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:56:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:56:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:56:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:56:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:56:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:56:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:56:39 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.651569, avg_loss=0.741289, seen=40, correct=21, accuracy=0.525000
2025-09-10 11:56:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:56:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:56:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:56:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:56:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=61 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:56:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=62 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:56:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=63 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:56:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=64 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:56:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=65 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:56:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=66 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:56:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=67 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:56:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=68 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:56:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=69 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:56:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=70 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:56:48 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-10 11:56:48 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-10 11:56:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-10 11:56:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:56:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:56:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-10 11:56:49 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=6.981715, avg_loss=0.634701, seen=11, correct=5, accuracy=0.454545
2025-09-10 11:56:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:56:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:56:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:56:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:56:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:56:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:56:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:56:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:56:54 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.201431, avg_loss=0.755036, seen=40, correct=23, accuracy=0.575000
2025-09-10 11:56:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:56:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:56:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:56:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:56:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=71 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:56:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=72 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:56:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=73 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:56:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=74 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:57:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=75 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:57:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=76 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:57:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=77 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:57:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=78 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:57:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=79 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:57:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=80 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:57:02 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-10 11:57:02 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-10 11:57:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-10 11:57:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:57:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:57:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-10 11:57:03 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.175327, avg_loss=0.652302, seen=11, correct=5, accuracy=0.454545
2025-09-10 11:57:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:57:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:57:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:57:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:57:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:57:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:57:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:57:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:57:07 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.020397, avg_loss=0.775510, seen=40, correct=24, accuracy=0.600000
2025-09-10 11:57:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:57:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:57:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:57:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:57:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=81 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:57:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=82 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:57:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=83 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:57:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=84 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:57:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=85 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:57:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=86 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:57:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=87 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:57:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=88 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:57:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=89 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:57:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=90 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:57:16 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-10 11:57:17 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-10 11:57:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-10 11:57:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:57:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:57:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-10 11:57:17 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.071356, avg_loss=0.642851, seen=11, correct=5, accuracy=0.454545
2025-09-10 11:57:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:57:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:57:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:57:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:57:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:57:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:57:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:57:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:57:21 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.608362, avg_loss=0.790209, seen=40, correct=22, accuracy=0.550000
2025-09-10 11:57:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:57:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:57:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:57:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:57:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=91 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:57:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=92 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:57:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=93 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:57:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=94 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:57:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=95 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:57:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=96 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:57:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=97 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:57:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=98 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:57:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=99 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:57:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=100 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:57:28 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-10 11:57:30 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-10 11:57:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-10 11:57:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:57:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:57:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-10 11:57:30 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=6.938856, avg_loss=0.630805, seen=11, correct=6, accuracy=0.545455
2025-09-10 11:57:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:57:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:57:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:57:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:57:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:57:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:57:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:57:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:57:34 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=33.209660, avg_loss=0.830241, seen=40, correct=21, accuracy=0.525000
2025-09-10 11:57:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:57:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:57:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:57:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:57:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-10 11:57:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-10 11:57:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:57:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:57:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2250MB allocated=2204MB
2025-09-10 11:57:37 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #4', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-10 11:57:37 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #4', 'Round': 0, 'Results_raw': {}}
2025-09-10 11:57:37 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 11:57:37 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-10 11:57:38 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-10 11:57:38 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-10 11:57:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-09-10 11:57:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:57:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=73, num_train_batch_last_epoch=27, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:57:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-10 11:57:41 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=146, loss_sum=96.828064, avg_loss=0.663206, seen=146, correct=88, accuracy=0.602740
2025-09-10 11:57:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:57:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:57:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:57:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 11:57:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:57:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:57:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:57:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:57:43 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.190872, avg_loss=0.579772, seen=40, correct=26, accuracy=0.650000
2025-09-10 11:57:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:57:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:57:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:57:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 11:57:45 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-10 11:57:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=699, total=2793)
2025-09-10 11:57:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:57:45 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-10 11:57:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:57:45 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=350, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-10 11:57:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:57:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:57:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:57:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:57:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:57:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:57:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:57:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:57:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:57:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:57:53 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-10 11:57:54 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-10 11:57:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-09-10 11:57:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:57:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=73, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:57:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-10 11:57:56 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=146, loss_sum=96.824921, avg_loss=0.663184, seen=146, correct=88, accuracy=0.602740
2025-09-10 11:57:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:57:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:57:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:58:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:58:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:58:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:58:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:58:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:58:03 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.410179, avg_loss=0.585254, seen=40, correct=27, accuracy=0.675000
2025-09-10 11:58:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:58:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:58:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:58:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:58:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:58:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:58:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:58:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:58:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:58:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:58:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:58:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:58:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:58:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:58:13 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-10 11:58:16 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-10 11:58:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-09-10 11:58:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:58:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=73, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:58:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-10 11:58:18 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=146, loss_sum=98.400818, avg_loss=0.673978, seen=146, correct=90, accuracy=0.616438
2025-09-10 11:58:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:58:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:58:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:58:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:58:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:58:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:58:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:58:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:58:22 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.381529, avg_loss=0.609538, seen=40, correct=25, accuracy=0.625000
2025-09-10 11:58:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:58:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:58:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:58:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:58:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:58:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:58:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:58:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:58:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:58:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:58:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:58:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:58:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:58:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:58:32 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-10 11:58:34 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-10 11:58:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-09-10 11:58:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:58:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=73, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:58:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-10 11:58:36 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=146, loss_sum=99.539421, avg_loss=0.681777, seen=146, correct=84, accuracy=0.575342
2025-09-10 11:58:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:58:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:58:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:58:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:58:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:58:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:58:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:58:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:58:40 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.085693, avg_loss=0.577142, seen=40, correct=28, accuracy=0.700000
2025-09-10 11:58:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:58:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:58:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:58:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:58:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=31 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:58:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=32 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:58:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=33 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:58:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=34 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:58:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=35 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:58:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=36 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:58:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=37 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:58:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=38 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:58:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=39 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:58:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=40 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:58:50 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-10 11:58:53 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-10 11:58:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-09-10 11:58:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:58:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=73, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:58:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-10 11:58:55 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=146, loss_sum=97.554695, avg_loss=0.668183, seen=146, correct=90, accuracy=0.616438
2025-09-10 11:58:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:58:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:58:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:58:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:58:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:58:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:58:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:58:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:58:59 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=22.544704, avg_loss=0.563618, seen=40, correct=26, accuracy=0.650000
2025-09-10 11:58:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:58:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:59:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:59:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:59:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=41 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:59:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=42 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:59:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=43 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:59:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=44 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:59:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=45 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:59:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=46 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:59:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=47 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:59:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=48 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:59:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=49 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:59:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=50 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:59:09 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-10 11:59:10 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-10 11:59:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-09-10 11:59:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:59:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=73, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:59:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-10 11:59:12 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=146, loss_sum=96.478622, avg_loss=0.660812, seen=146, correct=92, accuracy=0.630137
2025-09-10 11:59:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:59:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:59:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:59:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:59:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:59:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:59:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:59:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:59:17 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=22.635742, avg_loss=0.565894, seen=40, correct=30, accuracy=0.750000
2025-09-10 11:59:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:59:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:59:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:59:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:59:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=51 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:59:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=52 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:59:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=53 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:59:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=54 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:59:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=55 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:59:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=56 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:59:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=57 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:59:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=58 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:59:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=59 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:59:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=60 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:59:26 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-10 11:59:28 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-10 11:59:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-09-10 11:59:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:59:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=73, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:59:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-10 11:59:31 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=146, loss_sum=93.773125, avg_loss=0.642282, seen=146, correct=95, accuracy=0.650685
2025-09-10 11:59:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:59:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:59:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:59:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:59:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:59:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:59:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:59:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:59:37 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=22.911741, avg_loss=0.572794, seen=40, correct=29, accuracy=0.725000
2025-09-10 11:59:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:59:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:59:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:59:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:59:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=61 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:59:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=62 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:59:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=63 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:59:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=64 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:59:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=65 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:59:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=66 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:59:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=67 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:59:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=68 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:59:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=69 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:59:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=70 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:59:46 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-10 11:59:49 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-10 11:59:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-09-10 11:59:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:59:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=73, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:59:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-10 11:59:51 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=146, loss_sum=93.873215, avg_loss=0.642967, seen=146, correct=92, accuracy=0.630137
2025-09-10 11:59:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:59:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:59:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:59:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:59:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 11:59:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:59:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 11:59:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 11:59:55 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.417118, avg_loss=0.585428, seen=40, correct=27, accuracy=0.675000
2025-09-10 11:59:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 11:59:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 11:59:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 11:59:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 11:59:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=71 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 11:59:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=72 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:00:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=73 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:00:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=74 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:00:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=75 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:00:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=76 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:00:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=77 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:00:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=78 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:00:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=79 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:00:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=80 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:00:03 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-10 12:00:06 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-10 12:00:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-09-10 12:00:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:00:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=73, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:00:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-10 12:00:08 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=146, loss_sum=93.491112, avg_loss=0.640350, seen=146, correct=92, accuracy=0.630137
2025-09-10 12:00:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:00:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:00:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:00:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 12:00:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:00:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:00:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:00:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:00:13 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.474012, avg_loss=0.586850, seen=40, correct=28, accuracy=0.700000
2025-09-10 12:00:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:00:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:00:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:00:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 12:00:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=81 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:00:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=82 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:00:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=83 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:00:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=84 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:00:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=85 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:00:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=86 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:00:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=87 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:00:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=88 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:00:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=89 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:00:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=90 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:00:22 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-10 12:00:23 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-10 12:00:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-09-10 12:00:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:00:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=73, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:00:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-10 12:00:26 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=146, loss_sum=93.655556, avg_loss=0.641476, seen=146, correct=94, accuracy=0.643836
2025-09-10 12:00:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:00:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:00:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:00:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 12:00:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:00:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:00:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:00:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:00:33 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.525137, avg_loss=0.588128, seen=40, correct=27, accuracy=0.675000
2025-09-10 12:00:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:00:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:00:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:00:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 12:00:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=91 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:00:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=92 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:00:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=93 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:00:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=94 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:00:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=95 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:00:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=96 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:00:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=97 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:00:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=98 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:00:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=99 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:00:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=100 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:00:42 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-10 12:00:44 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-10 12:00:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-09-10 12:00:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:00:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=73, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:00:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-10 12:00:46 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=146, loss_sum=93.862740, avg_loss=0.642895, seen=146, correct=89, accuracy=0.609589
2025-09-10 12:00:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:00:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:00:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:00:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 12:00:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:00:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:00:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:00:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:00:53 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.686523, avg_loss=0.592163, seen=40, correct=27, accuracy=0.675000
2025-09-10 12:00:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:00:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:00:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:00:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 12:00:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-10 12:00:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-10 12:00:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:00:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:00:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2234MB allocated=2204MB
2025-09-10 12:00:56 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #1', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-10 12:00:56 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #1', 'Round': 0, 'Results_raw': {}}
2025-09-10 12:00:56 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:00:57 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-10 12:00:57 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-10 12:00:57 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-10 12:00:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-09-10 12:00:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:00:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=23, num_train_batch_last_epoch=8, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:00:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-09-10 12:00:59 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=46, loss_sum=31.845770, avg_loss=0.692299, seen=46, correct=26, accuracy=0.565217
2025-09-10 12:00:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:00:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:01:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:01:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:01:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:01:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:01:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:01:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:01:03 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.690863, avg_loss=0.742272, seen=40, correct=18, accuracy=0.450000
2025-09-10 12:01:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:01:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:01:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:01:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:01:04 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-10 12:01:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=220, total=880)
2025-09-10 12:01:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:01:05 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-10 12:01:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:01:05 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=110, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-10 12:01:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:01:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:01:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:01:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:01:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:01:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:01:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:01:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:01:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:01:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:01:11 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-10 12:01:12 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-10 12:01:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-09-10 12:01:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:01:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=23, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:01:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-09-10 12:01:13 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=46, loss_sum=31.607307, avg_loss=0.687115, seen=46, correct=23, accuracy=0.500000
2025-09-10 12:01:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:01:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:01:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:01:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 12:01:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:01:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:01:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:01:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:01:19 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.791351, avg_loss=0.694784, seen=40, correct=23, accuracy=0.575000
2025-09-10 12:01:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:01:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:01:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:01:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 12:01:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:01:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:01:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:01:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:01:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:01:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:01:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:01:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:01:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:01:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:01:30 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-10 12:01:30 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-10 12:01:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-09-10 12:01:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:01:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=23, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:01:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-09-10 12:01:31 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=46, loss_sum=32.440071, avg_loss=0.705219, seen=46, correct=21, accuracy=0.456522
2025-09-10 12:01:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:01:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:01:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:01:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 12:01:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:01:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:01:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:01:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:01:35 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.939463, avg_loss=0.698487, seen=40, correct=20, accuracy=0.500000
2025-09-10 12:01:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:01:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:01:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:01:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 12:01:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:01:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:01:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:01:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:01:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:01:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:01:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:01:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:01:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:01:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:01:50 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-10 12:01:50 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-10 12:01:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-09-10 12:01:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:01:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=23, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:01:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-09-10 12:01:51 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=46, loss_sum=32.647110, avg_loss=0.709720, seen=46, correct=23, accuracy=0.500000
2025-09-10 12:01:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:01:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:01:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:01:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 12:01:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:01:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:01:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:01:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:01:57 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.199137, avg_loss=0.679978, seen=40, correct=20, accuracy=0.500000
2025-09-10 12:01:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:01:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:01:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:01:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 12:02:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=31 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:02:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=32 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:02:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=33 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:02:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=34 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:02:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=35 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:02:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=36 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:02:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=37 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:02:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=38 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:02:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=39 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:02:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=40 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:02:10 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-10 12:02:10 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-10 12:02:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-09-10 12:02:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:02:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=23, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:02:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-09-10 12:02:11 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=46, loss_sum=32.730865, avg_loss=0.711541, seen=46, correct=21, accuracy=0.456522
2025-09-10 12:02:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:02:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:02:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:02:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 12:02:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:02:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:02:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:02:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:02:15 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.154087, avg_loss=0.703852, seen=40, correct=21, accuracy=0.525000
2025-09-10 12:02:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:02:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:02:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:02:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 12:02:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=41 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:02:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=42 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:02:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=43 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:02:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=44 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:02:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=45 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:02:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=46 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:02:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=47 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:02:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=48 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:02:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=49 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:02:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=50 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:02:27 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-10 12:02:27 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-10 12:02:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-09-10 12:02:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:02:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=23, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:02:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-09-10 12:02:28 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=46, loss_sum=31.857176, avg_loss=0.692547, seen=46, correct=24, accuracy=0.521739
2025-09-10 12:02:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:02:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:02:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:02:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 12:02:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:02:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:02:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:02:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:02:36 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.721554, avg_loss=0.718039, seen=40, correct=19, accuracy=0.475000
2025-09-10 12:02:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:02:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:02:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:02:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 12:02:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=51 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:02:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=52 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:02:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=53 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:02:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=54 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:02:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=55 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:02:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=56 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:02:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=57 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:02:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=58 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:02:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=59 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:02:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=60 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:02:45 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-10 12:02:47 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-10 12:02:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-09-10 12:02:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:02:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=23, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:02:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-09-10 12:02:48 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=46, loss_sum=31.519741, avg_loss=0.685212, seen=46, correct=24, accuracy=0.521739
2025-09-10 12:02:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:02:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:02:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:02:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 12:02:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:02:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:02:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:02:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:02:52 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.194653, avg_loss=0.704866, seen=40, correct=21, accuracy=0.525000
2025-09-10 12:02:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:02:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:02:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:02:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 12:02:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=61 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:02:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=62 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:03:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=63 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:03:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=64 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:03:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=65 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:03:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=66 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:03:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=67 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:03:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=68 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:03:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=69 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:03:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=70 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:03:04 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-10 12:03:05 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-10 12:03:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-09-10 12:03:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:03:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=23, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:03:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-09-10 12:03:06 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=46, loss_sum=31.821487, avg_loss=0.691771, seen=46, correct=22, accuracy=0.478261
2025-09-10 12:03:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:03:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:03:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:03:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 12:03:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:03:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:03:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:03:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:03:10 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.292994, avg_loss=0.707325, seen=40, correct=22, accuracy=0.550000
2025-09-10 12:03:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:03:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:03:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:03:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 12:03:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=71 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:03:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=72 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:03:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=73 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:03:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=74 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:03:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=75 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:03:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=76 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:03:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=77 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:03:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=78 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:03:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=79 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:03:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=80 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:03:22 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-10 12:03:22 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-10 12:03:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-09-10 12:03:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:03:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=23, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:03:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-09-10 12:03:23 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=46, loss_sum=32.607735, avg_loss=0.708864, seen=46, correct=22, accuracy=0.478261
2025-09-10 12:03:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:03:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:03:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:03:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 12:03:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:03:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:03:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:03:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:03:28 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.323738, avg_loss=0.683093, seen=40, correct=23, accuracy=0.575000
2025-09-10 12:03:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:03:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:03:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:03:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 12:03:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=81 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:03:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=82 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:03:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=83 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:03:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=84 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:03:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=85 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:03:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=86 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:03:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=87 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:03:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=88 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:03:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=89 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:03:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=90 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:03:38 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-10 12:03:38 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-10 12:03:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-09-10 12:03:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:03:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=23, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:03:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-09-10 12:03:39 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=46, loss_sum=32.758575, avg_loss=0.712143, seen=46, correct=20, accuracy=0.434783
2025-09-10 12:03:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:03:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:03:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:03:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 12:03:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:03:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:03:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:03:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:03:43 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.560230, avg_loss=0.689006, seen=40, correct=23, accuracy=0.575000
2025-09-10 12:03:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:03:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:03:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:03:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 12:03:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=91 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:03:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=92 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:03:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=93 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:03:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=94 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:03:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=95 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:03:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=96 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:03:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=97 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:03:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=98 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:03:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=99 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:03:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=100 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:03:54 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-10 12:03:54 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-10 12:03:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-09-10 12:03:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:03:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=23, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:03:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-09-10 12:03:55 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=46, loss_sum=32.068764, avg_loss=0.697147, seen=46, correct=23, accuracy=0.500000
2025-09-10 12:03:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:03:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:03:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:03:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 12:03:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:03:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:03:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:04:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:04:01 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.470264, avg_loss=0.711757, seen=40, correct=19, accuracy=0.475000
2025-09-10 12:04:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:04:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:04:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:04:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 12:04:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-10 12:04:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-10 12:04:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:04:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:04:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 12:04:05 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #48', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-10 12:04:05 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #48', 'Round': 0, 'Results_raw': {}}
2025-09-10 12:04:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:04:05 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-10 12:04:05 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-10 12:04:05 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-10 12:04:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-09-10 12:04:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:04:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=50, num_train_batch_last_epoch=50, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:04:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-09-10 12:04:08 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=100, loss_sum=65.359192, avg_loss=0.653592, seen=100, correct=62, accuracy=0.620000
2025-09-10 12:04:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:04:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:04:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:04:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:04:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:04:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:04:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:04:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:04:12 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.093536, avg_loss=0.702338, seen=40, correct=22, accuracy=0.550000
2025-09-10 12:04:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:04:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:04:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:04:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:04:14 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-10 12:04:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=476, total=1901)
2025-09-10 12:04:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:04:15 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-10 12:04:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:04:15 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=238, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-10 12:04:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:04:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:04:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:04:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:04:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:04:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:04:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:04:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:04:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:04:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:04:25 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-10 12:04:26 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-10 12:04:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-09-10 12:04:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:04:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=50, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:04:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-09-10 12:04:28 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=100, loss_sum=70.782501, avg_loss=0.707825, seen=100, correct=51, accuracy=0.510000
2025-09-10 12:04:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:04:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:04:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:04:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 12:04:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:04:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:04:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:04:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:04:34 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.124083, avg_loss=0.728102, seen=40, correct=19, accuracy=0.475000
2025-09-10 12:04:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:04:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:04:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:04:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 12:04:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:04:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:04:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:04:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:04:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:04:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:04:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:04:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:04:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:04:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:04:47 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-10 12:04:48 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-10 12:04:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-09-10 12:04:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:04:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=50, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:04:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-09-10 12:04:49 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=100, loss_sum=72.490074, avg_loss=0.724901, seen=100, correct=49, accuracy=0.490000
2025-09-10 12:04:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:04:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:04:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:04:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 12:04:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:04:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:04:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:04:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:04:55 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.924244, avg_loss=0.723106, seen=40, correct=18, accuracy=0.450000
2025-09-10 12:04:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:04:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:04:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:04:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 12:04:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:04:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:04:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:05:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:05:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:05:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:05:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:05:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:05:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:05:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:05:04 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-10 12:05:05 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-10 12:05:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-09-10 12:05:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:05:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=50, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:05:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-09-10 12:05:07 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=100, loss_sum=74.030746, avg_loss=0.740307, seen=100, correct=48, accuracy=0.480000
2025-09-10 12:05:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:05:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:05:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:05:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 12:05:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:05:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:05:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:05:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:05:11 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.542027, avg_loss=0.738551, seen=40, correct=18, accuracy=0.450000
2025-09-10 12:05:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:05:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:05:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:05:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 12:05:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=31 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:05:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=32 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:05:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=33 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:05:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=34 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:05:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=35 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:05:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=36 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:05:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=37 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:05:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=38 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:05:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=39 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:05:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=40 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:05:22 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-10 12:05:23 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-10 12:05:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-09-10 12:05:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:05:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=50, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:05:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-09-10 12:05:24 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=100, loss_sum=69.872238, avg_loss=0.698722, seen=100, correct=56, accuracy=0.560000
2025-09-10 12:05:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:05:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:05:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:05:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 12:05:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:05:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:05:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:05:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:05:30 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.026375, avg_loss=0.725659, seen=40, correct=19, accuracy=0.475000
2025-09-10 12:05:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:05:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:05:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:05:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 12:05:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=41 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:05:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=42 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:05:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=43 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:05:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=44 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:05:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=45 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:05:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=46 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:05:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=47 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:05:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=48 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:05:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=49 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:05:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=50 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:05:39 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-10 12:05:39 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-10 12:05:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-09-10 12:05:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:05:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=50, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:05:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-09-10 12:05:41 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=100, loss_sum=66.131363, avg_loss=0.661314, seen=100, correct=62, accuracy=0.620000
2025-09-10 12:05:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:05:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:05:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:05:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 12:05:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:05:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:05:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:05:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:05:46 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.748447, avg_loss=0.693711, seen=40, correct=24, accuracy=0.600000
2025-09-10 12:05:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:05:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:05:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:05:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 12:05:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=51 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:05:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=52 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:05:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=53 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:05:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=54 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:05:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=55 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:05:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=56 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:05:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=57 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:05:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=58 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:05:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=59 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:05:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=60 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:05:57 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-10 12:05:57 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-10 12:05:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-09-10 12:05:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:05:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=50, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:05:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-09-10 12:05:58 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=100, loss_sum=65.317390, avg_loss=0.653174, seen=100, correct=64, accuracy=0.640000
2025-09-10 12:05:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:05:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:06:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:06:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 12:06:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:06:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:06:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:06:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:06:02 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.776146, avg_loss=0.694404, seen=40, correct=22, accuracy=0.550000
2025-09-10 12:06:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:06:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:06:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:06:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 12:06:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=61 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:06:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=62 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:06:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=63 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:06:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=64 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:06:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=65 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:06:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=66 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:06:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=67 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:06:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=68 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:06:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=69 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:06:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=70 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:06:15 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-10 12:06:15 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-10 12:06:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-09-10 12:06:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:06:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=50, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:06:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-09-10 12:06:17 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=100, loss_sum=65.782578, avg_loss=0.657826, seen=100, correct=61, accuracy=0.610000
2025-09-10 12:06:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:06:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:06:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:06:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 12:06:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:06:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:06:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:06:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:06:22 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.198517, avg_loss=0.704963, seen=40, correct=21, accuracy=0.525000
2025-09-10 12:06:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:06:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:06:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:06:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 12:06:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=71 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:06:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=72 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:06:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=73 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:06:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=74 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:06:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=75 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:06:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=76 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:06:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=77 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:06:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=78 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:06:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=79 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:06:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=80 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:06:34 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-10 12:06:35 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-10 12:06:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-09-10 12:06:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:06:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=50, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:06:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-09-10 12:06:37 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=100, loss_sum=68.676720, avg_loss=0.686767, seen=100, correct=54, accuracy=0.540000
2025-09-10 12:06:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:06:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:06:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:06:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 12:06:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:06:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:06:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:06:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:06:41 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.349382, avg_loss=0.708735, seen=40, correct=19, accuracy=0.475000
2025-09-10 12:06:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:06:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:06:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:06:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 12:06:45 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=81 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:06:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=82 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:06:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=83 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:06:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=84 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:06:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=85 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:06:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=86 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:06:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=87 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:06:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=88 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:06:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=89 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:06:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=90 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:06:53 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-10 12:06:53 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-10 12:06:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-09-10 12:06:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:06:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=50, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:06:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-09-10 12:06:55 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=100, loss_sum=70.188919, avg_loss=0.701889, seen=100, correct=53, accuracy=0.530000
2025-09-10 12:06:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:06:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:06:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:06:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 12:06:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:06:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:06:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:06:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:06:58 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.694691, avg_loss=0.742367, seen=40, correct=18, accuracy=0.450000
2025-09-10 12:06:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:06:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:06:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:07:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 12:07:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=91 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:07:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=92 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:07:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=93 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:07:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=94 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:07:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=95 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:07:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=96 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:07:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=97 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:07:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=98 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:07:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=99 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:07:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=100 (sync_gradients=True), num_train_batch_last_epoch=200
2025-09-10 12:07:08 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-10 12:07:09 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-10 12:07:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-09-10 12:07:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:07:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=50, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:07:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-09-10 12:07:11 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=100, loss_sum=67.162445, avg_loss=0.671624, seen=100, correct=59, accuracy=0.590000
2025-09-10 12:07:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:07:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:07:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:07:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 12:07:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:07:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:07:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:07:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:07:15 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.472992, avg_loss=0.736825, seen=40, correct=20, accuracy=0.500000
2025-09-10 12:07:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:07:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:07:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:07:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 12:07:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-10 12:07:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-10 12:07:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:07:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:07:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2236MB allocated=2204MB
2025-09-10 12:07:19 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #45', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-10 12:07:19 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #45', 'Round': 0, 'Results_raw': {}}
2025-09-10 12:07:20 (federatedscope.core.workers.server:433) INFO: Server: Training is finished! Starting evaluation.
2025-09-10 12:07:20 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:07:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-09-10 12:07:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:07:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=73, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:07:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-10 12:07:23 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=146, loss_sum=96.828064, avg_loss=0.663206, seen=146, correct=88, accuracy=0.602740
2025-09-10 12:07:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:07:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:07:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:07:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:07:25 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 146, 'val_loss': 96.82806396484375, 'val_avg_loss': 0.663205917567423, 'val_seen': 146, 'val_correct': 88, 'val_acc': 0.6027397260273972}
2025-09-10 12:07:25 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-10 12:07:25 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 146, 'val_loss': 96.82806396484375, 'val_avg_loss': 0.663205917567423, 'val_seen': 146, 'val_correct': 88, 'val_acc': 0.6027397260273972}
2025-09-10 12:07:25 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #1', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 37, 'val_loss': 21.352861404418945, 'val_avg_loss': 0.5771043622815931, 'val_seen': 37, 'val_correct': 28, 'val_acc': 0.7567567567567568}}
2025-09-10 12:07:25 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #1', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 146, 'val_loss': 96.82806396484375, 'val_avg_loss': 0.663205917567423, 'val_seen': 146, 'val_correct': 88, 'val_acc': 0.6027397260273972}}
2025-09-10 12:07:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:07:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:07:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:07:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:07:26 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.190872, avg_loss=0.579772, seen=40, correct=26, accuracy=0.650000
2025-09-10 12:07:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:07:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:07:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:07:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:07:28 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 23.190872192382812, 'test_avg_loss': 0.5797718048095704, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-10 12:07:28 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 146, 'val_loss': 96.82806396484375, 'val_avg_loss': 0.663205917567423, 'val_seen': 146, 'val_correct': 88, 'val_acc': 0.6027397260273972}
2025-09-10 12:07:28 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 23.190872192382812, 'test_avg_loss': 0.5797718048095704, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-10 12:07:28 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #1', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 5.059710741043091, 'test_avg_loss': 0.5059710741043091, 'test_seen': 10, 'test_correct': 8, 'test_acc': 0.8}}
2025-09-10 12:07:28 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #1', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 23.190872192382812, 'test_avg_loss': 0.5797718048095704, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}}
2025-09-10 12:07:28 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 23.190872192382812, 'test_avg_loss': 0.5797718048095704, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}, metrics={'val_total': 146, 'val_loss': 96.82806396484375, 'val_avg_loss': 0.663205917567423, 'val_seen': 146, 'val_correct': 88, 'val_acc': 0.6027397260273972, 'test_total': 40, 'test_loss': 23.190872192382812, 'test_avg_loss': 0.5797718048095704, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-10 12:07:28 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-10 12:07:28 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:07:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-10 12:07:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:07:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:07:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-10 12:07:30 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=9.719805, avg_loss=0.883619, seen=11, correct=4, accuracy=0.363636
2025-09-10 12:07:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:07:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:07:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:07:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:07:31 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 11, 'val_loss': 9.719804763793945, 'val_avg_loss': 0.8836186148903586, 'val_seen': 11, 'val_correct': 4, 'val_acc': 0.36363636363636365}
2025-09-10 12:07:31 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-10 12:07:31 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 11, 'val_loss': 9.719804763793945, 'val_avg_loss': 0.8836186148903586, 'val_seen': 11, 'val_correct': 4, 'val_acc': 0.36363636363636365}
2025-09-10 12:07:31 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #2', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 3, 'val_loss': 2.6199156045913696, 'val_avg_loss': 0.8733052015304565, 'val_seen': 3, 'val_correct': 1, 'val_acc': 0.3333333333333333}}
2025-09-10 12:07:31 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #2', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 11, 'val_loss': 9.719804763793945, 'val_avg_loss': 0.8836186148903586, 'val_seen': 11, 'val_correct': 4, 'val_acc': 0.36363636363636365}}
2025-09-10 12:07:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:07:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:07:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:07:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:07:33 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.738035, avg_loss=0.693451, seen=40, correct=21, accuracy=0.525000
2025-09-10 12:07:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:07:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:07:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:07:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:07:34 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 27.738035202026367, 'test_avg_loss': 0.6934508800506591, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-10 12:07:34 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 11, 'val_loss': 9.719804763793945, 'val_avg_loss': 0.8836186148903586, 'val_seen': 11, 'val_correct': 4, 'val_acc': 0.36363636363636365}
2025-09-10 12:07:34 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 27.738035202026367, 'test_avg_loss': 0.6934508800506591, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-10 12:07:34 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #2', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.091633200645447, 'test_avg_loss': 0.6091633200645447, 'test_seen': 10, 'test_correct': 6, 'test_acc': 0.6}}
2025-09-10 12:07:34 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #2', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 27.738035202026367, 'test_avg_loss': 0.6934508800506591, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}}
2025-09-10 12:07:34 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 27.738035202026367, 'test_avg_loss': 0.6934508800506591, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}, metrics={'val_total': 11, 'val_loss': 9.719804763793945, 'val_avg_loss': 0.8836186148903586, 'val_seen': 11, 'val_correct': 4, 'val_acc': 0.36363636363636365, 'test_total': 40, 'test_loss': 27.738035202026367, 'test_avg_loss': 0.6934508800506591, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-10 12:07:34 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-10 12:07:34 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:07:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-09-10 12:07:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:07:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=18, num_train_batch_last_epoch=200, num_train_epoch=6, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:07:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-09-10 12:07:36 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=36, loss_sum=26.420765, avg_loss=0.733910, seen=36, correct=18, accuracy=0.500000
2025-09-10 12:07:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:07:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:07:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:07:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:07:38 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 36, 'val_loss': 26.420764923095703, 'val_avg_loss': 0.7339101367526584, 'val_seen': 36, 'val_correct': 18, 'val_acc': 0.5}
2025-09-10 12:07:38 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-10 12:07:38 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 36, 'val_loss': 26.420764923095703, 'val_avg_loss': 0.7339101367526584, 'val_seen': 36, 'val_correct': 18, 'val_acc': 0.5}
2025-09-10 12:07:38 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #3', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 9, 'val_loss': 7.274620711803436, 'val_avg_loss': 0.8082911902003818, 'val_seen': 9, 'val_correct': 4, 'val_acc': 0.4444444444444444}}
2025-09-10 12:07:38 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #3', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 36, 'val_loss': 26.420764923095703, 'val_avg_loss': 0.7339101367526584, 'val_seen': 36, 'val_correct': 18, 'val_acc': 0.5}}
2025-09-10 12:07:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:07:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:07:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:07:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:07:39 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.402245, avg_loss=0.710056, seen=40, correct=23, accuracy=0.575000
2025-09-10 12:07:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:07:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:07:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:07:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:07:42 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 28.402244567871094, 'test_avg_loss': 0.7100561141967774, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-10 12:07:42 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 36, 'val_loss': 26.420764923095703, 'val_avg_loss': 0.7339101367526584, 'val_seen': 36, 'val_correct': 18, 'val_acc': 0.5}
2025-09-10 12:07:42 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 28.402244567871094, 'test_avg_loss': 0.7100561141967774, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-10 12:07:42 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #3', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 8.18344533443451, 'test_avg_loss': 0.8183445334434509, 'test_seen': 10, 'test_correct': 4, 'test_acc': 0.4}}
2025-09-10 12:07:42 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #3', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 28.402244567871094, 'test_avg_loss': 0.7100561141967774, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}}
2025-09-10 12:07:42 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 28.402244567871094, 'test_avg_loss': 0.7100561141967774, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}, metrics={'val_total': 36, 'val_loss': 26.420764923095703, 'val_avg_loss': 0.7339101367526584, 'val_seen': 36, 'val_correct': 18, 'val_acc': 0.5, 'test_total': 40, 'test_loss': 28.402244567871094, 'test_avg_loss': 0.7100561141967774, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-10 12:07:42 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-10 12:07:42 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:07:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-10 12:07:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:07:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:07:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-10 12:07:43 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.073946, avg_loss=0.643086, seen=11, correct=7, accuracy=0.636364
2025-09-10 12:07:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:07:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:07:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:07:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:07:45 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 11, 'val_loss': 7.07394552230835, 'val_avg_loss': 0.6430859565734863, 'val_seen': 11, 'val_correct': 7, 'val_acc': 0.6363636363636364}
2025-09-10 12:07:45 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-10 12:07:45 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 11, 'val_loss': 7.07394552230835, 'val_avg_loss': 0.6430859565734863, 'val_seen': 11, 'val_correct': 7, 'val_acc': 0.6363636363636364}
2025-09-10 12:07:45 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #4', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 3, 'val_loss': 2.411185145378113, 'val_avg_loss': 0.8037283817927042, 'val_seen': 3, 'val_correct': 1, 'val_acc': 0.3333333333333333}}
2025-09-10 12:07:45 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #4', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 11, 'val_loss': 7.07394552230835, 'val_avg_loss': 0.6430859565734863, 'val_seen': 11, 'val_correct': 7, 'val_acc': 0.6363636363636364}}
2025-09-10 12:07:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:07:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:07:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:07:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:07:46 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.096897, avg_loss=0.777422, seen=40, correct=18, accuracy=0.450000
2025-09-10 12:07:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:07:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:07:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:07:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:07:48 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 31.09689712524414, 'test_avg_loss': 0.7774224281311035, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-10 12:07:48 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 11, 'val_loss': 7.07394552230835, 'val_avg_loss': 0.6430859565734863, 'val_seen': 11, 'val_correct': 7, 'val_acc': 0.6363636363636364}
2025-09-10 12:07:48 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 31.09689712524414, 'test_avg_loss': 0.7774224281311035, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-10 12:07:48 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #4', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 8.882105350494385, 'test_avg_loss': 0.8882105350494385, 'test_seen': 10, 'test_correct': 4, 'test_acc': 0.4}}
2025-09-10 12:07:48 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #4', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 31.09689712524414, 'test_avg_loss': 0.7774224281311035, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}}
2025-09-10 12:07:48 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 31.09689712524414, 'test_avg_loss': 0.7774224281311035, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}, metrics={'val_total': 11, 'val_loss': 7.07394552230835, 'val_avg_loss': 0.6430859565734863, 'val_seen': 11, 'val_correct': 7, 'val_acc': 0.6363636363636364, 'test_total': 40, 'test_loss': 31.09689712524414, 'test_avg_loss': 0.7774224281311035, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-10 12:07:48 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-10 12:07:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:07:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-09-10 12:07:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:07:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=7, num_train_batch_last_epoch=200, num_train_epoch=15, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:07:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-09-10 12:07:49 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=14, loss_sum=9.031166, avg_loss=0.645083, seen=14, correct=9, accuracy=0.642857
2025-09-10 12:07:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:07:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:07:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:07:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:07:51 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 14, 'val_loss': 9.031166076660156, 'val_avg_loss': 0.6450832911900112, 'val_seen': 14, 'val_correct': 9, 'val_acc': 0.6428571428571429}
2025-09-10 12:07:51 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-10 12:07:51 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 14, 'val_loss': 9.031166076660156, 'val_avg_loss': 0.6450832911900112, 'val_seen': 14, 'val_correct': 9, 'val_acc': 0.6428571428571429}
2025-09-10 12:07:51 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #5', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 4, 'val_loss': 2.4012633562088013, 'val_avg_loss': 0.6003158390522003, 'val_seen': 4, 'val_correct': 3, 'val_acc': 0.75}}
2025-09-10 12:07:51 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #5', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 14, 'val_loss': 9.031166076660156, 'val_avg_loss': 0.6450832911900112, 'val_seen': 14, 'val_correct': 9, 'val_acc': 0.6428571428571429}}
2025-09-10 12:07:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:07:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:07:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:07:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:07:53 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.361305, avg_loss=0.609033, seen=40, correct=27, accuracy=0.675000
2025-09-10 12:07:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:07:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:07:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:07:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:07:55 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 24.361305236816406, 'test_avg_loss': 0.6090326309204102, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-10 12:07:55 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 14, 'val_loss': 9.031166076660156, 'val_avg_loss': 0.6450832911900112, 'val_seen': 14, 'val_correct': 9, 'val_acc': 0.6428571428571429}
2025-09-10 12:07:55 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 24.361305236816406, 'test_avg_loss': 0.6090326309204102, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-10 12:07:55 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #5', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.639134645462036, 'test_avg_loss': 0.6639134645462036, 'test_seen': 10, 'test_correct': 6, 'test_acc': 0.6}}
2025-09-10 12:07:55 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #5', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 24.361305236816406, 'test_avg_loss': 0.6090326309204102, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}}
2025-09-10 12:07:55 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 24.361305236816406, 'test_avg_loss': 0.6090326309204102, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}, metrics={'val_total': 14, 'val_loss': 9.031166076660156, 'val_avg_loss': 0.6450832911900112, 'val_seen': 14, 'val_correct': 9, 'val_acc': 0.6428571428571429, 'test_total': 40, 'test_loss': 24.361305236816406, 'test_avg_loss': 0.6090326309204102, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-10 12:07:55 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-10 12:07:55 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:07:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-09-10 12:07:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:07:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:07:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 12:07:59 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=134, loss_sum=87.186226, avg_loss=0.650643, seen=134, correct=86, accuracy=0.641791
2025-09-10 12:07:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:07:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:08:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:08:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:08:01 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 134, 'val_loss': 87.18622589111328, 'val_avg_loss': 0.6506434767993529, 'val_seen': 134, 'val_correct': 86, 'val_acc': 0.6417910447761194}
2025-09-10 12:08:01 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-10 12:08:01 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 134, 'val_loss': 87.18622589111328, 'val_avg_loss': 0.6506434767993529, 'val_seen': 134, 'val_correct': 86, 'val_acc': 0.6417910447761194}
2025-09-10 12:08:01 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #6', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 34, 'val_loss': 20.184759199619293, 'val_avg_loss': 0.5936693882240969, 'val_seen': 34, 'val_correct': 25, 'val_acc': 0.7352941176470589}}
2025-09-10 12:08:01 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #6', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 134, 'val_loss': 87.18622589111328, 'val_avg_loss': 0.6506434767993529, 'val_seen': 134, 'val_correct': 86, 'val_acc': 0.6417910447761194}}
2025-09-10 12:08:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:08:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:08:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:08:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:08:02 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.164089, avg_loss=0.754102, seen=40, correct=21, accuracy=0.525000
2025-09-10 12:08:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:08:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:08:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:08:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:08:04 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 30.16408920288086, 'test_avg_loss': 0.7541022300720215, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-10 12:08:04 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 134, 'val_loss': 87.18622589111328, 'val_avg_loss': 0.6506434767993529, 'val_seen': 134, 'val_correct': 86, 'val_acc': 0.6417910447761194}
2025-09-10 12:08:04 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 30.16408920288086, 'test_avg_loss': 0.7541022300720215, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-10 12:08:04 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #6', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.808527588844299, 'test_avg_loss': 0.78085275888443, 'test_seen': 10, 'test_correct': 6, 'test_acc': 0.6}}
2025-09-10 12:08:04 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #6', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 30.16408920288086, 'test_avg_loss': 0.7541022300720215, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}}
2025-09-10 12:08:04 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 30.16408920288086, 'test_avg_loss': 0.7541022300720215, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}, metrics={'val_total': 134, 'val_loss': 87.18622589111328, 'val_avg_loss': 0.6506434767993529, 'val_seen': 134, 'val_correct': 86, 'val_acc': 0.6417910447761194, 'test_total': 40, 'test_loss': 30.16408920288086, 'test_avg_loss': 0.7541022300720215, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-10 12:08:04 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-10 12:08:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:08:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-09-10 12:08:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:08:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=29, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:08:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-09-10 12:08:07 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=57, loss_sum=37.267483, avg_loss=0.653815, seen=57, correct=34, accuracy=0.596491
2025-09-10 12:08:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:08:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:08:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:08:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:08:09 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 57, 'val_loss': 37.26748275756836, 'val_avg_loss': 0.6538154869748835, 'val_seen': 57, 'val_correct': 34, 'val_acc': 0.5964912280701754}
2025-09-10 12:08:09 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-10 12:08:09 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 57, 'val_loss': 37.26748275756836, 'val_avg_loss': 0.6538154869748835, 'val_seen': 57, 'val_correct': 34, 'val_acc': 0.5964912280701754}
2025-09-10 12:08:09 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #7', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 15, 'val_loss': 10.024388194084167, 'val_avg_loss': 0.6682925462722779, 'val_seen': 15, 'val_correct': 8, 'val_acc': 0.5333333333333333}}
2025-09-10 12:08:09 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #7', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 57, 'val_loss': 37.26748275756836, 'val_avg_loss': 0.6538154869748835, 'val_seen': 57, 'val_correct': 34, 'val_acc': 0.5964912280701754}}
2025-09-10 12:08:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:08:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:08:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:08:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:08:11 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.277843, avg_loss=0.606946, seen=40, correct=27, accuracy=0.675000
2025-09-10 12:08:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:08:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:08:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:08:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:08:13 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 24.277843475341797, 'test_avg_loss': 0.6069460868835449, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-10 12:08:13 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 57, 'val_loss': 37.26748275756836, 'val_avg_loss': 0.6538154869748835, 'val_seen': 57, 'val_correct': 34, 'val_acc': 0.5964912280701754}
2025-09-10 12:08:13 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 24.277843475341797, 'test_avg_loss': 0.6069460868835449, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-10 12:08:13 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #7', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 4.7226027846336365, 'test_avg_loss': 0.47226027846336366, 'test_seen': 10, 'test_correct': 7, 'test_acc': 0.7}}
2025-09-10 12:08:13 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #7', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 24.277843475341797, 'test_avg_loss': 0.6069460868835449, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}}
2025-09-10 12:08:13 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 24.277843475341797, 'test_avg_loss': 0.6069460868835449, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}, metrics={'val_total': 57, 'val_loss': 37.26748275756836, 'val_avg_loss': 0.6538154869748835, 'val_seen': 57, 'val_correct': 34, 'val_acc': 0.5964912280701754, 'test_total': 40, 'test_loss': 24.277843475341797, 'test_avg_loss': 0.6069460868835449, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-10 12:08:13 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-10 12:08:13 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:08:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-09-10 12:08:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:08:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=35, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:08:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-10 12:08:15 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=69, loss_sum=45.597900, avg_loss=0.660839, seen=69, correct=42, accuracy=0.608696
2025-09-10 12:08:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:08:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:08:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:08:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:08:17 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 69, 'val_loss': 45.597900390625, 'val_avg_loss': 0.6608391360960145, 'val_seen': 69, 'val_correct': 42, 'val_acc': 0.6086956521739131}
2025-09-10 12:08:17 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-10 12:08:17 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 69, 'val_loss': 45.597900390625, 'val_avg_loss': 0.6608391360960145, 'val_seen': 69, 'val_correct': 42, 'val_acc': 0.6086956521739131}
2025-09-10 12:08:17 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #8', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 18, 'val_loss': 11.61257940530777, 'val_avg_loss': 0.6451433002948761, 'val_seen': 18, 'val_correct': 11, 'val_acc': 0.6111111111111112}}
2025-09-10 12:08:17 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #8', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 69, 'val_loss': 45.597900390625, 'val_avg_loss': 0.6608391360960145, 'val_seen': 69, 'val_correct': 42, 'val_acc': 0.6086956521739131}}
2025-09-10 12:08:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:08:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:08:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:08:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:08:19 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.644970, avg_loss=0.766124, seen=40, correct=19, accuracy=0.475000
2025-09-10 12:08:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:08:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:08:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:08:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:08:20 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 30.644969940185547, 'test_avg_loss': 0.7661242485046387, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-10 12:08:20 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 69, 'val_loss': 45.597900390625, 'val_avg_loss': 0.6608391360960145, 'val_seen': 69, 'val_correct': 42, 'val_acc': 0.6086956521739131}
2025-09-10 12:08:20 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 30.644969940185547, 'test_avg_loss': 0.7661242485046387, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-10 12:08:20 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #8', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 8.418378233909607, 'test_avg_loss': 0.8418378233909607, 'test_seen': 10, 'test_correct': 5, 'test_acc': 0.5}}
2025-09-10 12:08:20 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #8', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 30.644969940185547, 'test_avg_loss': 0.7661242485046387, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}}
2025-09-10 12:08:20 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 30.644969940185547, 'test_avg_loss': 0.7661242485046387, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}, metrics={'val_total': 69, 'val_loss': 45.597900390625, 'val_avg_loss': 0.6608391360960145, 'val_seen': 69, 'val_correct': 42, 'val_acc': 0.6086956521739131, 'test_total': 40, 'test_loss': 30.644969940185547, 'test_avg_loss': 0.7661242485046387, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-10 12:08:20 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-10 12:08:20 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:08:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-10 12:08:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:08:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:08:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-10 12:08:25 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=126.516037, avg_loss=0.672958, seen=188, correct=110, accuracy=0.585106
2025-09-10 12:08:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:08:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:08:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:08:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:08:27 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 188, 'val_loss': 126.51603698730469, 'val_avg_loss': 0.6729576435494931, 'val_seen': 188, 'val_correct': 110, 'val_acc': 0.5851063829787234}
2025-09-10 12:08:27 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-10 12:08:27 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 188, 'val_loss': 126.51603698730469, 'val_avg_loss': 0.6729576435494931, 'val_seen': 188, 'val_correct': 110, 'val_acc': 0.5851063829787234}
2025-09-10 12:08:27 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #9', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 47, 'val_loss': 32.33557319641113, 'val_avg_loss': 0.6879909190725773, 'val_seen': 47, 'val_correct': 28, 'val_acc': 0.5957446808510638}}
2025-09-10 12:08:27 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #9', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 188, 'val_loss': 126.51603698730469, 'val_avg_loss': 0.6729576435494931, 'val_seen': 188, 'val_correct': 110, 'val_acc': 0.5851063829787234}}
2025-09-10 12:08:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:08:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:08:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:08:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:08:28 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.840139, avg_loss=0.671003, seen=40, correct=25, accuracy=0.625000
2025-09-10 12:08:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:08:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:08:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:08:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:08:30 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 26.840139389038086, 'test_avg_loss': 0.6710034847259522, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-10 12:08:30 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 188, 'val_loss': 126.51603698730469, 'val_avg_loss': 0.6729576435494931, 'val_seen': 188, 'val_correct': 110, 'val_acc': 0.5851063829787234}
2025-09-10 12:08:30 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 26.840139389038086, 'test_avg_loss': 0.6710034847259522, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-10 12:08:30 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #9', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 5.529775142669678, 'test_avg_loss': 0.5529775142669677, 'test_seen': 10, 'test_correct': 8, 'test_acc': 0.8}}
2025-09-10 12:08:30 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #9', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 26.840139389038086, 'test_avg_loss': 0.6710034847259522, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}}
2025-09-10 12:08:30 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 26.840139389038086, 'test_avg_loss': 0.6710034847259522, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}, metrics={'val_total': 188, 'val_loss': 126.51603698730469, 'val_avg_loss': 0.6729576435494931, 'val_seen': 188, 'val_correct': 110, 'val_acc': 0.5851063829787234, 'test_total': 40, 'test_loss': 26.840139389038086, 'test_avg_loss': 0.6710034847259522, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-10 12:08:30 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-10 12:08:31 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:08:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-09-10 12:08:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:08:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=32, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:08:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-09-10 12:08:33 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=63, loss_sum=41.428799, avg_loss=0.657600, seen=63, correct=40, accuracy=0.634921
2025-09-10 12:08:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:08:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:08:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:08:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:08:35 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 63, 'val_loss': 41.42879867553711, 'val_avg_loss': 0.6575999789767795, 'val_seen': 63, 'val_correct': 40, 'val_acc': 0.6349206349206349}
2025-09-10 12:08:35 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-10 12:08:35 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 63, 'val_loss': 41.42879867553711, 'val_avg_loss': 0.6575999789767795, 'val_seen': 63, 'val_correct': 40, 'val_acc': 0.6349206349206349}
2025-09-10 12:08:35 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #10', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 16, 'val_loss': 10.250514924526215, 'val_avg_loss': 0.6406571827828884, 'val_seen': 16, 'val_correct': 11, 'val_acc': 0.6875}}
2025-09-10 12:08:35 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #10', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 63, 'val_loss': 41.42879867553711, 'val_avg_loss': 0.6575999789767795, 'val_seen': 63, 'val_correct': 40, 'val_acc': 0.6349206349206349}}
2025-09-10 12:08:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:08:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:08:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:08:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:08:36 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.548016, avg_loss=0.638700, seen=40, correct=24, accuracy=0.600000
2025-09-10 12:08:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:08:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:08:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:08:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:08:38 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 25.548015594482422, 'test_avg_loss': 0.6387003898620606, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-10 12:08:38 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 63, 'val_loss': 41.42879867553711, 'val_avg_loss': 0.6575999789767795, 'val_seen': 63, 'val_correct': 40, 'val_acc': 0.6349206349206349}
2025-09-10 12:08:38 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 25.548015594482422, 'test_avg_loss': 0.6387003898620606, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-10 12:08:38 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #10', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.757979512214661, 'test_avg_loss': 0.675797951221466, 'test_seen': 10, 'test_correct': 6, 'test_acc': 0.6}}
2025-09-10 12:08:38 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #10', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 25.548015594482422, 'test_avg_loss': 0.6387003898620606, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}}
2025-09-10 12:08:38 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 25.548015594482422, 'test_avg_loss': 0.6387003898620606, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}, metrics={'val_total': 63, 'val_loss': 41.42879867553711, 'val_avg_loss': 0.6575999789767795, 'val_seen': 63, 'val_correct': 40, 'val_acc': 0.6349206349206349, 'test_total': 40, 'test_loss': 25.548015594482422, 'test_avg_loss': 0.6387003898620606, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-10 12:08:38 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-10 12:08:38 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:08:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-09-10 12:08:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:08:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=16, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:08:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-10 12:08:40 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=32, loss_sum=19.565603, avg_loss=0.611425, seen=32, correct=20, accuracy=0.625000
2025-09-10 12:08:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:08:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:08:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:08:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:08:41 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 32, 'val_loss': 19.565603256225586, 'val_avg_loss': 0.6114251017570496, 'val_seen': 32, 'val_correct': 20, 'val_acc': 0.625}
2025-09-10 12:08:41 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-10 12:08:41 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 32, 'val_loss': 19.565603256225586, 'val_avg_loss': 0.6114251017570496, 'val_seen': 32, 'val_correct': 20, 'val_acc': 0.625}
2025-09-10 12:08:41 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #11', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 8, 'val_loss': 4.626810431480408, 'val_avg_loss': 0.578351303935051, 'val_seen': 8, 'val_correct': 6, 'val_acc': 0.75}}
2025-09-10 12:08:41 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #11', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 32, 'val_loss': 19.565603256225586, 'val_avg_loss': 0.6114251017570496, 'val_seen': 32, 'val_correct': 20, 'val_acc': 0.625}}
2025-09-10 12:08:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:08:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:08:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:08:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:08:43 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.473003, avg_loss=0.736825, seen=40, correct=19, accuracy=0.475000
2025-09-10 12:08:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:08:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:08:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:08:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:08:45 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 29.473003387451172, 'test_avg_loss': 0.7368250846862793, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-10 12:08:45 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 32, 'val_loss': 19.565603256225586, 'val_avg_loss': 0.6114251017570496, 'val_seen': 32, 'val_correct': 20, 'val_acc': 0.625}
2025-09-10 12:08:45 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 29.473003387451172, 'test_avg_loss': 0.7368250846862793, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-10 12:08:45 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #11', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.275775849819183, 'test_avg_loss': 0.6275775849819183, 'test_seen': 10, 'test_correct': 6, 'test_acc': 0.6}}
2025-09-10 12:08:45 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #11', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 29.473003387451172, 'test_avg_loss': 0.7368250846862793, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}}
2025-09-10 12:08:45 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 29.473003387451172, 'test_avg_loss': 0.7368250846862793, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}, metrics={'val_total': 32, 'val_loss': 19.565603256225586, 'val_avg_loss': 0.6114251017570496, 'val_seen': 32, 'val_correct': 20, 'val_acc': 0.625, 'test_total': 40, 'test_loss': 29.473003387451172, 'test_avg_loss': 0.7368250846862793, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-10 12:08:45 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-10 12:08:45 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:08:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-09-10 12:08:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:08:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=69, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:08:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-09-10 12:08:48 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=137, loss_sum=91.303375, avg_loss=0.666448, seen=137, correct=81, accuracy=0.591241
2025-09-10 12:08:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:08:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:08:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:08:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:08:50 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 137, 'val_loss': 91.30337524414062, 'val_avg_loss': 0.6664479944827782, 'val_seen': 137, 'val_correct': 81, 'val_acc': 0.5912408759124088}
2025-09-10 12:08:50 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-10 12:08:50 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 137, 'val_loss': 91.30337524414062, 'val_avg_loss': 0.6664479944827782, 'val_seen': 137, 'val_correct': 81, 'val_acc': 0.5912408759124088}
2025-09-10 12:08:50 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #12', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 35, 'val_loss': 25.052981853485107, 'val_avg_loss': 0.7157994815281459, 'val_seen': 35, 'val_correct': 20, 'val_acc': 0.5714285714285714}}
2025-09-10 12:08:50 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #12', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 137, 'val_loss': 91.30337524414062, 'val_avg_loss': 0.6664479944827782, 'val_seen': 137, 'val_correct': 81, 'val_acc': 0.5912408759124088}}
2025-09-10 12:08:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:08:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:08:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:08:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:08:51 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.508492, avg_loss=0.637712, seen=40, correct=27, accuracy=0.675000
2025-09-10 12:08:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:08:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:08:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:08:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:08:53 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 25.50849151611328, 'test_avg_loss': 0.6377122879028321, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-10 12:08:53 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 137, 'val_loss': 91.30337524414062, 'val_avg_loss': 0.6664479944827782, 'val_seen': 137, 'val_correct': 81, 'val_acc': 0.5912408759124088}
2025-09-10 12:08:53 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 25.50849151611328, 'test_avg_loss': 0.6377122879028321, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-10 12:08:53 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #12', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 4.6494399309158325, 'test_avg_loss': 0.46494399309158324, 'test_seen': 10, 'test_correct': 9, 'test_acc': 0.9}}
2025-09-10 12:08:53 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #12', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 25.50849151611328, 'test_avg_loss': 0.6377122879028321, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}}
2025-09-10 12:08:53 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 25.50849151611328, 'test_avg_loss': 0.6377122879028321, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}, metrics={'val_total': 137, 'val_loss': 91.30337524414062, 'val_avg_loss': 0.6664479944827782, 'val_seen': 137, 'val_correct': 81, 'val_acc': 0.5912408759124088, 'test_total': 40, 'test_loss': 25.50849151611328, 'test_avg_loss': 0.6377122879028321, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-10 12:08:53 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-10 12:08:53 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:08:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-09-10 12:08:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:08:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=36, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:08:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-10 12:08:56 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=72, loss_sum=47.249607, avg_loss=0.656245, seen=72, correct=43, accuracy=0.597222
2025-09-10 12:08:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:08:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:08:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:08:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:08:58 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 72, 'val_loss': 47.24960708618164, 'val_avg_loss': 0.6562445428636339, 'val_seen': 72, 'val_correct': 43, 'val_acc': 0.5972222222222222}
2025-09-10 12:08:58 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-10 12:08:58 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 72, 'val_loss': 47.24960708618164, 'val_avg_loss': 0.6562445428636339, 'val_seen': 72, 'val_correct': 43, 'val_acc': 0.5972222222222222}
2025-09-10 12:08:58 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #13', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 18, 'val_loss': 11.59383037686348, 'val_avg_loss': 0.6441016876035266, 'val_seen': 18, 'val_correct': 11, 'val_acc': 0.6111111111111112}}
2025-09-10 12:08:58 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #13', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 72, 'val_loss': 47.24960708618164, 'val_avg_loss': 0.6562445428636339, 'val_seen': 72, 'val_correct': 43, 'val_acc': 0.5972222222222222}}
2025-09-10 12:08:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:08:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:08:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:09:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:09:00 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.553753, avg_loss=0.638844, seen=40, correct=26, accuracy=0.650000
2025-09-10 12:09:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:09:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:09:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:09:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:09:02 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 25.553752899169922, 'test_avg_loss': 0.6388438224792481, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-10 12:09:02 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 72, 'val_loss': 47.24960708618164, 'val_avg_loss': 0.6562445428636339, 'val_seen': 72, 'val_correct': 43, 'val_acc': 0.5972222222222222}
2025-09-10 12:09:02 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 25.553752899169922, 'test_avg_loss': 0.6388438224792481, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-10 12:09:02 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #13', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 5.583183765411377, 'test_avg_loss': 0.5583183765411377, 'test_seen': 10, 'test_correct': 7, 'test_acc': 0.7}}
2025-09-10 12:09:02 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #13', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 25.553752899169922, 'test_avg_loss': 0.6388438224792481, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}}
2025-09-10 12:09:02 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 25.553752899169922, 'test_avg_loss': 0.6388438224792481, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}, metrics={'val_total': 72, 'val_loss': 47.24960708618164, 'val_avg_loss': 0.6562445428636339, 'val_seen': 72, 'val_correct': 43, 'val_acc': 0.5972222222222222, 'test_total': 40, 'test_loss': 25.553752899169922, 'test_avg_loss': 0.6388438224792481, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-10 12:09:02 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-10 12:09:02 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:09:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-09-10 12:09:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:09:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=80, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:09:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-09-10 12:09:06 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=160, loss_sum=105.378448, avg_loss=0.658615, seen=160, correct=98, accuracy=0.612500
2025-09-10 12:09:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:09:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:09:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:09:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:09:09 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 160, 'val_loss': 105.37844848632812, 'val_avg_loss': 0.6586153030395507, 'val_seen': 160, 'val_correct': 98, 'val_acc': 0.6125}
2025-09-10 12:09:09 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-10 12:09:09 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 160, 'val_loss': 105.37844848632812, 'val_avg_loss': 0.6586153030395507, 'val_seen': 160, 'val_correct': 98, 'val_acc': 0.6125}
2025-09-10 12:09:09 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #14', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 40, 'val_loss': 24.626299619674683, 'val_avg_loss': 0.615657490491867, 'val_seen': 40, 'val_correct': 22, 'val_acc': 0.55}}
2025-09-10 12:09:09 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #14', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 160, 'val_loss': 105.37844848632812, 'val_avg_loss': 0.6586153030395507, 'val_seen': 160, 'val_correct': 98, 'val_acc': 0.6125}}
2025-09-10 12:09:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:09:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:09:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:09:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:09:11 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.685223, avg_loss=0.592131, seen=40, correct=29, accuracy=0.725000
2025-09-10 12:09:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:09:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:09:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:09:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:09:14 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 23.685222625732422, 'test_avg_loss': 0.5921305656433106, 'test_seen': 40, 'test_correct': 29, 'test_acc': 0.725}
2025-09-10 12:09:14 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 160, 'val_loss': 105.37844848632812, 'val_avg_loss': 0.6586153030395507, 'val_seen': 160, 'val_correct': 98, 'val_acc': 0.6125}
2025-09-10 12:09:14 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 23.685222625732422, 'test_avg_loss': 0.5921305656433106, 'test_seen': 40, 'test_correct': 29, 'test_acc': 0.725}
2025-09-10 12:09:14 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #14', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 5.337053537368774, 'test_avg_loss': 0.5337053537368774, 'test_seen': 10, 'test_correct': 9, 'test_acc': 0.9}}
2025-09-10 12:09:14 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #14', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 23.685222625732422, 'test_avg_loss': 0.5921305656433106, 'test_seen': 40, 'test_correct': 29, 'test_acc': 0.725}}
2025-09-10 12:09:14 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 23.685222625732422, 'test_avg_loss': 0.5921305656433106, 'test_seen': 40, 'test_correct': 29, 'test_acc': 0.725}, metrics={'val_total': 160, 'val_loss': 105.37844848632812, 'val_avg_loss': 0.6586153030395507, 'val_seen': 160, 'val_correct': 98, 'val_acc': 0.6125, 'test_total': 40, 'test_loss': 23.685222625732422, 'test_avg_loss': 0.5921305656433106, 'test_seen': 40, 'test_correct': 29, 'test_acc': 0.725}
2025-09-10 12:09:14 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-10 12:09:14 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:09:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 12:09:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:09:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:09:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 12:09:19 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=133.652313, avg_loss=0.668262, seen=200, correct=128, accuracy=0.640000
2025-09-10 12:09:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:09:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:09:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:09:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:09:20 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 200, 'val_loss': 133.65231323242188, 'val_avg_loss': 0.6682615661621094, 'val_seen': 200, 'val_correct': 128, 'val_acc': 0.64}
2025-09-10 12:09:20 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-10 12:09:20 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 200, 'val_loss': 133.65231323242188, 'val_avg_loss': 0.6682615661621094, 'val_seen': 200, 'val_correct': 128, 'val_acc': 0.64}
2025-09-10 12:09:20 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #15', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 50, 'val_loss': 31.822262167930603, 'val_avg_loss': 0.6364452433586121, 'val_seen': 50, 'val_correct': 34, 'val_acc': 0.68}}
2025-09-10 12:09:20 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #15', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 200, 'val_loss': 133.65231323242188, 'val_avg_loss': 0.6682615661621094, 'val_seen': 200, 'val_correct': 128, 'val_acc': 0.64}}
2025-09-10 12:09:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:09:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:09:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:09:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:09:22 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.865768, avg_loss=0.696644, seen=40, correct=23, accuracy=0.575000
2025-09-10 12:09:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:09:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:09:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:09:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:09:25 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 27.865768432617188, 'test_avg_loss': 0.6966442108154297, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-10 12:09:25 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 200, 'val_loss': 133.65231323242188, 'val_avg_loss': 0.6682615661621094, 'val_seen': 200, 'val_correct': 128, 'val_acc': 0.64}
2025-09-10 12:09:25 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 27.865768432617188, 'test_avg_loss': 0.6966442108154297, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-10 12:09:25 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #15', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.531301498413086, 'test_avg_loss': 0.6531301498413086, 'test_seen': 10, 'test_correct': 5, 'test_acc': 0.5}}
2025-09-10 12:09:25 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #15', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 27.865768432617188, 'test_avg_loss': 0.6966442108154297, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}}
2025-09-10 12:09:25 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 27.865768432617188, 'test_avg_loss': 0.6966442108154297, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}, metrics={'val_total': 200, 'val_loss': 133.65231323242188, 'val_avg_loss': 0.6682615661621094, 'val_seen': 200, 'val_correct': 128, 'val_acc': 0.64, 'test_total': 40, 'test_loss': 27.865768432617188, 'test_avg_loss': 0.6966442108154297, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-10 12:09:25 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-10 12:09:25 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:09:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-09-10 12:09:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:09:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:09:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 12:09:29 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=136, loss_sum=88.547684, avg_loss=0.651086, seen=136, correct=85, accuracy=0.625000
2025-09-10 12:09:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:09:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:09:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:09:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:09:32 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 136, 'val_loss': 88.54768371582031, 'val_avg_loss': 0.6510859096751493, 'val_seen': 136, 'val_correct': 85, 'val_acc': 0.625}
2025-09-10 12:09:32 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-10 12:09:32 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 136, 'val_loss': 88.54768371582031, 'val_avg_loss': 0.6510859096751493, 'val_seen': 136, 'val_correct': 85, 'val_acc': 0.625}
2025-09-10 12:09:32 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #16', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 34, 'val_loss': 21.875456929206848, 'val_avg_loss': 0.6433957920354956, 'val_seen': 34, 'val_correct': 25, 'val_acc': 0.7352941176470589}}
2025-09-10 12:09:32 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #16', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 136, 'val_loss': 88.54768371582031, 'val_avg_loss': 0.6510859096751493, 'val_seen': 136, 'val_correct': 85, 'val_acc': 0.625}}
2025-09-10 12:09:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:09:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:09:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:09:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:09:34 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.744915, avg_loss=0.693623, seen=40, correct=25, accuracy=0.625000
2025-09-10 12:09:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:09:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:09:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:09:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:09:36 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 27.744915008544922, 'test_avg_loss': 0.6936228752136231, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-10 12:09:36 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 136, 'val_loss': 88.54768371582031, 'val_avg_loss': 0.6510859096751493, 'val_seen': 136, 'val_correct': 85, 'val_acc': 0.625}
2025-09-10 12:09:36 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 27.744915008544922, 'test_avg_loss': 0.6936228752136231, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-10 12:09:36 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #16', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.972298741340637, 'test_avg_loss': 0.6972298741340637, 'test_seen': 10, 'test_correct': 6, 'test_acc': 0.6}}
2025-09-10 12:09:36 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #16', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 27.744915008544922, 'test_avg_loss': 0.6936228752136231, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}}
2025-09-10 12:09:36 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 27.744915008544922, 'test_avg_loss': 0.6936228752136231, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}, metrics={'val_total': 136, 'val_loss': 88.54768371582031, 'val_avg_loss': 0.6510859096751493, 'val_seen': 136, 'val_correct': 85, 'val_acc': 0.625, 'test_total': 40, 'test_loss': 27.744915008544922, 'test_avg_loss': 0.6936228752136231, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-10 12:09:36 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-10 12:09:36 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:09:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 12:09:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:09:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:09:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 12:09:40 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=128.078110, avg_loss=0.640391, seen=200, correct=128, accuracy=0.640000
2025-09-10 12:09:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:09:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:09:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:09:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:09:43 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 200, 'val_loss': 128.07810974121094, 'val_avg_loss': 0.6403905487060547, 'val_seen': 200, 'val_correct': 128, 'val_acc': 0.64}
2025-09-10 12:09:43 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-10 12:09:43 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 200, 'val_loss': 128.07810974121094, 'val_avg_loss': 0.6403905487060547, 'val_seen': 200, 'val_correct': 128, 'val_acc': 0.64}
2025-09-10 12:09:43 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #17', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 50, 'val_loss': 30.59964507818222, 'val_avg_loss': 0.6119929015636444, 'val_seen': 50, 'val_correct': 33, 'val_acc': 0.66}}
2025-09-10 12:09:43 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #17', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 200, 'val_loss': 128.07810974121094, 'val_avg_loss': 0.6403905487060547, 'val_seen': 200, 'val_correct': 128, 'val_acc': 0.64}}
2025-09-10 12:09:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:09:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:09:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:09:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:09:45 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.435249, avg_loss=0.685881, seen=40, correct=23, accuracy=0.575000
2025-09-10 12:09:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:09:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:09:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:09:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:09:48 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 27.43524932861328, 'test_avg_loss': 0.685881233215332, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-10 12:09:48 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 200, 'val_loss': 128.07810974121094, 'val_avg_loss': 0.6403905487060547, 'val_seen': 200, 'val_correct': 128, 'val_acc': 0.64}
2025-09-10 12:09:48 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 27.43524932861328, 'test_avg_loss': 0.685881233215332, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-10 12:09:48 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #17', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 8.550152122974396, 'test_avg_loss': 0.8550152122974396, 'test_seen': 10, 'test_correct': 5, 'test_acc': 0.5}}
2025-09-10 12:09:48 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #17', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 27.43524932861328, 'test_avg_loss': 0.685881233215332, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}}
2025-09-10 12:09:48 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 27.43524932861328, 'test_avg_loss': 0.685881233215332, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}, metrics={'val_total': 200, 'val_loss': 128.07810974121094, 'val_avg_loss': 0.6403905487060547, 'val_seen': 200, 'val_correct': 128, 'val_acc': 0.64, 'test_total': 40, 'test_loss': 27.43524932861328, 'test_avg_loss': 0.685881233215332, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-10 12:09:48 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-10 12:09:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:09:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-09-10 12:09:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:09:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:09:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 12:09:52 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=135, loss_sum=93.260048, avg_loss=0.690815, seen=135, correct=84, accuracy=0.622222
2025-09-10 12:09:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:09:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:09:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:09:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:09:54 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 135, 'val_loss': 93.26004791259766, 'val_avg_loss': 0.6908151697229457, 'val_seen': 135, 'val_correct': 84, 'val_acc': 0.6222222222222222}
2025-09-10 12:09:54 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-10 12:09:54 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 135, 'val_loss': 93.26004791259766, 'val_avg_loss': 0.6908151697229457, 'val_seen': 135, 'val_correct': 84, 'val_acc': 0.6222222222222222}
2025-09-10 12:09:54 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #18', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 34, 'val_loss': 23.20473998785019, 'val_avg_loss': 0.6824923525838291, 'val_seen': 34, 'val_correct': 20, 'val_acc': 0.5882352941176471}}
2025-09-10 12:09:54 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #18', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 135, 'val_loss': 93.26004791259766, 'val_avg_loss': 0.6908151697229457, 'val_seen': 135, 'val_correct': 84, 'val_acc': 0.6222222222222222}}
2025-09-10 12:09:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:09:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:09:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:09:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:09:56 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.197376, avg_loss=0.604934, seen=40, correct=26, accuracy=0.650000
2025-09-10 12:09:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:09:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:09:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:09:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:09:57 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 24.197376251220703, 'test_avg_loss': 0.6049344062805175, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-10 12:09:57 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 135, 'val_loss': 93.26004791259766, 'val_avg_loss': 0.6908151697229457, 'val_seen': 135, 'val_correct': 84, 'val_acc': 0.6222222222222222}
2025-09-10 12:09:57 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 24.197376251220703, 'test_avg_loss': 0.6049344062805175, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-10 12:09:57 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #18', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 5.907379627227783, 'test_avg_loss': 0.5907379627227783, 'test_seen': 10, 'test_correct': 6, 'test_acc': 0.6}}
2025-09-10 12:09:57 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #18', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 24.197376251220703, 'test_avg_loss': 0.6049344062805175, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}}
2025-09-10 12:09:57 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 24.197376251220703, 'test_avg_loss': 0.6049344062805175, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}, metrics={'val_total': 135, 'val_loss': 93.26004791259766, 'val_avg_loss': 0.6908151697229457, 'val_seen': 135, 'val_correct': 84, 'val_acc': 0.6222222222222222, 'test_total': 40, 'test_loss': 24.197376251220703, 'test_avg_loss': 0.6049344062805175, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-10 12:09:57 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-10 12:09:57 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:09:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-10 12:09:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:09:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:10:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-10 12:10:00 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=74.888779, avg_loss=0.680807, seen=110, correct=65, accuracy=0.590909
2025-09-10 12:10:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:10:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:10:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:10:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:10:02 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 110, 'val_loss': 74.88877868652344, 'val_avg_loss': 0.6808070789683949, 'val_seen': 110, 'val_correct': 65, 'val_acc': 0.5909090909090909}
2025-09-10 12:10:02 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-10 12:10:02 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 110, 'val_loss': 74.88877868652344, 'val_avg_loss': 0.6808070789683949, 'val_seen': 110, 'val_correct': 65, 'val_acc': 0.5909090909090909}
2025-09-10 12:10:02 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #19', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 28, 'val_loss': 18.067535281181335, 'val_avg_loss': 0.6452691171850476, 'val_seen': 28, 'val_correct': 18, 'val_acc': 0.6428571428571429}}
2025-09-10 12:10:02 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #19', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 110, 'val_loss': 74.88877868652344, 'val_avg_loss': 0.6808070789683949, 'val_seen': 110, 'val_correct': 65, 'val_acc': 0.5909090909090909}}
2025-09-10 12:10:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:10:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:10:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:10:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:10:03 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.449982, avg_loss=0.786250, seen=40, correct=20, accuracy=0.500000
2025-09-10 12:10:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:10:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:10:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:10:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:10:06 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 31.449981689453125, 'test_avg_loss': 0.7862495422363281, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-10 12:10:06 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 110, 'val_loss': 74.88877868652344, 'val_avg_loss': 0.6808070789683949, 'val_seen': 110, 'val_correct': 65, 'val_acc': 0.5909090909090909}
2025-09-10 12:10:06 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 31.449981689453125, 'test_avg_loss': 0.7862495422363281, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-10 12:10:06 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #19', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 8.108741164207458, 'test_avg_loss': 0.8108741164207458, 'test_seen': 10, 'test_correct': 6, 'test_acc': 0.6}}
2025-09-10 12:10:06 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #19', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 31.449981689453125, 'test_avg_loss': 0.7862495422363281, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}}
2025-09-10 12:10:06 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 31.449981689453125, 'test_avg_loss': 0.7862495422363281, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}, metrics={'val_total': 110, 'val_loss': 74.88877868652344, 'val_avg_loss': 0.6808070789683949, 'val_seen': 110, 'val_correct': 65, 'val_acc': 0.5909090909090909, 'test_total': 40, 'test_loss': 31.449981689453125, 'test_avg_loss': 0.7862495422363281, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-10 12:10:06 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-10 12:10:06 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:10:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-09-10 12:10:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:10:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=63, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:10:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-09-10 12:10:09 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=126, loss_sum=86.682602, avg_loss=0.687957, seen=126, correct=69, accuracy=0.547619
2025-09-10 12:10:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:10:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:10:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:10:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:10:11 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 126, 'val_loss': 86.68260192871094, 'val_avg_loss': 0.6879571581643725, 'val_seen': 126, 'val_correct': 69, 'val_acc': 0.5476190476190477}
2025-09-10 12:10:11 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-10 12:10:11 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 126, 'val_loss': 86.68260192871094, 'val_avg_loss': 0.6879571581643725, 'val_seen': 126, 'val_correct': 69, 'val_acc': 0.5476190476190477}
2025-09-10 12:10:11 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #20', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 32, 'val_loss': 21.23532348871231, 'val_avg_loss': 0.6636038590222597, 'val_seen': 32, 'val_correct': 17, 'val_acc': 0.53125}}
2025-09-10 12:10:11 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #20', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 126, 'val_loss': 86.68260192871094, 'val_avg_loss': 0.6879571581643725, 'val_seen': 126, 'val_correct': 69, 'val_acc': 0.5476190476190477}}
2025-09-10 12:10:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:10:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:10:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:10:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:10:12 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.159351, avg_loss=0.628984, seen=40, correct=29, accuracy=0.725000
2025-09-10 12:10:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:10:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:10:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:10:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:10:14 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 25.159351348876953, 'test_avg_loss': 0.6289837837219239, 'test_seen': 40, 'test_correct': 29, 'test_acc': 0.725}
2025-09-10 12:10:14 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 126, 'val_loss': 86.68260192871094, 'val_avg_loss': 0.6879571581643725, 'val_seen': 126, 'val_correct': 69, 'val_acc': 0.5476190476190477}
2025-09-10 12:10:14 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 25.159351348876953, 'test_avg_loss': 0.6289837837219239, 'test_seen': 40, 'test_correct': 29, 'test_acc': 0.725}
2025-09-10 12:10:14 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #20', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.101680397987366, 'test_avg_loss': 0.6101680397987366, 'test_seen': 10, 'test_correct': 9, 'test_acc': 0.9}}
2025-09-10 12:10:14 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #20', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 25.159351348876953, 'test_avg_loss': 0.6289837837219239, 'test_seen': 40, 'test_correct': 29, 'test_acc': 0.725}}
2025-09-10 12:10:14 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 25.159351348876953, 'test_avg_loss': 0.6289837837219239, 'test_seen': 40, 'test_correct': 29, 'test_acc': 0.725}, metrics={'val_total': 126, 'val_loss': 86.68260192871094, 'val_avg_loss': 0.6879571581643725, 'val_seen': 126, 'val_correct': 69, 'val_acc': 0.5476190476190477, 'test_total': 40, 'test_loss': 25.159351348876953, 'test_avg_loss': 0.6289837837219239, 'test_seen': 40, 'test_correct': 29, 'test_acc': 0.725}
2025-09-10 12:10:14 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-10 12:10:14 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:10:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-09-10 12:10:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:10:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=77, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:10:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-09-10 12:10:19 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=153, loss_sum=101.798248, avg_loss=0.665348, seen=153, correct=94, accuracy=0.614379
2025-09-10 12:10:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:10:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:10:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:10:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:10:21 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 153, 'val_loss': 101.79824829101562, 'val_avg_loss': 0.6653480280458538, 'val_seen': 153, 'val_correct': 94, 'val_acc': 0.6143790849673203}
2025-09-10 12:10:21 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-10 12:10:21 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 153, 'val_loss': 101.79824829101562, 'val_avg_loss': 0.6653480280458538, 'val_seen': 153, 'val_correct': 94, 'val_acc': 0.6143790849673203}
2025-09-10 12:10:21 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #21', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 39, 'val_loss': 22.933708041906357, 'val_avg_loss': 0.5880437959463168, 'val_seen': 39, 'val_correct': 28, 'val_acc': 0.717948717948718}}
2025-09-10 12:10:21 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #21', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 153, 'val_loss': 101.79824829101562, 'val_avg_loss': 0.6653480280458538, 'val_seen': 153, 'val_correct': 94, 'val_acc': 0.6143790849673203}}
2025-09-10 12:10:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:10:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:10:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:10:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:10:22 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.286346, avg_loss=0.757159, seen=40, correct=18, accuracy=0.450000
2025-09-10 12:10:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:10:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:10:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:10:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:10:24 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 30.286346435546875, 'test_avg_loss': 0.7571586608886719, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-10 12:10:24 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 153, 'val_loss': 101.79824829101562, 'val_avg_loss': 0.6653480280458538, 'val_seen': 153, 'val_correct': 94, 'val_acc': 0.6143790849673203}
2025-09-10 12:10:24 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 30.286346435546875, 'test_avg_loss': 0.7571586608886719, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-10 12:10:24 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #21', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.462833762168884, 'test_avg_loss': 0.6462833762168885, 'test_seen': 10, 'test_correct': 6, 'test_acc': 0.6}}
2025-09-10 12:10:24 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #21', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 30.286346435546875, 'test_avg_loss': 0.7571586608886719, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}}
2025-09-10 12:10:24 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 30.286346435546875, 'test_avg_loss': 0.7571586608886719, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}, metrics={'val_total': 153, 'val_loss': 101.79824829101562, 'val_avg_loss': 0.6653480280458538, 'val_seen': 153, 'val_correct': 94, 'val_acc': 0.6143790849673203, 'test_total': 40, 'test_loss': 30.286346435546875, 'test_avg_loss': 0.7571586608886719, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-10 12:10:24 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-10 12:10:24 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:10:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-10 12:10:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:10:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:10:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-10 12:10:25 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.633342, avg_loss=0.693940, seen=11, correct=9, accuracy=0.818182
2025-09-10 12:10:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:10:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:10:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:10:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:10:27 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 11, 'val_loss': 7.6333417892456055, 'val_avg_loss': 0.6939401626586914, 'val_seen': 11, 'val_correct': 9, 'val_acc': 0.8181818181818182}
2025-09-10 12:10:27 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-10 12:10:27 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 11, 'val_loss': 7.6333417892456055, 'val_avg_loss': 0.6939401626586914, 'val_seen': 11, 'val_correct': 9, 'val_acc': 0.8181818181818182}
2025-09-10 12:10:27 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #22', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 3, 'val_loss': 1.8844288885593414, 'val_avg_loss': 0.6281429628531138, 'val_seen': 3, 'val_correct': 3, 'val_acc': 1.0}}
2025-09-10 12:10:27 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #22', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 11, 'val_loss': 7.6333417892456055, 'val_avg_loss': 0.6939401626586914, 'val_seen': 11, 'val_correct': 9, 'val_acc': 0.8181818181818182}}
2025-09-10 12:10:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:10:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:10:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:10:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:10:29 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.549261, avg_loss=0.738732, seen=40, correct=22, accuracy=0.550000
2025-09-10 12:10:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:10:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:10:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:10:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:10:31 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 29.54926109313965, 'test_avg_loss': 0.7387315273284912, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-10 12:10:31 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 11, 'val_loss': 7.6333417892456055, 'val_avg_loss': 0.6939401626586914, 'val_seen': 11, 'val_correct': 9, 'val_acc': 0.8181818181818182}
2025-09-10 12:10:31 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 29.54926109313965, 'test_avg_loss': 0.7387315273284912, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-10 12:10:31 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #22', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.812929153442383, 'test_avg_loss': 0.7812929153442383, 'test_seen': 10, 'test_correct': 5, 'test_acc': 0.5}}
2025-09-10 12:10:31 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #22', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 29.54926109313965, 'test_avg_loss': 0.7387315273284912, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}}
2025-09-10 12:10:31 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 29.54926109313965, 'test_avg_loss': 0.7387315273284912, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}, metrics={'val_total': 11, 'val_loss': 7.6333417892456055, 'val_avg_loss': 0.6939401626586914, 'val_seen': 11, 'val_correct': 9, 'val_acc': 0.8181818181818182, 'test_total': 40, 'test_loss': 29.54926109313965, 'test_avg_loss': 0.7387315273284912, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-10 12:10:31 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-10 12:10:31 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:10:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-09-10 12:10:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:10:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=15, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:10:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-10 12:10:33 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=30, loss_sum=18.069229, avg_loss=0.602308, seen=30, correct=20, accuracy=0.666667
2025-09-10 12:10:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:10:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:10:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:10:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:10:34 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 30, 'val_loss': 18.069229125976562, 'val_avg_loss': 0.6023076375325521, 'val_seen': 30, 'val_correct': 20, 'val_acc': 0.6666666666666666}
2025-09-10 12:10:34 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-10 12:10:34 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 30, 'val_loss': 18.069229125976562, 'val_avg_loss': 0.6023076375325521, 'val_seen': 30, 'val_correct': 20, 'val_acc': 0.6666666666666666}
2025-09-10 12:10:34 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #23', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 8, 'val_loss': 5.226491093635559, 'val_avg_loss': 0.6533113867044449, 'val_seen': 8, 'val_correct': 5, 'val_acc': 0.625}}
2025-09-10 12:10:34 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #23', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 30, 'val_loss': 18.069229125976562, 'val_avg_loss': 0.6023076375325521, 'val_seen': 30, 'val_correct': 20, 'val_acc': 0.6666666666666666}}
2025-09-10 12:10:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:10:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:10:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:10:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:10:37 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.220098, avg_loss=0.605502, seen=40, correct=24, accuracy=0.600000
2025-09-10 12:10:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:10:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:10:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:10:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:10:40 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 24.2200984954834, 'test_avg_loss': 0.6055024623870849, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-10 12:10:40 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 30, 'val_loss': 18.069229125976562, 'val_avg_loss': 0.6023076375325521, 'val_seen': 30, 'val_correct': 20, 'val_acc': 0.6666666666666666}
2025-09-10 12:10:40 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 24.2200984954834, 'test_avg_loss': 0.6055024623870849, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-10 12:10:40 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #23', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 5.977050304412842, 'test_avg_loss': 0.5977050304412842, 'test_seen': 10, 'test_correct': 7, 'test_acc': 0.7}}
2025-09-10 12:10:40 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #23', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 24.2200984954834, 'test_avg_loss': 0.6055024623870849, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}}
2025-09-10 12:10:40 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 24.2200984954834, 'test_avg_loss': 0.6055024623870849, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}, metrics={'val_total': 30, 'val_loss': 18.069229125976562, 'val_avg_loss': 0.6023076375325521, 'val_seen': 30, 'val_correct': 20, 'val_acc': 0.6666666666666666, 'test_total': 40, 'test_loss': 24.2200984954834, 'test_avg_loss': 0.6055024623870849, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-10 12:10:40 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-10 12:10:40 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:10:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 12:10:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:10:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:10:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 12:10:44 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=134.021912, avg_loss=0.670110, seen=200, correct=117, accuracy=0.585000
2025-09-10 12:10:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:10:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:10:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:10:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:10:46 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 200, 'val_loss': 134.02191162109375, 'val_avg_loss': 0.6701095581054688, 'val_seen': 200, 'val_correct': 117, 'val_acc': 0.585}
2025-09-10 12:10:46 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-10 12:10:46 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 200, 'val_loss': 134.02191162109375, 'val_avg_loss': 0.6701095581054688, 'val_seen': 200, 'val_correct': 117, 'val_acc': 0.585}
2025-09-10 12:10:46 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #24', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 50, 'val_loss': 30.688389778137207, 'val_avg_loss': 0.6137677955627442, 'val_seen': 50, 'val_correct': 33, 'val_acc': 0.66}}
2025-09-10 12:10:46 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #24', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 200, 'val_loss': 134.02191162109375, 'val_avg_loss': 0.6701095581054688, 'val_seen': 200, 'val_correct': 117, 'val_acc': 0.585}}
2025-09-10 12:10:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:10:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:10:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:10:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:10:47 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.223301, avg_loss=0.755583, seen=40, correct=21, accuracy=0.525000
2025-09-10 12:10:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:10:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:10:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:10:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:10:49 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 30.22330093383789, 'test_avg_loss': 0.7555825233459472, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-10 12:10:49 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 200, 'val_loss': 134.02191162109375, 'val_avg_loss': 0.6701095581054688, 'val_seen': 200, 'val_correct': 117, 'val_acc': 0.585}
2025-09-10 12:10:49 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 30.22330093383789, 'test_avg_loss': 0.7555825233459472, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-10 12:10:49 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #24', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.3984493017196655, 'test_avg_loss': 0.6398449301719665, 'test_seen': 10, 'test_correct': 8, 'test_acc': 0.8}}
2025-09-10 12:10:49 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #24', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 30.22330093383789, 'test_avg_loss': 0.7555825233459472, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}}
2025-09-10 12:10:49 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 30.22330093383789, 'test_avg_loss': 0.7555825233459472, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}, metrics={'val_total': 200, 'val_loss': 134.02191162109375, 'val_avg_loss': 0.6701095581054688, 'val_seen': 200, 'val_correct': 117, 'val_acc': 0.585, 'test_total': 40, 'test_loss': 30.22330093383789, 'test_avg_loss': 0.7555825233459472, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-10 12:10:49 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-10 12:10:49 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:10:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 12:10:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:10:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:10:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 12:10:54 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=142.585159, avg_loss=0.712926, seen=200, correct=107, accuracy=0.535000
2025-09-10 12:10:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:10:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:10:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:10:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:10:56 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 200, 'val_loss': 142.5851593017578, 'val_avg_loss': 0.7129257965087891, 'val_seen': 200, 'val_correct': 107, 'val_acc': 0.535}
2025-09-10 12:10:56 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-10 12:10:56 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 200, 'val_loss': 142.5851593017578, 'val_avg_loss': 0.7129257965087891, 'val_seen': 200, 'val_correct': 107, 'val_acc': 0.535}
2025-09-10 12:10:56 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #25', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 50, 'val_loss': 36.02649337053299, 'val_avg_loss': 0.7205298674106598, 'val_seen': 50, 'val_correct': 23, 'val_acc': 0.46}}
2025-09-10 12:10:56 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #25', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 200, 'val_loss': 142.5851593017578, 'val_avg_loss': 0.7129257965087891, 'val_seen': 200, 'val_correct': 107, 'val_acc': 0.535}}
2025-09-10 12:10:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:10:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:10:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:10:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:10:57 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=32.601845, avg_loss=0.815046, seen=40, correct=21, accuracy=0.525000
2025-09-10 12:10:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:10:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:10:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:10:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:10:59 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 32.601844787597656, 'test_avg_loss': 0.8150461196899415, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-10 12:10:59 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 200, 'val_loss': 142.5851593017578, 'val_avg_loss': 0.7129257965087891, 'val_seen': 200, 'val_correct': 107, 'val_acc': 0.535}
2025-09-10 12:10:59 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 32.601844787597656, 'test_avg_loss': 0.8150461196899415, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-10 12:10:59 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #25', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.8514440059661865, 'test_avg_loss': 0.7851444005966186, 'test_seen': 10, 'test_correct': 5, 'test_acc': 0.5}}
2025-09-10 12:10:59 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #25', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 32.601844787597656, 'test_avg_loss': 0.8150461196899415, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}}
2025-09-10 12:10:59 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 32.601844787597656, 'test_avg_loss': 0.8150461196899415, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}, metrics={'val_total': 200, 'val_loss': 142.5851593017578, 'val_avg_loss': 0.7129257965087891, 'val_seen': 200, 'val_correct': 107, 'val_acc': 0.535, 'test_total': 40, 'test_loss': 32.601844787597656, 'test_avg_loss': 0.8150461196899415, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-10 12:10:59 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-10 12:10:59 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:11:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-09-10 12:11:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:11:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=81, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:11:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-09-10 12:11:03 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=161, loss_sum=100.699905, avg_loss=0.625465, seen=161, correct=107, accuracy=0.664596
2025-09-10 12:11:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:11:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:11:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:11:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:11:05 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 161, 'val_loss': 100.69990539550781, 'val_avg_loss': 0.6254652509037752, 'val_seen': 161, 'val_correct': 107, 'val_acc': 0.6645962732919255}
2025-09-10 12:11:05 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-10 12:11:05 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 161, 'val_loss': 100.69990539550781, 'val_avg_loss': 0.6254652509037752, 'val_seen': 161, 'val_correct': 107, 'val_acc': 0.6645962732919255}
2025-09-10 12:11:05 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #26', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 41, 'val_loss': 24.72766125202179, 'val_avg_loss': 0.6031136890737022, 'val_seen': 41, 'val_correct': 26, 'val_acc': 0.6341463414634146}}
2025-09-10 12:11:05 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #26', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 161, 'val_loss': 100.69990539550781, 'val_avg_loss': 0.6254652509037752, 'val_seen': 161, 'val_correct': 107, 'val_acc': 0.6645962732919255}}
2025-09-10 12:11:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:11:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:11:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:11:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:11:07 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.701393, avg_loss=0.617535, seen=40, correct=27, accuracy=0.675000
2025-09-10 12:11:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:11:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:11:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:11:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:11:08 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 24.701393127441406, 'test_avg_loss': 0.6175348281860351, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-10 12:11:08 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 161, 'val_loss': 100.69990539550781, 'val_avg_loss': 0.6254652509037752, 'val_seen': 161, 'val_correct': 107, 'val_acc': 0.6645962732919255}
2025-09-10 12:11:08 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 24.701393127441406, 'test_avg_loss': 0.6175348281860351, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-10 12:11:08 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #26', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.1247735023498535, 'test_avg_loss': 0.6124773502349854, 'test_seen': 10, 'test_correct': 7, 'test_acc': 0.7}}
2025-09-10 12:11:08 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #26', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 24.701393127441406, 'test_avg_loss': 0.6175348281860351, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}}
2025-09-10 12:11:08 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 24.701393127441406, 'test_avg_loss': 0.6175348281860351, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}, metrics={'val_total': 161, 'val_loss': 100.69990539550781, 'val_avg_loss': 0.6254652509037752, 'val_seen': 161, 'val_correct': 107, 'val_acc': 0.6645962732919255, 'test_total': 40, 'test_loss': 24.701393127441406, 'test_avg_loss': 0.6175348281860351, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-10 12:11:08 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-10 12:11:08 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:11:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-09-10 12:11:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:11:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=62, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:11:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-09-10 12:11:12 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=123, loss_sum=81.165985, avg_loss=0.659886, seen=123, correct=80, accuracy=0.650407
2025-09-10 12:11:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:11:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:11:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:11:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:11:14 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 123, 'val_loss': 81.16598510742188, 'val_avg_loss': 0.6598860577839177, 'val_seen': 123, 'val_correct': 80, 'val_acc': 0.6504065040650406}
2025-09-10 12:11:14 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-10 12:11:14 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 123, 'val_loss': 81.16598510742188, 'val_avg_loss': 0.6598860577839177, 'val_seen': 123, 'val_correct': 80, 'val_acc': 0.6504065040650406}
2025-09-10 12:11:14 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #27', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 31, 'val_loss': 19.609616816043854, 'val_avg_loss': 0.6325682843885114, 'val_seen': 31, 'val_correct': 23, 'val_acc': 0.7419354838709677}}
2025-09-10 12:11:14 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #27', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 123, 'val_loss': 81.16598510742188, 'val_avg_loss': 0.6598860577839177, 'val_seen': 123, 'val_correct': 80, 'val_acc': 0.6504065040650406}}
2025-09-10 12:11:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:11:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:11:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:11:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:11:16 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.890369, avg_loss=0.672259, seen=40, correct=23, accuracy=0.575000
2025-09-10 12:11:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:11:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:11:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:11:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:11:18 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 26.890369415283203, 'test_avg_loss': 0.6722592353820801, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-10 12:11:18 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 123, 'val_loss': 81.16598510742188, 'val_avg_loss': 0.6598860577839177, 'val_seen': 123, 'val_correct': 80, 'val_acc': 0.6504065040650406}
2025-09-10 12:11:18 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 26.890369415283203, 'test_avg_loss': 0.6722592353820801, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-10 12:11:18 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #27', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 5.914837598800659, 'test_avg_loss': 0.591483759880066, 'test_seen': 10, 'test_correct': 7, 'test_acc': 0.7}}
2025-09-10 12:11:18 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #27', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 26.890369415283203, 'test_avg_loss': 0.6722592353820801, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}}
2025-09-10 12:11:18 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 26.890369415283203, 'test_avg_loss': 0.6722592353820801, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}, metrics={'val_total': 123, 'val_loss': 81.16598510742188, 'val_avg_loss': 0.6598860577839177, 'val_seen': 123, 'val_correct': 80, 'val_acc': 0.6504065040650406, 'test_total': 40, 'test_loss': 26.890369415283203, 'test_avg_loss': 0.6722592353820801, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-10 12:11:18 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-10 12:11:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:11:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-09-10 12:11:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:11:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=38, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:11:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-10 12:11:22 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=75, loss_sum=50.737251, avg_loss=0.676497, seen=75, correct=38, accuracy=0.506667
2025-09-10 12:11:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:11:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:11:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:11:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:11:24 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 75, 'val_loss': 50.73725128173828, 'val_avg_loss': 0.6764966837565104, 'val_seen': 75, 'val_correct': 38, 'val_acc': 0.5066666666666667}
2025-09-10 12:11:24 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-10 12:11:24 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 75, 'val_loss': 50.73725128173828, 'val_avg_loss': 0.6764966837565104, 'val_seen': 75, 'val_correct': 38, 'val_acc': 0.5066666666666667}
2025-09-10 12:11:24 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #28', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 19, 'val_loss': 14.243391036987305, 'val_avg_loss': 0.7496521598414371, 'val_seen': 19, 'val_correct': 8, 'val_acc': 0.42105263157894735}}
2025-09-10 12:11:24 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #28', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 75, 'val_loss': 50.73725128173828, 'val_avg_loss': 0.6764966837565104, 'val_seen': 75, 'val_correct': 38, 'val_acc': 0.5066666666666667}}
2025-09-10 12:11:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:11:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:11:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:11:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:11:25 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.241827, avg_loss=0.656046, seen=40, correct=26, accuracy=0.650000
2025-09-10 12:11:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:11:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:11:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:11:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:11:27 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 26.2418270111084, 'test_avg_loss': 0.65604567527771, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-10 12:11:27 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 75, 'val_loss': 50.73725128173828, 'val_avg_loss': 0.6764966837565104, 'val_seen': 75, 'val_correct': 38, 'val_acc': 0.5066666666666667}
2025-09-10 12:11:27 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 26.2418270111084, 'test_avg_loss': 0.65604567527771, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-10 12:11:27 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #28', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 5.938172698020935, 'test_avg_loss': 0.5938172698020935, 'test_seen': 10, 'test_correct': 8, 'test_acc': 0.8}}
2025-09-10 12:11:27 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #28', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 26.2418270111084, 'test_avg_loss': 0.65604567527771, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}}
2025-09-10 12:11:27 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 26.2418270111084, 'test_avg_loss': 0.65604567527771, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}, metrics={'val_total': 75, 'val_loss': 50.73725128173828, 'val_avg_loss': 0.6764966837565104, 'val_seen': 75, 'val_correct': 38, 'val_acc': 0.5066666666666667, 'test_total': 40, 'test_loss': 26.2418270111084, 'test_avg_loss': 0.65604567527771, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-10 12:11:27 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-10 12:11:27 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:11:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 12:11:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:11:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:11:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 12:11:31 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.849258, avg_loss=0.679246, seen=200, correct=107, accuracy=0.535000
2025-09-10 12:11:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:11:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:11:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:11:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:11:34 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 200, 'val_loss': 135.84925842285156, 'val_avg_loss': 0.6792462921142578, 'val_seen': 200, 'val_correct': 107, 'val_acc': 0.535}
2025-09-10 12:11:34 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-10 12:11:34 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 200, 'val_loss': 135.84925842285156, 'val_avg_loss': 0.6792462921142578, 'val_seen': 200, 'val_correct': 107, 'val_acc': 0.535}
2025-09-10 12:11:34 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #29', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 50, 'val_loss': 34.172968566417694, 'val_avg_loss': 0.6834593713283539, 'val_seen': 50, 'val_correct': 27, 'val_acc': 0.54}}
2025-09-10 12:11:34 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #29', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 200, 'val_loss': 135.84925842285156, 'val_avg_loss': 0.6792462921142578, 'val_seen': 200, 'val_correct': 107, 'val_acc': 0.535}}
2025-09-10 12:11:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:11:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:11:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:11:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:11:36 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.110645, avg_loss=0.602766, seen=40, correct=27, accuracy=0.675000
2025-09-10 12:11:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:11:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:11:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:11:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:11:38 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 24.110645294189453, 'test_avg_loss': 0.6027661323547363, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-10 12:11:38 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 200, 'val_loss': 135.84925842285156, 'val_avg_loss': 0.6792462921142578, 'val_seen': 200, 'val_correct': 107, 'val_acc': 0.535}
2025-09-10 12:11:38 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 24.110645294189453, 'test_avg_loss': 0.6027661323547363, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-10 12:11:38 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #29', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.763907790184021, 'test_avg_loss': 0.7763907790184021, 'test_seen': 10, 'test_correct': 6, 'test_acc': 0.6}}
2025-09-10 12:11:38 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #29', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 24.110645294189453, 'test_avg_loss': 0.6027661323547363, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}}
2025-09-10 12:11:38 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 24.110645294189453, 'test_avg_loss': 0.6027661323547363, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}, metrics={'val_total': 200, 'val_loss': 135.84925842285156, 'val_avg_loss': 0.6792462921142578, 'val_seen': 200, 'val_correct': 107, 'val_acc': 0.535, 'test_total': 40, 'test_loss': 24.110645294189453, 'test_avg_loss': 0.6027661323547363, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-10 12:11:38 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-10 12:11:38 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:11:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-09-10 12:11:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:11:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=85, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:11:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-09-10 12:11:42 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=170, loss_sum=112.575615, avg_loss=0.662209, seen=170, correct=103, accuracy=0.605882
2025-09-10 12:11:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:11:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:11:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:11:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:11:45 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 170, 'val_loss': 112.57561492919922, 'val_avg_loss': 0.6622094995835248, 'val_seen': 170, 'val_correct': 103, 'val_acc': 0.6058823529411764}
2025-09-10 12:11:45 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-10 12:11:45 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 170, 'val_loss': 112.57561492919922, 'val_avg_loss': 0.6622094995835248, 'val_seen': 170, 'val_correct': 103, 'val_acc': 0.6058823529411764}
2025-09-10 12:11:45 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #30', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 43, 'val_loss': 29.136039078235626, 'val_avg_loss': 0.6775823041450145, 'val_seen': 43, 'val_correct': 27, 'val_acc': 0.627906976744186}}
2025-09-10 12:11:45 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #30', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 170, 'val_loss': 112.57561492919922, 'val_avg_loss': 0.6622094995835248, 'val_seen': 170, 'val_correct': 103, 'val_acc': 0.6058823529411764}}
2025-09-10 12:11:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:11:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:11:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:11:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:11:47 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.904398, avg_loss=0.647610, seen=40, correct=24, accuracy=0.600000
2025-09-10 12:11:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:11:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:11:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:11:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:11:49 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 25.90439796447754, 'test_avg_loss': 0.6476099491119385, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-10 12:11:49 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 170, 'val_loss': 112.57561492919922, 'val_avg_loss': 0.6622094995835248, 'val_seen': 170, 'val_correct': 103, 'val_acc': 0.6058823529411764}
2025-09-10 12:11:49 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 25.90439796447754, 'test_avg_loss': 0.6476099491119385, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-10 12:11:49 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #30', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.456823706626892, 'test_avg_loss': 0.7456823706626892, 'test_seen': 10, 'test_correct': 4, 'test_acc': 0.4}}
2025-09-10 12:11:49 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #30', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 25.90439796447754, 'test_avg_loss': 0.6476099491119385, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}}
2025-09-10 12:11:49 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 25.90439796447754, 'test_avg_loss': 0.6476099491119385, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}, metrics={'val_total': 170, 'val_loss': 112.57561492919922, 'val_avg_loss': 0.6622094995835248, 'val_seen': 170, 'val_correct': 103, 'val_acc': 0.6058823529411764, 'test_total': 40, 'test_loss': 25.90439796447754, 'test_avg_loss': 0.6476099491119385, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-10 12:11:49 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-10 12:11:49 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:11:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-09-10 12:11:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:11:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=97, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:11:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-09-10 12:11:53 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=193, loss_sum=133.792099, avg_loss=0.693223, seen=193, correct=110, accuracy=0.569948
2025-09-10 12:11:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:11:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:11:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:11:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:11:56 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 193, 'val_loss': 133.79209899902344, 'val_avg_loss': 0.6932233108757692, 'val_seen': 193, 'val_correct': 110, 'val_acc': 0.5699481865284974}
2025-09-10 12:11:56 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-10 12:11:56 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 193, 'val_loss': 133.79209899902344, 'val_avg_loss': 0.6932233108757692, 'val_seen': 193, 'val_correct': 110, 'val_acc': 0.5699481865284974}
2025-09-10 12:11:56 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #31', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 49, 'val_loss': 33.63331472873688, 'val_avg_loss': 0.6863941781374873, 'val_seen': 49, 'val_correct': 31, 'val_acc': 0.6326530612244898}}
2025-09-10 12:11:56 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #31', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 193, 'val_loss': 133.79209899902344, 'val_avg_loss': 0.6932233108757692, 'val_seen': 193, 'val_correct': 110, 'val_acc': 0.5699481865284974}}
2025-09-10 12:11:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:11:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:11:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:11:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:11:58 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.187391, avg_loss=0.654685, seen=40, correct=23, accuracy=0.575000
2025-09-10 12:11:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:11:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:12:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:12:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:12:00 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 26.18739128112793, 'test_avg_loss': 0.6546847820281982, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-10 12:12:00 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 193, 'val_loss': 133.79209899902344, 'val_avg_loss': 0.6932233108757692, 'val_seen': 193, 'val_correct': 110, 'val_acc': 0.5699481865284974}
2025-09-10 12:12:00 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 26.18739128112793, 'test_avg_loss': 0.6546847820281982, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-10 12:12:00 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #31', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.544812798500061, 'test_avg_loss': 0.7544812798500061, 'test_seen': 10, 'test_correct': 6, 'test_acc': 0.6}}
2025-09-10 12:12:00 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #31', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 26.18739128112793, 'test_avg_loss': 0.6546847820281982, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}}
2025-09-10 12:12:00 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 26.18739128112793, 'test_avg_loss': 0.6546847820281982, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}, metrics={'val_total': 193, 'val_loss': 133.79209899902344, 'val_avg_loss': 0.6932233108757692, 'val_seen': 193, 'val_correct': 110, 'val_acc': 0.5699481865284974, 'test_total': 40, 'test_loss': 26.18739128112793, 'test_avg_loss': 0.6546847820281982, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-10 12:12:00 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-10 12:12:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:12:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-09-10 12:12:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:12:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=56, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:12:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-10 12:12:05 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=112, loss_sum=66.521698, avg_loss=0.593944, seen=112, correct=76, accuracy=0.678571
2025-09-10 12:12:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:12:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:12:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:12:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:12:07 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 112, 'val_loss': 66.52169799804688, 'val_avg_loss': 0.5939437321254185, 'val_seen': 112, 'val_correct': 76, 'val_acc': 0.6785714285714286}
2025-09-10 12:12:07 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-10 12:12:07 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 112, 'val_loss': 66.52169799804688, 'val_avg_loss': 0.5939437321254185, 'val_seen': 112, 'val_correct': 76, 'val_acc': 0.6785714285714286}
2025-09-10 12:12:07 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #32', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 28, 'val_loss': 16.541321456432343, 'val_avg_loss': 0.5907614805868694, 'val_seen': 28, 'val_correct': 21, 'val_acc': 0.75}}
2025-09-10 12:12:07 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #32', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 112, 'val_loss': 66.52169799804688, 'val_avg_loss': 0.5939437321254185, 'val_seen': 112, 'val_correct': 76, 'val_acc': 0.6785714285714286}}
2025-09-10 12:12:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:12:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:12:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:12:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:12:08 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.103971, avg_loss=0.702599, seen=40, correct=21, accuracy=0.525000
2025-09-10 12:12:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:12:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:12:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:12:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:12:11 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 28.103971481323242, 'test_avg_loss': 0.702599287033081, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-10 12:12:11 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 112, 'val_loss': 66.52169799804688, 'val_avg_loss': 0.5939437321254185, 'val_seen': 112, 'val_correct': 76, 'val_acc': 0.6785714285714286}
2025-09-10 12:12:11 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 28.103971481323242, 'test_avg_loss': 0.702599287033081, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-10 12:12:11 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #32', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.365666776895523, 'test_avg_loss': 0.7365666776895523, 'test_seen': 10, 'test_correct': 3, 'test_acc': 0.3}}
2025-09-10 12:12:11 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #32', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 28.103971481323242, 'test_avg_loss': 0.702599287033081, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}}
2025-09-10 12:12:11 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 28.103971481323242, 'test_avg_loss': 0.702599287033081, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}, metrics={'val_total': 112, 'val_loss': 66.52169799804688, 'val_avg_loss': 0.5939437321254185, 'val_seen': 112, 'val_correct': 76, 'val_acc': 0.6785714285714286, 'test_total': 40, 'test_loss': 28.103971481323242, 'test_avg_loss': 0.702599287033081, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-10 12:12:11 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-10 12:12:11 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:12:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-09-10 12:12:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:12:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=37, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:12:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-10 12:12:13 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=74, loss_sum=52.216740, avg_loss=0.705632, seen=74, correct=44, accuracy=0.594595
2025-09-10 12:12:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:12:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:12:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:12:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:12:15 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 74, 'val_loss': 52.216739654541016, 'val_avg_loss': 0.7056316169532569, 'val_seen': 74, 'val_correct': 44, 'val_acc': 0.5945945945945946}
2025-09-10 12:12:15 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-10 12:12:15 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 74, 'val_loss': 52.216739654541016, 'val_avg_loss': 0.7056316169532569, 'val_seen': 74, 'val_correct': 44, 'val_acc': 0.5945945945945946}
2025-09-10 12:12:15 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #33', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 19, 'val_loss': 14.05133318901062, 'val_avg_loss': 0.7395438520531905, 'val_seen': 19, 'val_correct': 10, 'val_acc': 0.5263157894736842}}
2025-09-10 12:12:15 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #33', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 74, 'val_loss': 52.216739654541016, 'val_avg_loss': 0.7056316169532569, 'val_seen': 74, 'val_correct': 44, 'val_acc': 0.5945945945945946}}
2025-09-10 12:12:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:12:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:12:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:12:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:12:17 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.734932, avg_loss=0.668373, seen=40, correct=21, accuracy=0.525000
2025-09-10 12:12:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:12:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:12:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:12:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:12:20 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 26.73493194580078, 'test_avg_loss': 0.6683732986450195, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-10 12:12:20 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 74, 'val_loss': 52.216739654541016, 'val_avg_loss': 0.7056316169532569, 'val_seen': 74, 'val_correct': 44, 'val_acc': 0.5945945945945946}
2025-09-10 12:12:20 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 26.73493194580078, 'test_avg_loss': 0.6683732986450195, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-10 12:12:20 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #33', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.582665503025055, 'test_avg_loss': 0.6582665503025055, 'test_seen': 10, 'test_correct': 6, 'test_acc': 0.6}}
2025-09-10 12:12:20 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #33', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 26.73493194580078, 'test_avg_loss': 0.6683732986450195, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}}
2025-09-10 12:12:20 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 26.73493194580078, 'test_avg_loss': 0.6683732986450195, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}, metrics={'val_total': 74, 'val_loss': 52.216739654541016, 'val_avg_loss': 0.7056316169532569, 'val_seen': 74, 'val_correct': 44, 'val_acc': 0.5945945945945946, 'test_total': 40, 'test_loss': 26.73493194580078, 'test_avg_loss': 0.6683732986450195, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-10 12:12:20 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-10 12:12:20 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:12:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 12:12:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:12:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:12:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 12:12:24 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=126.042274, avg_loss=0.630211, seen=200, correct=134, accuracy=0.670000
2025-09-10 12:12:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:12:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:12:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:12:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:12:27 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 200, 'val_loss': 126.04227447509766, 'val_avg_loss': 0.6302113723754883, 'val_seen': 200, 'val_correct': 134, 'val_acc': 0.67}
2025-09-10 12:12:27 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-10 12:12:27 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 200, 'val_loss': 126.04227447509766, 'val_avg_loss': 0.6302113723754883, 'val_seen': 200, 'val_correct': 134, 'val_acc': 0.67}
2025-09-10 12:12:27 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #34', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 50, 'val_loss': 30.01540595293045, 'val_avg_loss': 0.600308119058609, 'val_seen': 50, 'val_correct': 37, 'val_acc': 0.74}}
2025-09-10 12:12:27 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #34', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 200, 'val_loss': 126.04227447509766, 'val_avg_loss': 0.6302113723754883, 'val_seen': 200, 'val_correct': 134, 'val_acc': 0.67}}
2025-09-10 12:12:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:12:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:12:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:12:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:12:29 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.428854, avg_loss=0.585721, seen=40, correct=29, accuracy=0.725000
2025-09-10 12:12:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:12:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:12:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:12:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:12:31 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 23.42885398864746, 'test_avg_loss': 0.5857213497161865, 'test_seen': 40, 'test_correct': 29, 'test_acc': 0.725}
2025-09-10 12:12:31 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 200, 'val_loss': 126.04227447509766, 'val_avg_loss': 0.6302113723754883, 'val_seen': 200, 'val_correct': 134, 'val_acc': 0.67}
2025-09-10 12:12:31 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 23.42885398864746, 'test_avg_loss': 0.5857213497161865, 'test_seen': 40, 'test_correct': 29, 'test_acc': 0.725}
2025-09-10 12:12:31 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #34', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.047233462333679, 'test_avg_loss': 0.6047233462333679, 'test_seen': 10, 'test_correct': 8, 'test_acc': 0.8}}
2025-09-10 12:12:31 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #34', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 23.42885398864746, 'test_avg_loss': 0.5857213497161865, 'test_seen': 40, 'test_correct': 29, 'test_acc': 0.725}}
2025-09-10 12:12:31 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 23.42885398864746, 'test_avg_loss': 0.5857213497161865, 'test_seen': 40, 'test_correct': 29, 'test_acc': 0.725}, metrics={'val_total': 200, 'val_loss': 126.04227447509766, 'val_avg_loss': 0.6302113723754883, 'val_seen': 200, 'val_correct': 134, 'val_acc': 0.67, 'test_total': 40, 'test_loss': 23.42885398864746, 'test_avg_loss': 0.5857213497161865, 'test_seen': 40, 'test_correct': 29, 'test_acc': 0.725}
2025-09-10 12:12:31 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-10 12:12:31 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:12:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 12:12:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:12:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:12:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 12:12:35 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=128.196396, avg_loss=0.640982, seen=200, correct=124, accuracy=0.620000
2025-09-10 12:12:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:12:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:12:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:12:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:12:38 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 200, 'val_loss': 128.19639587402344, 'val_avg_loss': 0.6409819793701171, 'val_seen': 200, 'val_correct': 124, 'val_acc': 0.62}
2025-09-10 12:12:38 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-10 12:12:38 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 200, 'val_loss': 128.19639587402344, 'val_avg_loss': 0.6409819793701171, 'val_seen': 200, 'val_correct': 124, 'val_acc': 0.62}
2025-09-10 12:12:38 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #35', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 50, 'val_loss': 33.70693612098694, 'val_avg_loss': 0.6741387224197388, 'val_seen': 50, 'val_correct': 31, 'val_acc': 0.62}}
2025-09-10 12:12:38 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #35', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 200, 'val_loss': 128.19639587402344, 'val_avg_loss': 0.6409819793701171, 'val_seen': 200, 'val_correct': 124, 'val_acc': 0.62}}
2025-09-10 12:12:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:12:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:12:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:12:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:12:40 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.239861, avg_loss=0.655997, seen=40, correct=28, accuracy=0.700000
2025-09-10 12:12:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:12:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:12:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:12:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:12:41 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 26.23986053466797, 'test_avg_loss': 0.6559965133666992, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}
2025-09-10 12:12:41 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 200, 'val_loss': 128.19639587402344, 'val_avg_loss': 0.6409819793701171, 'val_seen': 200, 'val_correct': 124, 'val_acc': 0.62}
2025-09-10 12:12:41 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 26.23986053466797, 'test_avg_loss': 0.6559965133666992, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}
2025-09-10 12:12:41 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #35', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 5.044857859611511, 'test_avg_loss': 0.5044857859611511, 'test_seen': 10, 'test_correct': 9, 'test_acc': 0.9}}
2025-09-10 12:12:41 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #35', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 26.23986053466797, 'test_avg_loss': 0.6559965133666992, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}}
2025-09-10 12:12:41 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 26.23986053466797, 'test_avg_loss': 0.6559965133666992, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}, metrics={'val_total': 200, 'val_loss': 128.19639587402344, 'val_avg_loss': 0.6409819793701171, 'val_seen': 200, 'val_correct': 124, 'val_acc': 0.62, 'test_total': 40, 'test_loss': 26.23986053466797, 'test_avg_loss': 0.6559965133666992, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}
2025-09-10 12:12:41 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-10 12:12:41 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:12:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-09-10 12:12:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:12:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=27, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:12:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-09-10 12:12:44 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=54, loss_sum=36.923828, avg_loss=0.683775, seen=54, correct=30, accuracy=0.555556
2025-09-10 12:12:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:12:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:12:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:12:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:12:47 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 54, 'val_loss': 36.923828125, 'val_avg_loss': 0.6837745949074074, 'val_seen': 54, 'val_correct': 30, 'val_acc': 0.5555555555555556}
2025-09-10 12:12:47 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-10 12:12:47 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 54, 'val_loss': 36.923828125, 'val_avg_loss': 0.6837745949074074, 'val_seen': 54, 'val_correct': 30, 'val_acc': 0.5555555555555556}
2025-09-10 12:12:47 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #36', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 14, 'val_loss': 9.471544742584229, 'val_avg_loss': 0.6765389101845878, 'val_seen': 14, 'val_correct': 6, 'val_acc': 0.42857142857142855}}
2025-09-10 12:12:47 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #36', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 54, 'val_loss': 36.923828125, 'val_avg_loss': 0.6837745949074074, 'val_seen': 54, 'val_correct': 30, 'val_acc': 0.5555555555555556}}
2025-09-10 12:12:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:12:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:12:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:12:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:12:49 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.266973, avg_loss=0.606674, seen=40, correct=28, accuracy=0.700000
2025-09-10 12:12:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:12:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:12:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:12:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:12:52 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 24.2669734954834, 'test_avg_loss': 0.606674337387085, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}
2025-09-10 12:12:52 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 54, 'val_loss': 36.923828125, 'val_avg_loss': 0.6837745949074074, 'val_seen': 54, 'val_correct': 30, 'val_acc': 0.5555555555555556}
2025-09-10 12:12:52 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 24.2669734954834, 'test_avg_loss': 0.606674337387085, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}
2025-09-10 12:12:52 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #36', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.3167537450790405, 'test_avg_loss': 0.631675374507904, 'test_seen': 10, 'test_correct': 6, 'test_acc': 0.6}}
2025-09-10 12:12:52 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #36', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 24.2669734954834, 'test_avg_loss': 0.606674337387085, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}}
2025-09-10 12:12:52 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 24.2669734954834, 'test_avg_loss': 0.606674337387085, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}, metrics={'val_total': 54, 'val_loss': 36.923828125, 'val_avg_loss': 0.6837745949074074, 'val_seen': 54, 'val_correct': 30, 'val_acc': 0.5555555555555556, 'test_total': 40, 'test_loss': 24.2669734954834, 'test_avg_loss': 0.606674337387085, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}
2025-09-10 12:12:52 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-10 12:12:52 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:12:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 12:12:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:12:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:12:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 12:12:56 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=137.101715, avg_loss=0.685509, seen=200, correct=110, accuracy=0.550000
2025-09-10 12:12:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:12:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:12:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:12:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:12:58 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 200, 'val_loss': 137.10171508789062, 'val_avg_loss': 0.6855085754394531, 'val_seen': 200, 'val_correct': 110, 'val_acc': 0.55}
2025-09-10 12:12:58 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-10 12:12:58 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 200, 'val_loss': 137.10171508789062, 'val_avg_loss': 0.6855085754394531, 'val_seen': 200, 'val_correct': 110, 'val_acc': 0.55}
2025-09-10 12:12:58 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #37', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 50, 'val_loss': 36.11500358581543, 'val_avg_loss': 0.7223000717163086, 'val_seen': 50, 'val_correct': 27, 'val_acc': 0.54}}
2025-09-10 12:12:58 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #37', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 200, 'val_loss': 137.10171508789062, 'val_avg_loss': 0.6855085754394531, 'val_seen': 200, 'val_correct': 110, 'val_acc': 0.55}}
2025-09-10 12:12:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:12:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:12:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:12:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:12:59 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.103912, avg_loss=0.652598, seen=40, correct=25, accuracy=0.625000
2025-09-10 12:12:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:12:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:13:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:13:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:13:02 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 26.103912353515625, 'test_avg_loss': 0.6525978088378906, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-10 12:13:02 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 200, 'val_loss': 137.10171508789062, 'val_avg_loss': 0.6855085754394531, 'val_seen': 200, 'val_correct': 110, 'val_acc': 0.55}
2025-09-10 12:13:02 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 26.103912353515625, 'test_avg_loss': 0.6525978088378906, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-10 12:13:02 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #37', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 5.430750370025635, 'test_avg_loss': 0.5430750370025634, 'test_seen': 10, 'test_correct': 8, 'test_acc': 0.8}}
2025-09-10 12:13:02 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #37', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 26.103912353515625, 'test_avg_loss': 0.6525978088378906, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}}
2025-09-10 12:13:02 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 26.103912353515625, 'test_avg_loss': 0.6525978088378906, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}, metrics={'val_total': 200, 'val_loss': 137.10171508789062, 'val_avg_loss': 0.6855085754394531, 'val_seen': 200, 'val_correct': 110, 'val_acc': 0.55, 'test_total': 40, 'test_loss': 26.103912353515625, 'test_avg_loss': 0.6525978088378906, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-10 12:13:02 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-10 12:13:02 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:13:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 12:13:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:13:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:13:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 12:13:06 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=134.490265, avg_loss=0.672451, seen=200, correct=123, accuracy=0.615000
2025-09-10 12:13:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:13:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:13:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:13:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:13:08 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 200, 'val_loss': 134.49026489257812, 'val_avg_loss': 0.6724513244628906, 'val_seen': 200, 'val_correct': 123, 'val_acc': 0.615}
2025-09-10 12:13:08 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-10 12:13:08 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 200, 'val_loss': 134.49026489257812, 'val_avg_loss': 0.6724513244628906, 'val_seen': 200, 'val_correct': 123, 'val_acc': 0.615}
2025-09-10 12:13:08 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #38', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 50, 'val_loss': 34.132145285606384, 'val_avg_loss': 0.6826429057121277, 'val_seen': 50, 'val_correct': 33, 'val_acc': 0.66}}
2025-09-10 12:13:08 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #38', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 200, 'val_loss': 134.49026489257812, 'val_avg_loss': 0.6724513244628906, 'val_seen': 200, 'val_correct': 123, 'val_acc': 0.615}}
2025-09-10 12:13:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:13:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:13:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:13:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:13:11 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.324562, avg_loss=0.658114, seen=40, correct=24, accuracy=0.600000
2025-09-10 12:13:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:13:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:13:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:13:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:13:13 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 26.324562072753906, 'test_avg_loss': 0.6581140518188476, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-10 12:13:13 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 200, 'val_loss': 134.49026489257812, 'val_avg_loss': 0.6724513244628906, 'val_seen': 200, 'val_correct': 123, 'val_acc': 0.615}
2025-09-10 12:13:13 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 26.324562072753906, 'test_avg_loss': 0.6581140518188476, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-10 12:13:13 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #38', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.690945625305176, 'test_avg_loss': 0.7690945625305176, 'test_seen': 10, 'test_correct': 4, 'test_acc': 0.4}}
2025-09-10 12:13:13 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #38', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 26.324562072753906, 'test_avg_loss': 0.6581140518188476, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}}
2025-09-10 12:13:13 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 26.324562072753906, 'test_avg_loss': 0.6581140518188476, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}, metrics={'val_total': 200, 'val_loss': 134.49026489257812, 'val_avg_loss': 0.6724513244628906, 'val_seen': 200, 'val_correct': 123, 'val_acc': 0.615, 'test_total': 40, 'test_loss': 26.324562072753906, 'test_avg_loss': 0.6581140518188476, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-10 12:13:13 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-10 12:13:13 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:13:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-10 12:13:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:13:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:13:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-10 12:13:15 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=59.034561, avg_loss=0.711260, seen=83, correct=47, accuracy=0.566265
2025-09-10 12:13:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:13:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:13:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:13:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:13:18 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 83, 'val_loss': 59.03456115722656, 'val_avg_loss': 0.7112597729786333, 'val_seen': 83, 'val_correct': 47, 'val_acc': 0.5662650602409639}
2025-09-10 12:13:18 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-10 12:13:18 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 83, 'val_loss': 59.03456115722656, 'val_avg_loss': 0.7112597729786333, 'val_seen': 83, 'val_correct': 47, 'val_acc': 0.5662650602409639}
2025-09-10 12:13:18 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #39', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 21, 'val_loss': 17.002703189849854, 'val_avg_loss': 0.8096525328499931, 'val_seen': 21, 'val_correct': 7, 'val_acc': 0.3333333333333333}}
2025-09-10 12:13:18 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #39', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 83, 'val_loss': 59.03456115722656, 'val_avg_loss': 0.7112597729786333, 'val_seen': 83, 'val_correct': 47, 'val_acc': 0.5662650602409639}}
2025-09-10 12:13:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:13:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:13:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:13:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:13:20 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.964127, avg_loss=0.749103, seen=40, correct=20, accuracy=0.500000
2025-09-10 12:13:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:13:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:13:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:13:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:13:22 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 29.964126586914062, 'test_avg_loss': 0.7491031646728515, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-10 12:13:22 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 83, 'val_loss': 59.03456115722656, 'val_avg_loss': 0.7112597729786333, 'val_seen': 83, 'val_correct': 47, 'val_acc': 0.5662650602409639}
2025-09-10 12:13:22 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 29.964126586914062, 'test_avg_loss': 0.7491031646728515, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-10 12:13:22 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #39', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.451022148132324, 'test_avg_loss': 0.7451022148132325, 'test_seen': 10, 'test_correct': 5, 'test_acc': 0.5}}
2025-09-10 12:13:22 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #39', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 29.964126586914062, 'test_avg_loss': 0.7491031646728515, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}}
2025-09-10 12:13:22 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 29.964126586914062, 'test_avg_loss': 0.7491031646728515, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}, metrics={'val_total': 83, 'val_loss': 59.03456115722656, 'val_avg_loss': 0.7112597729786333, 'val_seen': 83, 'val_correct': 47, 'val_acc': 0.5662650602409639, 'test_total': 40, 'test_loss': 29.964126586914062, 'test_avg_loss': 0.7491031646728515, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-10 12:13:22 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-10 12:13:22 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:13:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 12:13:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:13:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:13:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 12:13:26 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=129.173080, avg_loss=0.645865, seen=200, correct=124, accuracy=0.620000
2025-09-10 12:13:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:13:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:13:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:13:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:13:29 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 200, 'val_loss': 129.17308044433594, 'val_avg_loss': 0.6458654022216797, 'val_seen': 200, 'val_correct': 124, 'val_acc': 0.62}
2025-09-10 12:13:29 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-10 12:13:29 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 200, 'val_loss': 129.17308044433594, 'val_avg_loss': 0.6458654022216797, 'val_seen': 200, 'val_correct': 124, 'val_acc': 0.62}
2025-09-10 12:13:29 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #40', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 50, 'val_loss': 33.15668702125549, 'val_avg_loss': 0.6631337404251099, 'val_seen': 50, 'val_correct': 27, 'val_acc': 0.54}}
2025-09-10 12:13:29 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #40', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 200, 'val_loss': 129.17308044433594, 'val_avg_loss': 0.6458654022216797, 'val_seen': 200, 'val_correct': 124, 'val_acc': 0.62}}
2025-09-10 12:13:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:13:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:13:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:13:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:13:31 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.898457, avg_loss=0.672461, seen=40, correct=26, accuracy=0.650000
2025-09-10 12:13:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:13:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:13:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:13:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:13:33 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 26.898456573486328, 'test_avg_loss': 0.6724614143371582, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-10 12:13:33 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 200, 'val_loss': 129.17308044433594, 'val_avg_loss': 0.6458654022216797, 'val_seen': 200, 'val_correct': 124, 'val_acc': 0.62}
2025-09-10 12:13:33 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 26.898456573486328, 'test_avg_loss': 0.6724614143371582, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-10 12:13:33 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #40', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.581612229347229, 'test_avg_loss': 0.6581612229347229, 'test_seen': 10, 'test_correct': 5, 'test_acc': 0.5}}
2025-09-10 12:13:33 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #40', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 26.898456573486328, 'test_avg_loss': 0.6724614143371582, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}}
2025-09-10 12:13:33 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 26.898456573486328, 'test_avg_loss': 0.6724614143371582, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}, metrics={'val_total': 200, 'val_loss': 129.17308044433594, 'val_avg_loss': 0.6458654022216797, 'val_seen': 200, 'val_correct': 124, 'val_acc': 0.62, 'test_total': 40, 'test_loss': 26.898456573486328, 'test_avg_loss': 0.6724614143371582, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-10 12:13:33 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-10 12:13:33 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:13:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-09-10 12:13:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:13:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=60, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:13:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-09-10 12:13:36 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=119, loss_sum=82.605721, avg_loss=0.694166, seen=119, correct=68, accuracy=0.571429
2025-09-10 12:13:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:13:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:13:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:13:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:13:38 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 119, 'val_loss': 82.60572052001953, 'val_avg_loss': 0.6941657186556263, 'val_seen': 119, 'val_correct': 68, 'val_acc': 0.5714285714285714}
2025-09-10 12:13:38 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-10 12:13:38 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 119, 'val_loss': 82.60572052001953, 'val_avg_loss': 0.6941657186556263, 'val_seen': 119, 'val_correct': 68, 'val_acc': 0.5714285714285714}
2025-09-10 12:13:38 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #41', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 30, 'val_loss': 17.754331052303314, 'val_avg_loss': 0.5918110350767771, 'val_seen': 30, 'val_correct': 22, 'val_acc': 0.7333333333333333}}
2025-09-10 12:13:38 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #41', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 119, 'val_loss': 82.60572052001953, 'val_avg_loss': 0.6941657186556263, 'val_seen': 119, 'val_correct': 68, 'val_acc': 0.5714285714285714}}
2025-09-10 12:13:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:13:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:13:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:13:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:13:39 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.771288, avg_loss=0.669282, seen=40, correct=22, accuracy=0.550000
2025-09-10 12:13:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:13:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:13:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:13:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:13:41 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 26.77128791809082, 'test_avg_loss': 0.6692821979522705, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-10 12:13:41 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 119, 'val_loss': 82.60572052001953, 'val_avg_loss': 0.6941657186556263, 'val_seen': 119, 'val_correct': 68, 'val_acc': 0.5714285714285714}
2025-09-10 12:13:41 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 26.77128791809082, 'test_avg_loss': 0.6692821979522705, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-10 12:13:41 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #41', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.492576539516449, 'test_avg_loss': 0.6492576539516449, 'test_seen': 10, 'test_correct': 7, 'test_acc': 0.7}}
2025-09-10 12:13:41 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #41', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 26.77128791809082, 'test_avg_loss': 0.6692821979522705, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}}
2025-09-10 12:13:41 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 26.77128791809082, 'test_avg_loss': 0.6692821979522705, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}, metrics={'val_total': 119, 'val_loss': 82.60572052001953, 'val_avg_loss': 0.6941657186556263, 'val_seen': 119, 'val_correct': 68, 'val_acc': 0.5714285714285714, 'test_total': 40, 'test_loss': 26.77128791809082, 'test_avg_loss': 0.6692821979522705, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-10 12:13:41 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-10 12:13:41 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:13:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 12:13:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:13:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:13:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 12:13:46 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=129.265320, avg_loss=0.646327, seen=200, correct=123, accuracy=0.615000
2025-09-10 12:13:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:13:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:13:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:13:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:13:49 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 200, 'val_loss': 129.26531982421875, 'val_avg_loss': 0.6463265991210938, 'val_seen': 200, 'val_correct': 123, 'val_acc': 0.615}
2025-09-10 12:13:49 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-10 12:13:49 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 200, 'val_loss': 129.26531982421875, 'val_avg_loss': 0.6463265991210938, 'val_seen': 200, 'val_correct': 123, 'val_acc': 0.615}
2025-09-10 12:13:49 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #42', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 50, 'val_loss': 34.020853996276855, 'val_avg_loss': 0.6804170799255371, 'val_seen': 50, 'val_correct': 27, 'val_acc': 0.54}}
2025-09-10 12:13:49 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #42', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 200, 'val_loss': 129.26531982421875, 'val_avg_loss': 0.6463265991210938, 'val_seen': 200, 'val_correct': 123, 'val_acc': 0.615}}
2025-09-10 12:13:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:13:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:13:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:13:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:13:51 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.370506, avg_loss=0.684263, seen=40, correct=26, accuracy=0.650000
2025-09-10 12:13:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:13:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:13:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:13:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:13:53 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 27.370506286621094, 'test_avg_loss': 0.6842626571655274, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-10 12:13:53 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 200, 'val_loss': 129.26531982421875, 'val_avg_loss': 0.6463265991210938, 'val_seen': 200, 'val_correct': 123, 'val_acc': 0.615}
2025-09-10 12:13:53 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 27.370506286621094, 'test_avg_loss': 0.6842626571655274, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-10 12:13:53 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #42', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.029444694519043, 'test_avg_loss': 0.7029444694519043, 'test_seen': 10, 'test_correct': 6, 'test_acc': 0.6}}
2025-09-10 12:13:53 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #42', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 27.370506286621094, 'test_avg_loss': 0.6842626571655274, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}}
2025-09-10 12:13:53 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 27.370506286621094, 'test_avg_loss': 0.6842626571655274, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}, metrics={'val_total': 200, 'val_loss': 129.26531982421875, 'val_avg_loss': 0.6463265991210938, 'val_seen': 200, 'val_correct': 123, 'val_acc': 0.615, 'test_total': 40, 'test_loss': 27.370506286621094, 'test_avg_loss': 0.6842626571655274, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-10 12:13:53 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-10 12:13:53 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:13:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-09-10 12:13:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:13:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=45, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:13:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-09-10 12:13:56 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=89, loss_sum=61.063286, avg_loss=0.686104, seen=89, correct=51, accuracy=0.573034
2025-09-10 12:13:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:13:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:13:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:13:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:13:57 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 89, 'val_loss': 61.06328582763672, 'val_avg_loss': 0.6861043351419857, 'val_seen': 89, 'val_correct': 51, 'val_acc': 0.5730337078651685}
2025-09-10 12:13:57 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-10 12:13:57 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 89, 'val_loss': 61.06328582763672, 'val_avg_loss': 0.6861043351419857, 'val_seen': 89, 'val_correct': 51, 'val_acc': 0.5730337078651685}
2025-09-10 12:13:57 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #43', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 23, 'val_loss': 17.29624378681183, 'val_avg_loss': 0.7520105994266012, 'val_seen': 23, 'val_correct': 12, 'val_acc': 0.5217391304347826}}
2025-09-10 12:13:57 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #43', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 89, 'val_loss': 61.06328582763672, 'val_avg_loss': 0.6861043351419857, 'val_seen': 89, 'val_correct': 51, 'val_acc': 0.5730337078651685}}
2025-09-10 12:13:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:13:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:13:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:13:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:13:59 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.150658, avg_loss=0.653766, seen=40, correct=23, accuracy=0.575000
2025-09-10 12:13:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:13:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:14:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:14:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:14:02 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 26.150657653808594, 'test_avg_loss': 0.6537664413452149, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-10 12:14:02 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 89, 'val_loss': 61.06328582763672, 'val_avg_loss': 0.6861043351419857, 'val_seen': 89, 'val_correct': 51, 'val_acc': 0.5730337078651685}
2025-09-10 12:14:02 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 26.150657653808594, 'test_avg_loss': 0.6537664413452149, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-10 12:14:02 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #43', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.006313621997833, 'test_avg_loss': 0.6006313621997833, 'test_seen': 10, 'test_correct': 5, 'test_acc': 0.5}}
2025-09-10 12:14:02 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #43', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 26.150657653808594, 'test_avg_loss': 0.6537664413452149, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}}
2025-09-10 12:14:02 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 26.150657653808594, 'test_avg_loss': 0.6537664413452149, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}, metrics={'val_total': 89, 'val_loss': 61.06328582763672, 'val_avg_loss': 0.6861043351419857, 'val_seen': 89, 'val_correct': 51, 'val_acc': 0.5730337078651685, 'test_total': 40, 'test_loss': 26.150657653808594, 'test_avg_loss': 0.6537664413452149, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-10 12:14:02 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-10 12:14:02 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:14:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 12:14:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:14:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:14:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 12:14:06 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=141.186523, avg_loss=0.705933, seen=200, correct=110, accuracy=0.550000
2025-09-10 12:14:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:14:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:14:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:14:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:14:08 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 200, 'val_loss': 141.1865234375, 'val_avg_loss': 0.7059326171875, 'val_seen': 200, 'val_correct': 110, 'val_acc': 0.55}
2025-09-10 12:14:08 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-10 12:14:08 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 200, 'val_loss': 141.1865234375, 'val_avg_loss': 0.7059326171875, 'val_seen': 200, 'val_correct': 110, 'val_acc': 0.55}
2025-09-10 12:14:08 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #44', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 50, 'val_loss': 39.00402355194092, 'val_avg_loss': 0.7800804710388184, 'val_seen': 50, 'val_correct': 24, 'val_acc': 0.48}}
2025-09-10 12:14:08 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #44', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 200, 'val_loss': 141.1865234375, 'val_avg_loss': 0.7059326171875, 'val_seen': 200, 'val_correct': 110, 'val_acc': 0.55}}
2025-09-10 12:14:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:14:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:14:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:14:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:14:09 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.158291, avg_loss=0.603957, seen=40, correct=25, accuracy=0.625000
2025-09-10 12:14:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:14:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:14:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:14:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:14:10 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 24.15829086303711, 'test_avg_loss': 0.6039572715759277, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-10 12:14:10 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 200, 'val_loss': 141.1865234375, 'val_avg_loss': 0.7059326171875, 'val_seen': 200, 'val_correct': 110, 'val_acc': 0.55}
2025-09-10 12:14:10 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 24.15829086303711, 'test_avg_loss': 0.6039572715759277, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-10 12:14:10 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #44', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.555295288562775, 'test_avg_loss': 0.6555295288562775, 'test_seen': 10, 'test_correct': 6, 'test_acc': 0.6}}
2025-09-10 12:14:10 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #44', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 24.15829086303711, 'test_avg_loss': 0.6039572715759277, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}}
2025-09-10 12:14:10 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 24.15829086303711, 'test_avg_loss': 0.6039572715759277, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}, metrics={'val_total': 200, 'val_loss': 141.1865234375, 'val_avg_loss': 0.7059326171875, 'val_seen': 200, 'val_correct': 110, 'val_acc': 0.55, 'test_total': 40, 'test_loss': 24.15829086303711, 'test_avg_loss': 0.6039572715759277, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-10 12:14:10 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-10 12:14:10 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:14:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-09-10 12:14:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:14:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=50, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:14:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-09-10 12:14:14 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=100, loss_sum=67.162445, avg_loss=0.671624, seen=100, correct=59, accuracy=0.590000
2025-09-10 12:14:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:14:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:14:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:14:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:14:16 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 100, 'val_loss': 67.16244506835938, 'val_avg_loss': 0.6716244506835938, 'val_seen': 100, 'val_correct': 59, 'val_acc': 0.59}
2025-09-10 12:14:16 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-10 12:14:16 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 100, 'val_loss': 67.16244506835938, 'val_avg_loss': 0.6716244506835938, 'val_seen': 100, 'val_correct': 59, 'val_acc': 0.59}
2025-09-10 12:14:16 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #45', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 25, 'val_loss': 16.2681565284729, 'val_avg_loss': 0.650726261138916, 'val_seen': 25, 'val_correct': 17, 'val_acc': 0.68}}
2025-09-10 12:14:16 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #45', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 100, 'val_loss': 67.16244506835938, 'val_avg_loss': 0.6716244506835938, 'val_seen': 100, 'val_correct': 59, 'val_acc': 0.59}}
2025-09-10 12:14:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:14:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:14:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:14:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:14:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.472992, avg_loss=0.736825, seen=40, correct=20, accuracy=0.500000
2025-09-10 12:14:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:14:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:14:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:14:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:14:20 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 29.472991943359375, 'test_avg_loss': 0.7368247985839844, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-10 12:14:20 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 100, 'val_loss': 67.16244506835938, 'val_avg_loss': 0.6716244506835938, 'val_seen': 100, 'val_correct': 59, 'val_acc': 0.59}
2025-09-10 12:14:20 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 29.472991943359375, 'test_avg_loss': 0.7368247985839844, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-10 12:14:20 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #45', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 8.367159128189087, 'test_avg_loss': 0.8367159128189087, 'test_seen': 10, 'test_correct': 4, 'test_acc': 0.4}}
2025-09-10 12:14:20 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #45', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 29.472991943359375, 'test_avg_loss': 0.7368247985839844, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}}
2025-09-10 12:14:20 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 29.472991943359375, 'test_avg_loss': 0.7368247985839844, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}, metrics={'val_total': 100, 'val_loss': 67.16244506835938, 'val_avg_loss': 0.6716244506835938, 'val_seen': 100, 'val_correct': 59, 'val_acc': 0.59, 'test_total': 40, 'test_loss': 29.472991943359375, 'test_avg_loss': 0.7368247985839844, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-10 12:14:20 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-10 12:14:20 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:14:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-10 12:14:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:14:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:14:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-10 12:14:23 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=79.933304, avg_loss=0.726666, seen=110, correct=57, accuracy=0.518182
2025-09-10 12:14:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:14:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:14:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:14:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:14:25 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 110, 'val_loss': 79.93330383300781, 'val_avg_loss': 0.7266663984818892, 'val_seen': 110, 'val_correct': 57, 'val_acc': 0.5181818181818182}
2025-09-10 12:14:25 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-10 12:14:25 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 110, 'val_loss': 79.93330383300781, 'val_avg_loss': 0.7266663984818892, 'val_seen': 110, 'val_correct': 57, 'val_acc': 0.5181818181818182}
2025-09-10 12:14:25 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #46', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 28, 'val_loss': 19.414919197559357, 'val_avg_loss': 0.6933899713414056, 'val_seen': 28, 'val_correct': 15, 'val_acc': 0.5357142857142857}}
2025-09-10 12:14:25 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #46', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 110, 'val_loss': 79.93330383300781, 'val_avg_loss': 0.7266663984818892, 'val_seen': 110, 'val_correct': 57, 'val_acc': 0.5181818181818182}}
2025-09-10 12:14:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:14:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:14:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:14:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:14:26 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.092684, avg_loss=0.602317, seen=40, correct=27, accuracy=0.675000
2025-09-10 12:14:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:14:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:14:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:14:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:14:28 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 24.092683792114258, 'test_avg_loss': 0.6023170948028564, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-10 12:14:28 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 110, 'val_loss': 79.93330383300781, 'val_avg_loss': 0.7266663984818892, 'val_seen': 110, 'val_correct': 57, 'val_acc': 0.5181818181818182}
2025-09-10 12:14:28 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 24.092683792114258, 'test_avg_loss': 0.6023170948028564, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-10 12:14:28 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #46', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 4.927217245101929, 'test_avg_loss': 0.49272172451019286, 'test_seen': 10, 'test_correct': 8, 'test_acc': 0.8}}
2025-09-10 12:14:28 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #46', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 24.092683792114258, 'test_avg_loss': 0.6023170948028564, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}}
2025-09-10 12:14:28 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 24.092683792114258, 'test_avg_loss': 0.6023170948028564, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}, metrics={'val_total': 110, 'val_loss': 79.93330383300781, 'val_avg_loss': 0.7266663984818892, 'val_seen': 110, 'val_correct': 57, 'val_acc': 0.5181818181818182, 'test_total': 40, 'test_loss': 24.092683792114258, 'test_avg_loss': 0.6023170948028564, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-10 12:14:28 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-10 12:14:28 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:14:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-09-10 12:14:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:14:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=74, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:14:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-10 12:14:32 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=147, loss_sum=103.468933, avg_loss=0.703870, seen=147, correct=76, accuracy=0.517007
2025-09-10 12:14:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:14:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:14:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:14:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:14:34 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 147, 'val_loss': 103.46893310546875, 'val_avg_loss': 0.7038702932344812, 'val_seen': 147, 'val_correct': 76, 'val_acc': 0.5170068027210885}
2025-09-10 12:14:34 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-10 12:14:34 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 147, 'val_loss': 103.46893310546875, 'val_avg_loss': 0.7038702932344812, 'val_seen': 147, 'val_correct': 76, 'val_acc': 0.5170068027210885}
2025-09-10 12:14:34 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #47', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 37, 'val_loss': 25.24212223291397, 'val_avg_loss': 0.6822195198084857, 'val_seen': 37, 'val_correct': 18, 'val_acc': 0.4864864864864865}}
2025-09-10 12:14:34 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #47', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 147, 'val_loss': 103.46893310546875, 'val_avg_loss': 0.7038702932344812, 'val_seen': 147, 'val_correct': 76, 'val_acc': 0.5170068027210885}}
2025-09-10 12:14:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:14:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:14:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:14:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:14:35 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.027332, avg_loss=0.600683, seen=40, correct=25, accuracy=0.625000
2025-09-10 12:14:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:14:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:14:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:14:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:14:37 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 24.027332305908203, 'test_avg_loss': 0.6006833076477051, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-10 12:14:37 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 147, 'val_loss': 103.46893310546875, 'val_avg_loss': 0.7038702932344812, 'val_seen': 147, 'val_correct': 76, 'val_acc': 0.5170068027210885}
2025-09-10 12:14:37 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 24.027332305908203, 'test_avg_loss': 0.6006833076477051, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-10 12:14:37 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #47', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.212461709976196, 'test_avg_loss': 0.6212461709976196, 'test_seen': 10, 'test_correct': 7, 'test_acc': 0.7}}
2025-09-10 12:14:37 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #47', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 24.027332305908203, 'test_avg_loss': 0.6006833076477051, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}}
2025-09-10 12:14:37 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 24.027332305908203, 'test_avg_loss': 0.6006833076477051, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}, metrics={'val_total': 147, 'val_loss': 103.46893310546875, 'val_avg_loss': 0.7038702932344812, 'val_seen': 147, 'val_correct': 76, 'val_acc': 0.5170068027210885, 'test_total': 40, 'test_loss': 24.027332305908203, 'test_avg_loss': 0.6006833076477051, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-10 12:14:37 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-10 12:14:37 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:14:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-09-10 12:14:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:14:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=23, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:14:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-09-10 12:14:39 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=46, loss_sum=32.797554, avg_loss=0.712990, seen=46, correct=22, accuracy=0.478261
2025-09-10 12:14:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:14:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:14:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:14:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:14:40 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 46, 'val_loss': 32.79755401611328, 'val_avg_loss': 0.7129903046981149, 'val_seen': 46, 'val_correct': 22, 'val_acc': 0.4782608695652174}
2025-09-10 12:14:40 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-10 12:14:40 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 46, 'val_loss': 32.79755401611328, 'val_avg_loss': 0.7129903046981149, 'val_seen': 46, 'val_correct': 22, 'val_acc': 0.4782608695652174}
2025-09-10 12:14:40 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #48', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 12, 'val_loss': 8.36408668756485, 'val_avg_loss': 0.6970072239637375, 'val_seen': 12, 'val_correct': 6, 'val_acc': 0.5}}
2025-09-10 12:14:40 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #48', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 46, 'val_loss': 32.79755401611328, 'val_avg_loss': 0.7129903046981149, 'val_seen': 46, 'val_correct': 22, 'val_acc': 0.4782608695652174}}
2025-09-10 12:14:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:14:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:14:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:14:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:14:41 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.123512, avg_loss=0.703088, seen=40, correct=20, accuracy=0.500000
2025-09-10 12:14:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:14:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:14:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:14:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:14:42 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 28.123512268066406, 'test_avg_loss': 0.7030878067016602, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-10 12:14:42 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 46, 'val_loss': 32.79755401611328, 'val_avg_loss': 0.7129903046981149, 'val_seen': 46, 'val_correct': 22, 'val_acc': 0.4782608695652174}
2025-09-10 12:14:42 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 28.123512268066406, 'test_avg_loss': 0.7030878067016602, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-10 12:14:42 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #48', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.103603422641754, 'test_avg_loss': 0.7103603422641754, 'test_seen': 10, 'test_correct': 5, 'test_acc': 0.5}}
2025-09-10 12:14:42 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #48', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 28.123512268066406, 'test_avg_loss': 0.7030878067016602, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}}
2025-09-10 12:14:42 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 28.123512268066406, 'test_avg_loss': 0.7030878067016602, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}, metrics={'val_total': 46, 'val_loss': 32.79755401611328, 'val_avg_loss': 0.7129903046981149, 'val_seen': 46, 'val_correct': 22, 'val_acc': 0.4782608695652174, 'test_total': 40, 'test_loss': 28.123512268066406, 'test_avg_loss': 0.7030878067016602, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-10 12:14:42 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-10 12:14:42 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:14:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-09-10 12:14:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:14:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=66, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:14:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-09-10 12:14:46 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=132, loss_sum=85.844482, avg_loss=0.650337, seen=132, correct=85, accuracy=0.643939
2025-09-10 12:14:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:14:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:14:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:14:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:14:48 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 132, 'val_loss': 85.844482421875, 'val_avg_loss': 0.6503369880445076, 'val_seen': 132, 'val_correct': 85, 'val_acc': 0.6439393939393939}
2025-09-10 12:14:48 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-10 12:14:48 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 132, 'val_loss': 85.844482421875, 'val_avg_loss': 0.6503369880445076, 'val_seen': 132, 'val_correct': 85, 'val_acc': 0.6439393939393939}
2025-09-10 12:14:48 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #49', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 33, 'val_loss': 17.815562069416046, 'val_avg_loss': 0.5398655172550317, 'val_seen': 33, 'val_correct': 23, 'val_acc': 0.696969696969697}}
2025-09-10 12:14:48 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #49', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 132, 'val_loss': 85.844482421875, 'val_avg_loss': 0.6503369880445076, 'val_seen': 132, 'val_correct': 85, 'val_acc': 0.6439393939393939}}
2025-09-10 12:14:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:14:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:14:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:14:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:14:50 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=32.986794, avg_loss=0.824670, seen=40, correct=19, accuracy=0.475000
2025-09-10 12:14:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:14:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:14:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:14:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:14:52 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 32.986793518066406, 'test_avg_loss': 0.8246698379516602, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-10 12:14:52 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 132, 'val_loss': 85.844482421875, 'val_avg_loss': 0.6503369880445076, 'val_seen': 132, 'val_correct': 85, 'val_acc': 0.6439393939393939}
2025-09-10 12:14:52 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 32.986793518066406, 'test_avg_loss': 0.8246698379516602, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-10 12:14:52 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #49', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 8.318078756332397, 'test_avg_loss': 0.8318078756332398, 'test_seen': 10, 'test_correct': 4, 'test_acc': 0.4}}
2025-09-10 12:14:52 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #49', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 32.986793518066406, 'test_avg_loss': 0.8246698379516602, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}}
2025-09-10 12:14:52 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 32.986793518066406, 'test_avg_loss': 0.8246698379516602, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}, metrics={'val_total': 132, 'val_loss': 85.844482421875, 'val_avg_loss': 0.6503369880445076, 'val_seen': 132, 'val_correct': 85, 'val_acc': 0.6439393939393939, 'test_total': 40, 'test_loss': 32.986793518066406, 'test_avg_loss': 0.8246698379516602, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-10 12:14:52 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-10 12:14:53 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:14:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-09-10 12:14:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:14:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:14:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-10 12:14:56 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=133, loss_sum=90.515244, avg_loss=0.680566, seen=133, correct=73, accuracy=0.548872
2025-09-10 12:14:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:14:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:14:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:14:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:14:58 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 133, 'val_loss': 90.51524353027344, 'val_avg_loss': 0.6805657408291236, 'val_seen': 133, 'val_correct': 73, 'val_acc': 0.5488721804511278}
2025-09-10 12:14:58 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-10 12:14:58 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 133, 'val_loss': 90.51524353027344, 'val_avg_loss': 0.6805657408291236, 'val_seen': 133, 'val_correct': 73, 'val_acc': 0.5488721804511278}
2025-09-10 12:14:58 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #50', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 34, 'val_loss': 25.90956550836563, 'val_avg_loss': 0.762046044363695, 'val_seen': 34, 'val_correct': 16, 'val_acc': 0.47058823529411764}}
2025-09-10 12:14:58 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #50', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 133, 'val_loss': 90.51524353027344, 'val_avg_loss': 0.6805657408291236, 'val_seen': 133, 'val_correct': 73, 'val_acc': 0.5488721804511278}}
2025-09-10 12:14:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:14:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:14:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:14:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:14:59 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.987129, avg_loss=0.774678, seen=40, correct=17, accuracy=0.425000
2025-09-10 12:14:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:14:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:15:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:15:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:15:01 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 30.98712921142578, 'test_avg_loss': 0.7746782302856445, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-09-10 12:15:01 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 133, 'val_loss': 90.51524353027344, 'val_avg_loss': 0.6805657408291236, 'val_seen': 133, 'val_correct': 73, 'val_acc': 0.5488721804511278}
2025-09-10 12:15:01 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 30.98712921142578, 'test_avg_loss': 0.7746782302856445, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-09-10 12:15:01 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #50', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 9.061574935913086, 'test_avg_loss': 0.9061574935913086, 'test_seen': 10, 'test_correct': 3, 'test_acc': 0.3}}
2025-09-10 12:15:01 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #50', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 30.98712921142578, 'test_avg_loss': 0.7746782302856445, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}}
2025-09-10 12:15:01 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 30.98712921142578, 'test_avg_loss': 0.7746782302856445, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}, metrics={'val_total': 133, 'val_loss': 90.51524353027344, 'val_avg_loss': 0.6805657408291236, 'val_seen': 133, 'val_correct': 73, 'val_acc': 0.5488721804511278, 'test_total': 40, 'test_loss': 30.98712921142578, 'test_avg_loss': 0.7746782302856445, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-09-10 12:15:01 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-10 12:15:01 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:15:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-10 12:15:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:15:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:15:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-10 12:15:04 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=57.255859, avg_loss=0.689830, seen=83, correct=48, accuracy=0.578313
2025-09-10 12:15:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:15:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:15:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:15:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:15:06 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 83, 'val_loss': 57.255859375, 'val_avg_loss': 0.6898296310240963, 'val_seen': 83, 'val_correct': 48, 'val_acc': 0.5783132530120482}
2025-09-10 12:15:06 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-10 12:15:06 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 83, 'val_loss': 57.255859375, 'val_avg_loss': 0.6898296310240963, 'val_seen': 83, 'val_correct': 48, 'val_acc': 0.5783132530120482}
2025-09-10 12:15:06 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #51', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 21, 'val_loss': 14.924913763999939, 'val_avg_loss': 0.7107101792380923, 'val_seen': 21, 'val_correct': 14, 'val_acc': 0.6666666666666666}}
2025-09-10 12:15:06 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #51', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 83, 'val_loss': 57.255859375, 'val_avg_loss': 0.6898296310240963, 'val_seen': 83, 'val_correct': 48, 'val_acc': 0.5783132530120482}}
2025-09-10 12:15:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:15:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:15:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:15:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:15:07 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.285465, avg_loss=0.632137, seen=40, correct=24, accuracy=0.600000
2025-09-10 12:15:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:15:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:15:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:15:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:15:09 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 25.285465240478516, 'test_avg_loss': 0.6321366310119629, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-10 12:15:09 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 83, 'val_loss': 57.255859375, 'val_avg_loss': 0.6898296310240963, 'val_seen': 83, 'val_correct': 48, 'val_acc': 0.5783132530120482}
2025-09-10 12:15:09 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 25.285465240478516, 'test_avg_loss': 0.6321366310119629, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-10 12:15:09 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #51', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 5.7477192878723145, 'test_avg_loss': 0.5747719287872315, 'test_seen': 10, 'test_correct': 6, 'test_acc': 0.6}}
2025-09-10 12:15:09 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #51', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 25.285465240478516, 'test_avg_loss': 0.6321366310119629, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}}
2025-09-10 12:15:09 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 25.285465240478516, 'test_avg_loss': 0.6321366310119629, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}, metrics={'val_total': 83, 'val_loss': 57.255859375, 'val_avg_loss': 0.6898296310240963, 'val_seen': 83, 'val_correct': 48, 'val_acc': 0.5783132530120482, 'test_total': 40, 'test_loss': 25.285465240478516, 'test_avg_loss': 0.6321366310119629, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-10 12:15:09 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-10 12:15:09 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:15:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-10 12:15:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:15:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:15:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-10 12:15:13 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=126.428444, avg_loss=0.672492, seen=188, correct=109, accuracy=0.579787
2025-09-10 12:15:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:15:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:15:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:15:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:15:15 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 188, 'val_loss': 126.4284439086914, 'val_avg_loss': 0.6724917229185713, 'val_seen': 188, 'val_correct': 109, 'val_acc': 0.5797872340425532}
2025-09-10 12:15:15 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-10 12:15:15 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 188, 'val_loss': 126.4284439086914, 'val_avg_loss': 0.6724917229185713, 'val_seen': 188, 'val_correct': 109, 'val_acc': 0.5797872340425532}
2025-09-10 12:15:15 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #52', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 47, 'val_loss': 30.416482746601105, 'val_avg_loss': 0.6471592073744916, 'val_seen': 47, 'val_correct': 30, 'val_acc': 0.6382978723404256}}
2025-09-10 12:15:15 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #52', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 188, 'val_loss': 126.4284439086914, 'val_avg_loss': 0.6724917229185713, 'val_seen': 188, 'val_correct': 109, 'val_acc': 0.5797872340425532}}
2025-09-10 12:15:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:15:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:15:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:15:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:15:16 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.729603, avg_loss=0.643240, seen=40, correct=24, accuracy=0.600000
2025-09-10 12:15:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:15:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:15:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:15:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:15:17 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 25.729602813720703, 'test_avg_loss': 0.6432400703430176, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-10 12:15:17 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 188, 'val_loss': 126.4284439086914, 'val_avg_loss': 0.6724917229185713, 'val_seen': 188, 'val_correct': 109, 'val_acc': 0.5797872340425532}
2025-09-10 12:15:17 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 25.729602813720703, 'test_avg_loss': 0.6432400703430176, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-10 12:15:17 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #52', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 8.915146708488464, 'test_avg_loss': 0.8915146708488464, 'test_seen': 10, 'test_correct': 3, 'test_acc': 0.3}}
2025-09-10 12:15:17 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #52', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 25.729602813720703, 'test_avg_loss': 0.6432400703430176, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}}
2025-09-10 12:15:17 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 25.729602813720703, 'test_avg_loss': 0.6432400703430176, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}, metrics={'val_total': 188, 'val_loss': 126.4284439086914, 'val_avg_loss': 0.6724917229185713, 'val_seen': 188, 'val_correct': 109, 'val_acc': 0.5797872340425532, 'test_total': 40, 'test_loss': 25.729602813720703, 'test_avg_loss': 0.6432400703430176, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-10 12:15:17 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-10 12:15:17 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:15:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-10 12:15:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:15:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:15:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-10 12:15:21 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=133.953094, avg_loss=0.669765, seen=200, correct=119, accuracy=0.595000
2025-09-10 12:15:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:15:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:15:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:15:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:15:24 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 200, 'val_loss': 133.95309448242188, 'val_avg_loss': 0.6697654724121094, 'val_seen': 200, 'val_correct': 119, 'val_acc': 0.595}
2025-09-10 12:15:24 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-10 12:15:24 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 200, 'val_loss': 133.95309448242188, 'val_avg_loss': 0.6697654724121094, 'val_seen': 200, 'val_correct': 119, 'val_acc': 0.595}
2025-09-10 12:15:24 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #53', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 50, 'val_loss': 32.8664813041687, 'val_avg_loss': 0.657329626083374, 'val_seen': 50, 'val_correct': 31, 'val_acc': 0.62}}
2025-09-10 12:15:24 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #53', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 200, 'val_loss': 133.95309448242188, 'val_avg_loss': 0.6697654724121094, 'val_seen': 200, 'val_correct': 119, 'val_acc': 0.595}}
2025-09-10 12:15:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-10 12:15:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:15:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-10 12:15:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-10 12:15:26 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.065966, avg_loss=0.626649, seen=40, correct=27, accuracy=0.675000
2025-09-10 12:15:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-10 12:15:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-10 12:15:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-10 12:15:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2226MB allocated=2187MB
2025-09-10 12:15:27 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 25.06596565246582, 'test_avg_loss': 0.6266491413116455, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-10 12:15:27 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 200, 'val_loss': 133.95309448242188, 'val_avg_loss': 0.6697654724121094, 'val_seen': 200, 'val_correct': 119, 'val_acc': 0.595}
2025-09-10 12:15:27 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 25.06596565246582, 'test_avg_loss': 0.6266491413116455, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-10 12:15:27 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #53', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 8.314651727676392, 'test_avg_loss': 0.8314651727676392, 'test_seen': 10, 'test_correct': 5, 'test_acc': 0.5}}
2025-09-10 12:15:27 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #53', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 25.06596565246582, 'test_avg_loss': 0.6266491413116455, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}}
2025-09-10 12:15:27 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 25.06596565246582, 'test_avg_loss': 0.6266491413116455, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}, metrics={'val_total': 200, 'val_loss': 133.95309448242188, 'val_avg_loss': 0.6697654724121094, 'val_seen': 200, 'val_correct': 119, 'val_acc': 0.595, 'test_total': 40, 'test_loss': 25.06596565246582, 'test_avg_loss': 0.6266491413116455, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-10 12:15:27 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-10 12:15:27 (federatedscope.core.workers.server:772) INFO: {'Role': 'Server #', 'Round': 0, 'Results_weighted_avg': {'val_total': 124.9622641509434, 'val_loss': 104.39527114683564, 'val_avg_loss': 0.669754723020862, 'val_acc': 0.5953495394836177, 'test_total': 40.0, 'test_loss': 26.972495636850034, 'test_avg_loss': 0.6743123909212508, 'test_acc': 0.5919811320754716}, 'Results_avg': {'val_total': 124.9622641509434, 'val_loss': 83.69406661447489, 'val_avg_loss': 0.6735522063028047, 'val_acc': 0.5940538357390138, 'test_total': 40.0, 'test_loss': 26.972495636850034, 'test_avg_loss': 0.6743123909212508, 'test_acc': 0.5919811320754719}, 'Results_fairness': {'val_total': 124.9622641509434, 'test_total': 40.0, 'val_loss_std': 41.60336427205757, 'val_loss_bottom_decile': 19.565603256225586, 'val_loss_top_decile': 134.02191162109375, 'val_loss_min': 7.07394552230835, 'val_loss_max': 142.5851593017578, 'val_loss_bottom10%': 10.305497455596925, 'val_loss_top10%': 137.5391387939453, 'val_loss_cos1': 0.8954675941182008, 'val_loss_entropy': 3.823409885237325, 'val_avg_loss_std': 0.040985674633729104, 'val_avg_loss_bottom_decile': 0.6403905487060547, 'val_avg_loss_top_decile': 0.7112597729786333, 'val_avg_loss_min': 0.5939437321254185, 'val_avg_loss_max': 0.8836186148903586, 'val_avg_loss_bottom10%': 0.6126706189388568, 'val_avg_loss_top10%': 0.7468951707184073, 'val_avg_loss_cos1': 0.9981537621892995, 'val_avg_loss_entropy': 3.968513687578216, 'val_acc_std': 0.06320071282710552, 'val_acc_bottom_decile': 0.5181818181818182, 'val_acc_top_decile': 0.6504065040650406, 'val_acc_min': 0.36363636363636365, 'val_acc_max': 0.8181818181818182, 'val_acc_bottom10%': 0.47311414051786727, 'val_acc_top10%': 0.6914037817961466, 'val_acc_cos1': 0.9943882975340717, 'val_acc_entropy': 3.9645062089927574, 'test_loss_std': 2.518742050761679, 'test_loss_bottom_decile': 24.110645294189453, 'test_loss_top_decile': 30.644969940185547, 'test_loss_min': 23.190872192382812, 'test_loss_max': 32.986793518066406, 'test_loss_bottom10%': 23.68499298095703, 'test_loss_top10%': 31.627936045328777, 'test_loss_cos1': 0.9956682231688659, 'test_loss_entropy': 3.9659950765257754, 'test_avg_loss_std': 0.06296855126904198, 'test_avg_loss_bottom_decile': 0.6027661323547363, 'test_avg_loss_top_decile': 0.7661242485046387, 'test_avg_loss_min': 0.5797718048095704, 'test_avg_loss_max': 0.8246698379516602, 'test_avg_loss_bottom10%': 0.5921248245239258, 'test_avg_loss_top10%': 0.7906984011332194, 'test_avg_loss_cos1': 0.9956682231688658, 'test_avg_loss_entropy': 3.96599507653813, 'test_acc_std': 0.07842397846780286, 'test_acc_bottom_decile': 0.475, 'test_acc_top_decile': 0.675, 'test_acc_min': 0.425, 'test_acc_max': 0.725, 'test_acc_bottom10%': 0.45499999999999996, 'test_acc_top10%': 0.7083333333333334, 'test_acc_cos1': 0.9913387410101133, 'test_acc_entropy': 3.9613791720803175}}
2025-09-10 12:15:27 (root:790) INFO: Find new best result: {'client_best_individual': {'val_loss': 7.07394552230835, 'val_total': 11.0, 'val_avg_loss': 0.5939437321254185, 'val_acc': 0.8181818181818182, 'test_total': 40.0, 'test_loss': 23.190872192382812, 'test_avg_loss': 0.5797718048095704, 'test_acc': 0.725}}
2025-09-10 12:15:27 (root:790) INFO: Find new best result: {'client_best_individual': {'val_loss': 7.07394552230835, 'val_total': 11.0, 'val_avg_loss': 0.5939437321254185, 'val_acc': 0.8181818181818182, 'test_total': 40.0, 'test_loss': 23.190872192382812, 'test_avg_loss': 0.5797718048095704, 'test_acc': 0.725}, 'client_summarized_weighted_avg': {'val_loss': 104.39527114683564, 'val_total': 124.9622641509434, 'val_avg_loss': 0.669754723020862, 'val_acc': 0.5953495394836177, 'test_total': 40.0, 'test_loss': 26.972495636850034, 'test_avg_loss': 0.6743123909212508, 'test_acc': 0.5919811320754716}}
2025-09-10 12:15:27 (root:790) INFO: Find new best result: {'client_best_individual': {'val_loss': 7.07394552230835, 'val_total': 11.0, 'val_avg_loss': 0.5939437321254185, 'val_acc': 0.8181818181818182, 'test_total': 40.0, 'test_loss': 23.190872192382812, 'test_avg_loss': 0.5797718048095704, 'test_acc': 0.725}, 'client_summarized_weighted_avg': {'val_loss': 104.39527114683564, 'val_total': 124.9622641509434, 'val_avg_loss': 0.669754723020862, 'val_acc': 0.5953495394836177, 'test_total': 40.0, 'test_loss': 26.972495636850034, 'test_avg_loss': 0.6743123909212508, 'test_acc': 0.5919811320754716}, 'client_summarized_avg': {'val_loss': 83.69406661447489, 'val_total': 124.9622641509434, 'val_avg_loss': 0.6735522063028047, 'val_acc': 0.5940538357390138, 'test_total': 40.0, 'test_loss': 26.972495636850034, 'test_avg_loss': 0.6743123909212508, 'test_acc': 0.5919811320754719}}
2025-09-10 12:15:27 (root:790) INFO: Find new best result: {'client_best_individual': {'val_loss': 7.07394552230835, 'val_total': 11.0, 'val_avg_loss': 0.5939437321254185, 'val_acc': 0.8181818181818182, 'test_total': 40.0, 'test_loss': 23.190872192382812, 'test_avg_loss': 0.5797718048095704, 'test_acc': 0.725}, 'client_summarized_weighted_avg': {'val_loss': 104.39527114683564, 'val_total': 124.9622641509434, 'val_avg_loss': 0.669754723020862, 'val_acc': 0.5953495394836177, 'test_total': 40.0, 'test_loss': 26.972495636850034, 'test_avg_loss': 0.6743123909212508, 'test_acc': 0.5919811320754716}, 'client_summarized_avg': {'val_loss': 83.69406661447489, 'val_total': 124.9622641509434, 'val_avg_loss': 0.6735522063028047, 'val_acc': 0.5940538357390138, 'test_total': 40.0, 'test_loss': 26.972495636850034, 'test_avg_loss': 0.6743123909212508, 'test_acc': 0.5919811320754719}, 'client_summarized_fairness': {'val_loss_entropy': 3.823409885237325, 'val_loss_cos1': 0.8954675941182008, 'val_loss_top10%': 137.5391387939453, 'val_loss_bottom10%': 10.305497455596925, 'val_loss_max': 142.5851593017578, 'val_loss_min': 7.07394552230835, 'val_loss_top_decile': 134.02191162109375, 'val_loss_bottom_decile': 19.565603256225586, 'val_loss_std': 41.60336427205757, 'val_total': 124.9622641509434, 'test_total': 40.0, 'val_avg_loss_std': 0.040985674633729104, 'val_avg_loss_bottom_decile': 0.6403905487060547, 'val_avg_loss_top_decile': 0.7112597729786333, 'val_avg_loss_min': 0.5939437321254185, 'val_avg_loss_max': 0.8836186148903586, 'val_avg_loss_bottom10%': 0.6126706189388568, 'val_avg_loss_top10%': 0.7468951707184073, 'val_avg_loss_cos1': 0.9981537621892995, 'val_avg_loss_entropy': 3.968513687578216, 'val_acc_std': 0.06320071282710552, 'val_acc_bottom_decile': 0.5181818181818182, 'val_acc_top_decile': 0.6504065040650406, 'val_acc_min': 0.36363636363636365, 'val_acc_max': 0.8181818181818182, 'val_acc_bottom10%': 0.47311414051786727, 'val_acc_top10%': 0.6914037817961466, 'val_acc_cos1': 0.9943882975340717, 'val_acc_entropy': 3.9645062089927574, 'test_loss_std': 2.518742050761679, 'test_loss_bottom_decile': 24.110645294189453, 'test_loss_top_decile': 30.644969940185547, 'test_loss_min': 23.190872192382812, 'test_loss_max': 32.986793518066406, 'test_loss_bottom10%': 23.68499298095703, 'test_loss_top10%': 31.627936045328777, 'test_loss_cos1': 0.9956682231688659, 'test_loss_entropy': 3.9659950765257754, 'test_avg_loss_std': 0.06296855126904198, 'test_avg_loss_bottom_decile': 0.6027661323547363, 'test_avg_loss_top_decile': 0.7661242485046387, 'test_avg_loss_min': 0.5797718048095704, 'test_avg_loss_max': 0.8246698379516602, 'test_avg_loss_bottom10%': 0.5921248245239258, 'test_avg_loss_top10%': 0.7906984011332194, 'test_avg_loss_cos1': 0.9956682231688658, 'test_avg_loss_entropy': 3.96599507653813, 'test_acc_std': 0.07842397846780286, 'test_acc_bottom_decile': 0.475, 'test_acc_top_decile': 0.675, 'test_acc_min': 0.425, 'test_acc_max': 0.725, 'test_acc_bottom10%': 0.45499999999999996, 'test_acc_top10%': 0.7083333333333334, 'test_acc_cos1': 0.9913387410101133, 'test_acc_entropy': 3.9613791720803175}}
2025-09-10 12:15:27 (federatedscope.core.workers.server:518) INFO: Server: Final evaluation is finished! Starting merging results.
2025-09-10 12:15:27 (federatedscope.core.workers.server:644) INFO: {'Role': 'Server #', 'Round': 'Final', 'Results_raw': {'client_best_individual': {'val_loss': 7.07394552230835, 'val_total': 11.0, 'val_avg_loss': 0.5939437321254185, 'val_acc': 0.8181818181818182, 'test_total': 40.0, 'test_loss': 23.190872192382812, 'test_avg_loss': 0.5797718048095704, 'test_acc': 0.725}, 'client_summarized_weighted_avg': {'val_loss': 104.39527114683564, 'val_total': 124.9622641509434, 'val_avg_loss': 0.669754723020862, 'val_acc': 0.5953495394836177, 'test_total': 40.0, 'test_loss': 26.972495636850034, 'test_avg_loss': 0.6743123909212508, 'test_acc': 0.5919811320754716}, 'client_summarized_avg': {'val_loss': 83.69406661447489, 'val_total': 124.9622641509434, 'val_avg_loss': 0.6735522063028047, 'val_acc': 0.5940538357390138, 'test_total': 40.0, 'test_loss': 26.972495636850034, 'test_avg_loss': 0.6743123909212508, 'test_acc': 0.5919811320754719}, 'client_summarized_fairness': {'val_loss_entropy': 3.823409885237325, 'val_loss_cos1': 0.8954675941182008, 'val_loss_top10%': 137.5391387939453, 'val_loss_bottom10%': 10.305497455596925, 'val_loss_max': 142.5851593017578, 'val_loss_min': 7.07394552230835, 'val_loss_top_decile': 134.02191162109375, 'val_loss_bottom_decile': 19.565603256225586, 'val_loss_std': 41.60336427205757, 'val_total': 124.9622641509434, 'test_total': 40.0, 'val_avg_loss_std': 0.040985674633729104, 'val_avg_loss_bottom_decile': 0.6403905487060547, 'val_avg_loss_top_decile': 0.7112597729786333, 'val_avg_loss_min': 0.5939437321254185, 'val_avg_loss_max': 0.8836186148903586, 'val_avg_loss_bottom10%': 0.6126706189388568, 'val_avg_loss_top10%': 0.7468951707184073, 'val_avg_loss_cos1': 0.9981537621892995, 'val_avg_loss_entropy': 3.968513687578216, 'val_acc_std': 0.06320071282710552, 'val_acc_bottom_decile': 0.5181818181818182, 'val_acc_top_decile': 0.6504065040650406, 'val_acc_min': 0.36363636363636365, 'val_acc_max': 0.8181818181818182, 'val_acc_bottom10%': 0.47311414051786727, 'val_acc_top10%': 0.6914037817961466, 'val_acc_cos1': 0.9943882975340717, 'val_acc_entropy': 3.9645062089927574, 'test_loss_std': 2.518742050761679, 'test_loss_bottom_decile': 24.110645294189453, 'test_loss_top_decile': 30.644969940185547, 'test_loss_min': 23.190872192382812, 'test_loss_max': 32.986793518066406, 'test_loss_bottom10%': 23.68499298095703, 'test_loss_top10%': 31.627936045328777, 'test_loss_cos1': 0.9956682231688659, 'test_loss_entropy': 3.9659950765257754, 'test_avg_loss_std': 0.06296855126904198, 'test_avg_loss_bottom_decile': 0.6027661323547363, 'test_avg_loss_top_decile': 0.7661242485046387, 'test_avg_loss_min': 0.5797718048095704, 'test_avg_loss_max': 0.8246698379516602, 'test_avg_loss_bottom10%': 0.5921248245239258, 'test_avg_loss_top10%': 0.7906984011332194, 'test_avg_loss_cos1': 0.9956682231688658, 'test_avg_loss_entropy': 3.96599507653813, 'test_acc_std': 0.07842397846780286, 'test_acc_bottom_decile': 0.475, 'test_acc_top_decile': 0.675, 'test_acc_min': 0.425, 'test_acc_max': 0.725, 'test_acc_bottom10%': 0.45499999999999996, 'test_acc_top10%': 0.7083333333333334, 'test_acc_cos1': 0.9913387410101133, 'test_acc_entropy': 3.9613791720803175}}}
2025-09-10 12:15:27 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #1', 'Round': 1, 'Results_raw': {'val_total': 146, 'val_loss': 96.82806396484375, 'val_avg_loss': 0.663205917567423, 'val_acc': 0.6027397260273972, 'test_total': 40, 'test_loss': 23.190872192382812, 'test_avg_loss': 0.5797718048095704, 'test_acc': 0.65}}
2025-09-10 12:15:27 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #2', 'Round': 1, 'Results_raw': {'val_total': 11, 'val_loss': 9.719804763793945, 'val_avg_loss': 0.8836186148903586, 'val_acc': 0.36363636363636365, 'test_total': 40, 'test_loss': 27.738035202026367, 'test_avg_loss': 0.6934508800506591, 'test_acc': 0.525}}
2025-09-10 12:15:27 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #3', 'Round': 1, 'Results_raw': {'val_total': 36, 'val_loss': 26.420764923095703, 'val_avg_loss': 0.7339101367526584, 'val_acc': 0.5, 'test_total': 40, 'test_loss': 28.402244567871094, 'test_avg_loss': 0.7100561141967774, 'test_acc': 0.575}}
2025-09-10 12:15:27 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #4', 'Round': 1, 'Results_raw': {'val_total': 11, 'val_loss': 7.07394552230835, 'val_avg_loss': 0.6430859565734863, 'val_acc': 0.6363636363636364, 'test_total': 40, 'test_loss': 31.09689712524414, 'test_avg_loss': 0.7774224281311035, 'test_acc': 0.45}}
2025-09-10 12:15:27 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #5', 'Round': 1, 'Results_raw': {'val_total': 14, 'val_loss': 9.031166076660156, 'val_avg_loss': 0.6450832911900112, 'val_acc': 0.6428571428571429, 'test_total': 40, 'test_loss': 24.361305236816406, 'test_avg_loss': 0.6090326309204102, 'test_acc': 0.675}}
2025-09-10 12:15:27 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #6', 'Round': 1, 'Results_raw': {'val_total': 134, 'val_loss': 87.18622589111328, 'val_avg_loss': 0.6506434767993529, 'val_acc': 0.6417910447761194, 'test_total': 40, 'test_loss': 30.16408920288086, 'test_avg_loss': 0.7541022300720215, 'test_acc': 0.525}}
2025-09-10 12:15:27 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #7', 'Round': 1, 'Results_raw': {'val_total': 57, 'val_loss': 37.26748275756836, 'val_avg_loss': 0.6538154869748835, 'val_acc': 0.5964912280701754, 'test_total': 40, 'test_loss': 24.277843475341797, 'test_avg_loss': 0.6069460868835449, 'test_acc': 0.675}}
2025-09-10 12:15:27 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #8', 'Round': 1, 'Results_raw': {'val_total': 69, 'val_loss': 45.597900390625, 'val_avg_loss': 0.6608391360960145, 'val_acc': 0.6086956521739131, 'test_total': 40, 'test_loss': 30.644969940185547, 'test_avg_loss': 0.7661242485046387, 'test_acc': 0.475}}
2025-09-10 12:15:27 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #9', 'Round': 1, 'Results_raw': {'val_total': 188, 'val_loss': 126.51603698730469, 'val_avg_loss': 0.6729576435494931, 'val_acc': 0.5851063829787234, 'test_total': 40, 'test_loss': 26.840139389038086, 'test_avg_loss': 0.6710034847259522, 'test_acc': 0.625}}
2025-09-10 12:15:27 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #10', 'Round': 1, 'Results_raw': {'val_total': 63, 'val_loss': 41.42879867553711, 'val_avg_loss': 0.6575999789767795, 'val_acc': 0.6349206349206349, 'test_total': 40, 'test_loss': 25.548015594482422, 'test_avg_loss': 0.6387003898620606, 'test_acc': 0.6}}
2025-09-10 12:15:27 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #11', 'Round': 1, 'Results_raw': {'val_total': 32, 'val_loss': 19.565603256225586, 'val_avg_loss': 0.6114251017570496, 'val_acc': 0.625, 'test_total': 40, 'test_loss': 29.473003387451172, 'test_avg_loss': 0.7368250846862793, 'test_acc': 0.475}}
2025-09-10 12:15:27 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #12', 'Round': 1, 'Results_raw': {'val_total': 137, 'val_loss': 91.30337524414062, 'val_avg_loss': 0.6664479944827782, 'val_acc': 0.5912408759124088, 'test_total': 40, 'test_loss': 25.50849151611328, 'test_avg_loss': 0.6377122879028321, 'test_acc': 0.675}}
2025-09-10 12:15:27 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #13', 'Round': 1, 'Results_raw': {'val_total': 72, 'val_loss': 47.24960708618164, 'val_avg_loss': 0.6562445428636339, 'val_acc': 0.5972222222222222, 'test_total': 40, 'test_loss': 25.553752899169922, 'test_avg_loss': 0.6388438224792481, 'test_acc': 0.65}}
2025-09-10 12:15:27 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #14', 'Round': 1, 'Results_raw': {'val_total': 160, 'val_loss': 105.37844848632812, 'val_avg_loss': 0.6586153030395507, 'val_acc': 0.6125, 'test_total': 40, 'test_loss': 23.685222625732422, 'test_avg_loss': 0.5921305656433106, 'test_acc': 0.725}}
2025-09-10 12:15:27 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #15', 'Round': 1, 'Results_raw': {'val_total': 200, 'val_loss': 133.65231323242188, 'val_avg_loss': 0.6682615661621094, 'val_acc': 0.64, 'test_total': 40, 'test_loss': 27.865768432617188, 'test_avg_loss': 0.6966442108154297, 'test_acc': 0.575}}
2025-09-10 12:15:27 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #16', 'Round': 1, 'Results_raw': {'val_total': 136, 'val_loss': 88.54768371582031, 'val_avg_loss': 0.6510859096751493, 'val_acc': 0.625, 'test_total': 40, 'test_loss': 27.744915008544922, 'test_avg_loss': 0.6936228752136231, 'test_acc': 0.625}}
2025-09-10 12:15:27 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #17', 'Round': 1, 'Results_raw': {'val_total': 200, 'val_loss': 128.07810974121094, 'val_avg_loss': 0.6403905487060547, 'val_acc': 0.64, 'test_total': 40, 'test_loss': 27.43524932861328, 'test_avg_loss': 0.685881233215332, 'test_acc': 0.575}}
2025-09-10 12:15:27 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #18', 'Round': 1, 'Results_raw': {'val_total': 135, 'val_loss': 93.26004791259766, 'val_avg_loss': 0.6908151697229457, 'val_acc': 0.6222222222222222, 'test_total': 40, 'test_loss': 24.197376251220703, 'test_avg_loss': 0.6049344062805175, 'test_acc': 0.65}}
2025-09-10 12:15:27 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #19', 'Round': 1, 'Results_raw': {'val_total': 110, 'val_loss': 74.88877868652344, 'val_avg_loss': 0.6808070789683949, 'val_acc': 0.5909090909090909, 'test_total': 40, 'test_loss': 31.449981689453125, 'test_avg_loss': 0.7862495422363281, 'test_acc': 0.5}}
2025-09-10 12:15:27 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #20', 'Round': 1, 'Results_raw': {'val_total': 126, 'val_loss': 86.68260192871094, 'val_avg_loss': 0.6879571581643725, 'val_acc': 0.5476190476190477, 'test_total': 40, 'test_loss': 25.159351348876953, 'test_avg_loss': 0.6289837837219239, 'test_acc': 0.725}}
2025-09-10 12:15:27 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #21', 'Round': 1, 'Results_raw': {'val_total': 153, 'val_loss': 101.79824829101562, 'val_avg_loss': 0.6653480280458538, 'val_acc': 0.6143790849673203, 'test_total': 40, 'test_loss': 30.286346435546875, 'test_avg_loss': 0.7571586608886719, 'test_acc': 0.45}}
2025-09-10 12:15:27 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #22', 'Round': 1, 'Results_raw': {'val_total': 11, 'val_loss': 7.6333417892456055, 'val_avg_loss': 0.6939401626586914, 'val_acc': 0.8181818181818182, 'test_total': 40, 'test_loss': 29.54926109313965, 'test_avg_loss': 0.7387315273284912, 'test_acc': 0.55}}
2025-09-10 12:15:27 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #23', 'Round': 1, 'Results_raw': {'val_total': 30, 'val_loss': 18.069229125976562, 'val_avg_loss': 0.6023076375325521, 'val_acc': 0.6666666666666666, 'test_total': 40, 'test_loss': 24.2200984954834, 'test_avg_loss': 0.6055024623870849, 'test_acc': 0.6}}
2025-09-10 12:15:27 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #24', 'Round': 1, 'Results_raw': {'val_total': 200, 'val_loss': 134.02191162109375, 'val_avg_loss': 0.6701095581054688, 'val_acc': 0.585, 'test_total': 40, 'test_loss': 30.22330093383789, 'test_avg_loss': 0.7555825233459472, 'test_acc': 0.525}}
2025-09-10 12:15:27 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #25', 'Round': 1, 'Results_raw': {'val_total': 200, 'val_loss': 142.5851593017578, 'val_avg_loss': 0.7129257965087891, 'val_acc': 0.535, 'test_total': 40, 'test_loss': 32.601844787597656, 'test_avg_loss': 0.8150461196899415, 'test_acc': 0.525}}
2025-09-10 12:15:27 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #26', 'Round': 1, 'Results_raw': {'val_total': 161, 'val_loss': 100.69990539550781, 'val_avg_loss': 0.6254652509037752, 'val_acc': 0.6645962732919255, 'test_total': 40, 'test_loss': 24.701393127441406, 'test_avg_loss': 0.6175348281860351, 'test_acc': 0.675}}
2025-09-10 12:15:27 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #27', 'Round': 1, 'Results_raw': {'val_total': 123, 'val_loss': 81.16598510742188, 'val_avg_loss': 0.6598860577839177, 'val_acc': 0.6504065040650406, 'test_total': 40, 'test_loss': 26.890369415283203, 'test_avg_loss': 0.6722592353820801, 'test_acc': 0.575}}
2025-09-10 12:15:27 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #28', 'Round': 1, 'Results_raw': {'val_total': 75, 'val_loss': 50.73725128173828, 'val_avg_loss': 0.6764966837565104, 'val_acc': 0.5066666666666667, 'test_total': 40, 'test_loss': 26.2418270111084, 'test_avg_loss': 0.65604567527771, 'test_acc': 0.65}}
2025-09-10 12:15:27 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #29', 'Round': 1, 'Results_raw': {'val_total': 200, 'val_loss': 135.84925842285156, 'val_avg_loss': 0.6792462921142578, 'val_acc': 0.535, 'test_total': 40, 'test_loss': 24.110645294189453, 'test_avg_loss': 0.6027661323547363, 'test_acc': 0.675}}
2025-09-10 12:15:27 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #30', 'Round': 1, 'Results_raw': {'val_total': 170, 'val_loss': 112.57561492919922, 'val_avg_loss': 0.6622094995835248, 'val_acc': 0.6058823529411764, 'test_total': 40, 'test_loss': 25.90439796447754, 'test_avg_loss': 0.6476099491119385, 'test_acc': 0.6}}
2025-09-10 12:15:27 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #31', 'Round': 1, 'Results_raw': {'val_total': 193, 'val_loss': 133.79209899902344, 'val_avg_loss': 0.6932233108757692, 'val_acc': 0.5699481865284974, 'test_total': 40, 'test_loss': 26.18739128112793, 'test_avg_loss': 0.6546847820281982, 'test_acc': 0.575}}
2025-09-10 12:15:27 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #32', 'Round': 1, 'Results_raw': {'val_total': 112, 'val_loss': 66.52169799804688, 'val_avg_loss': 0.5939437321254185, 'val_acc': 0.6785714285714286, 'test_total': 40, 'test_loss': 28.103971481323242, 'test_avg_loss': 0.702599287033081, 'test_acc': 0.525}}
2025-09-10 12:15:27 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #33', 'Round': 1, 'Results_raw': {'val_total': 74, 'val_loss': 52.216739654541016, 'val_avg_loss': 0.7056316169532569, 'val_acc': 0.5945945945945946, 'test_total': 40, 'test_loss': 26.73493194580078, 'test_avg_loss': 0.6683732986450195, 'test_acc': 0.525}}
2025-09-10 12:15:27 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #34', 'Round': 1, 'Results_raw': {'val_total': 200, 'val_loss': 126.04227447509766, 'val_avg_loss': 0.6302113723754883, 'val_acc': 0.67, 'test_total': 40, 'test_loss': 23.42885398864746, 'test_avg_loss': 0.5857213497161865, 'test_acc': 0.725}}
2025-09-10 12:15:27 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #35', 'Round': 1, 'Results_raw': {'val_total': 200, 'val_loss': 128.19639587402344, 'val_avg_loss': 0.6409819793701171, 'val_acc': 0.62, 'test_total': 40, 'test_loss': 26.23986053466797, 'test_avg_loss': 0.6559965133666992, 'test_acc': 0.7}}
2025-09-10 12:15:27 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #36', 'Round': 1, 'Results_raw': {'val_total': 54, 'val_loss': 36.923828125, 'val_avg_loss': 0.6837745949074074, 'val_acc': 0.5555555555555556, 'test_total': 40, 'test_loss': 24.2669734954834, 'test_avg_loss': 0.606674337387085, 'test_acc': 0.7}}
2025-09-10 12:15:27 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #37', 'Round': 1, 'Results_raw': {'val_total': 200, 'val_loss': 137.10171508789062, 'val_avg_loss': 0.6855085754394531, 'val_acc': 0.55, 'test_total': 40, 'test_loss': 26.103912353515625, 'test_avg_loss': 0.6525978088378906, 'test_acc': 0.625}}
2025-09-10 12:15:27 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #38', 'Round': 1, 'Results_raw': {'val_total': 200, 'val_loss': 134.49026489257812, 'val_avg_loss': 0.6724513244628906, 'val_acc': 0.615, 'test_total': 40, 'test_loss': 26.324562072753906, 'test_avg_loss': 0.6581140518188476, 'test_acc': 0.6}}
2025-09-10 12:15:27 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #39', 'Round': 1, 'Results_raw': {'val_total': 83, 'val_loss': 59.03456115722656, 'val_avg_loss': 0.7112597729786333, 'val_acc': 0.5662650602409639, 'test_total': 40, 'test_loss': 29.964126586914062, 'test_avg_loss': 0.7491031646728515, 'test_acc': 0.5}}
2025-09-10 12:15:27 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #40', 'Round': 1, 'Results_raw': {'val_total': 200, 'val_loss': 129.17308044433594, 'val_avg_loss': 0.6458654022216797, 'val_acc': 0.62, 'test_total': 40, 'test_loss': 26.898456573486328, 'test_avg_loss': 0.6724614143371582, 'test_acc': 0.65}}
2025-09-10 12:15:27 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #41', 'Round': 1, 'Results_raw': {'val_total': 119, 'val_loss': 82.60572052001953, 'val_avg_loss': 0.6941657186556263, 'val_acc': 0.5714285714285714, 'test_total': 40, 'test_loss': 26.77128791809082, 'test_avg_loss': 0.6692821979522705, 'test_acc': 0.55}}
2025-09-10 12:15:27 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #42', 'Round': 1, 'Results_raw': {'val_total': 200, 'val_loss': 129.26531982421875, 'val_avg_loss': 0.6463265991210938, 'val_acc': 0.615, 'test_total': 40, 'test_loss': 27.370506286621094, 'test_avg_loss': 0.6842626571655274, 'test_acc': 0.65}}
2025-09-10 12:15:27 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #43', 'Round': 1, 'Results_raw': {'val_total': 89, 'val_loss': 61.06328582763672, 'val_avg_loss': 0.6861043351419857, 'val_acc': 0.5730337078651685, 'test_total': 40, 'test_loss': 26.150657653808594, 'test_avg_loss': 0.6537664413452149, 'test_acc': 0.575}}
2025-09-10 12:15:27 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #44', 'Round': 1, 'Results_raw': {'val_total': 200, 'val_loss': 141.1865234375, 'val_avg_loss': 0.7059326171875, 'val_acc': 0.55, 'test_total': 40, 'test_loss': 24.15829086303711, 'test_avg_loss': 0.6039572715759277, 'test_acc': 0.625}}
2025-09-10 12:15:27 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #45', 'Round': 1, 'Results_raw': {'val_total': 100, 'val_loss': 67.16244506835938, 'val_avg_loss': 0.6716244506835938, 'val_acc': 0.59, 'test_total': 40, 'test_loss': 29.472991943359375, 'test_avg_loss': 0.7368247985839844, 'test_acc': 0.5}}
2025-09-10 12:15:27 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #46', 'Round': 1, 'Results_raw': {'val_total': 110, 'val_loss': 79.93330383300781, 'val_avg_loss': 0.7266663984818892, 'val_acc': 0.5181818181818182, 'test_total': 40, 'test_loss': 24.092683792114258, 'test_avg_loss': 0.6023170948028564, 'test_acc': 0.675}}
2025-09-10 12:15:27 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #47', 'Round': 1, 'Results_raw': {'val_total': 147, 'val_loss': 103.46893310546875, 'val_avg_loss': 0.7038702932344812, 'val_acc': 0.5170068027210885, 'test_total': 40, 'test_loss': 24.027332305908203, 'test_avg_loss': 0.6006833076477051, 'test_acc': 0.625}}
2025-09-10 12:15:27 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #48', 'Round': 1, 'Results_raw': {'val_total': 46, 'val_loss': 32.79755401611328, 'val_avg_loss': 0.7129903046981149, 'val_acc': 0.4782608695652174, 'test_total': 40, 'test_loss': 28.123512268066406, 'test_avg_loss': 0.7030878067016602, 'test_acc': 0.5}}
2025-09-10 12:15:27 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #49', 'Round': 1, 'Results_raw': {'val_total': 132, 'val_loss': 85.844482421875, 'val_avg_loss': 0.6503369880445076, 'val_acc': 0.6439393939393939, 'test_total': 40, 'test_loss': 32.986793518066406, 'test_avg_loss': 0.8246698379516602, 'test_acc': 0.475}}
2025-09-10 12:15:27 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #50', 'Round': 1, 'Results_raw': {'val_total': 133, 'val_loss': 90.51524353027344, 'val_avg_loss': 0.6805657408291236, 'val_acc': 0.5488721804511278, 'test_total': 40, 'test_loss': 30.98712921142578, 'test_avg_loss': 0.7746782302856445, 'test_acc': 0.425}}
2025-09-10 12:15:27 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #51', 'Round': 1, 'Results_raw': {'val_total': 83, 'val_loss': 57.255859375, 'val_avg_loss': 0.6898296310240963, 'val_acc': 0.5783132530120482, 'test_total': 40, 'test_loss': 25.285465240478516, 'test_avg_loss': 0.6321366310119629, 'test_acc': 0.6}}
2025-09-10 12:15:27 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #52', 'Round': 1, 'Results_raw': {'val_total': 188, 'val_loss': 126.4284439086914, 'val_avg_loss': 0.6724917229185713, 'val_acc': 0.5797872340425532, 'test_total': 40, 'test_loss': 25.729602813720703, 'test_avg_loss': 0.6432400703430176, 'test_acc': 0.6}}
2025-09-10 12:15:27 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #53', 'Round': 1, 'Results_raw': {'val_total': 200, 'val_loss': 133.95309448242188, 'val_avg_loss': 0.6697654724121094, 'val_acc': 0.595, 'test_total': 40, 'test_loss': 25.06596565246582, 'test_avg_loss': 0.6266491413116455, 'test_acc': 0.675}}
2025-09-10 12:15:27 (federatedscope.core.monitors.monitor:268) INFO: In worker #0, the system-related metrics are: {'id': 0, 'fl_end_time_minutes': 193.61172945, 'total_model_size': 0, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 92208, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-10 12:15:27 (federatedscope.core.workers.client:842) INFO: ================= client 1 received finish message =================
2025-09-10 12:15:27 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:15:27 (federatedscope.core.monitors.monitor:268) INFO: In worker #1, the system-related metrics are: {'id': 1, 'fl_end_time_minutes': 193.61304936666664, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-10 12:15:27 (federatedscope.core.workers.client:842) INFO: ================= client 2 received finish message =================
2025-09-10 12:15:28 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:15:28 (federatedscope.core.monitors.monitor:268) INFO: In worker #2, the system-related metrics are: {'id': 2, 'fl_end_time_minutes': 193.55058433333335, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367552, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-10 12:15:28 (federatedscope.core.workers.client:842) INFO: ================= client 3 received finish message =================
2025-09-10 12:15:28 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:15:28 (federatedscope.core.monitors.monitor:268) INFO: In worker #3, the system-related metrics are: {'id': 3, 'fl_end_time_minutes': 193.50997719999998, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367552, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-10 12:15:28 (federatedscope.core.workers.client:842) INFO: ================= client 4 received finish message =================
2025-09-10 12:15:28 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:15:28 (federatedscope.core.monitors.monitor:268) INFO: In worker #4, the system-related metrics are: {'id': 4, 'fl_end_time_minutes': 193.46392683333335, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367552, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-10 12:15:28 (federatedscope.core.workers.client:842) INFO: ================= client 5 received finish message =================
2025-09-10 12:15:28 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:15:28 (federatedscope.core.monitors.monitor:268) INFO: In worker #5, the system-related metrics are: {'id': 5, 'fl_end_time_minutes': 193.4235117833333, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367552, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-10 12:15:28 (federatedscope.core.workers.client:842) INFO: ================= client 6 received finish message =================
2025-09-10 12:15:28 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:15:28 (federatedscope.core.monitors.monitor:268) INFO: In worker #6, the system-related metrics are: {'id': 6, 'fl_end_time_minutes': 193.37677985, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367552, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-10 12:15:28 (federatedscope.core.workers.client:842) INFO: ================= client 7 received finish message =================
2025-09-10 12:15:28 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:15:28 (federatedscope.core.monitors.monitor:268) INFO: In worker #7, the system-related metrics are: {'id': 7, 'fl_end_time_minutes': 193.3362517, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-10 12:15:28 (federatedscope.core.workers.client:842) INFO: ================= client 8 received finish message =================
2025-09-10 12:15:28 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:15:28 (federatedscope.core.monitors.monitor:268) INFO: In worker #8, the system-related metrics are: {'id': 8, 'fl_end_time_minutes': 193.28985511666667, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-10 12:15:28 (federatedscope.core.workers.client:842) INFO: ================= client 9 received finish message =================
2025-09-10 12:15:28 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:15:28 (federatedscope.core.monitors.monitor:268) INFO: In worker #9, the system-related metrics are: {'id': 9, 'fl_end_time_minutes': 193.24899158333335, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-10 12:15:28 (federatedscope.core.workers.client:842) INFO: ================= client 10 received finish message =================
2025-09-10 12:15:28 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:15:28 (federatedscope.core.monitors.monitor:268) INFO: In worker #10, the system-related metrics are: {'id': 10, 'fl_end_time_minutes': 193.20879828333332, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367552, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-10 12:15:28 (federatedscope.core.workers.client:842) INFO: ================= client 11 received finish message =================
2025-09-10 12:15:28 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:15:28 (federatedscope.core.monitors.monitor:268) INFO: In worker #11, the system-related metrics are: {'id': 11, 'fl_end_time_minutes': 193.16571770000002, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367552, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-10 12:15:28 (federatedscope.core.workers.client:842) INFO: ================= client 12 received finish message =================
2025-09-10 12:15:28 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:15:28 (federatedscope.core.monitors.monitor:268) INFO: In worker #12, the system-related metrics are: {'id': 12, 'fl_end_time_minutes': 193.12135608333335, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-10 12:15:28 (federatedscope.core.workers.client:842) INFO: ================= client 13 received finish message =================
2025-09-10 12:15:28 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:15:28 (federatedscope.core.monitors.monitor:268) INFO: In worker #13, the system-related metrics are: {'id': 13, 'fl_end_time_minutes': 193.08013795, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367552, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-10 12:15:28 (federatedscope.core.workers.client:842) INFO: ================= client 14 received finish message =================
2025-09-10 12:15:28 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:15:28 (federatedscope.core.monitors.monitor:268) INFO: In worker #14, the system-related metrics are: {'id': 14, 'fl_end_time_minutes': 193.0387336166667, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-10 12:15:28 (federatedscope.core.workers.client:842) INFO: ================= client 15 received finish message =================
2025-09-10 12:15:28 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:15:28 (federatedscope.core.monitors.monitor:268) INFO: In worker #15, the system-related metrics are: {'id': 15, 'fl_end_time_minutes': 192.99453393333334, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367552, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-10 12:15:28 (federatedscope.core.workers.client:842) INFO: ================= client 16 received finish message =================
2025-09-10 12:15:28 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:15:28 (federatedscope.core.monitors.monitor:268) INFO: In worker #16, the system-related metrics are: {'id': 16, 'fl_end_time_minutes': 192.95037361666667, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367552, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-10 12:15:28 (federatedscope.core.workers.client:842) INFO: ================= client 17 received finish message =================
2025-09-10 12:15:28 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:15:28 (federatedscope.core.monitors.monitor:268) INFO: In worker #17, the system-related metrics are: {'id': 17, 'fl_end_time_minutes': 192.90854338333332, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-10 12:15:28 (federatedscope.core.workers.client:842) INFO: ================= client 18 received finish message =================
2025-09-10 12:15:28 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:15:28 (federatedscope.core.monitors.monitor:268) INFO: In worker #18, the system-related metrics are: {'id': 18, 'fl_end_time_minutes': 192.86682486666666, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-10 12:15:28 (federatedscope.core.workers.client:842) INFO: ================= client 19 received finish message =================
2025-09-10 12:15:28 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:15:28 (federatedscope.core.monitors.monitor:268) INFO: In worker #19, the system-related metrics are: {'id': 19, 'fl_end_time_minutes': 192.82560651666665, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-10 12:15:28 (federatedscope.core.workers.client:842) INFO: ================= client 20 received finish message =================
2025-09-10 12:15:28 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:15:28 (federatedscope.core.monitors.monitor:268) INFO: In worker #20, the system-related metrics are: {'id': 20, 'fl_end_time_minutes': 192.76547043333332, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367552, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-10 12:15:28 (federatedscope.core.workers.client:842) INFO: ================= client 21 received finish message =================
2025-09-10 12:15:28 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:15:28 (federatedscope.core.monitors.monitor:268) INFO: In worker #21, the system-related metrics are: {'id': 21, 'fl_end_time_minutes': 192.71812218333335, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-10 12:15:28 (federatedscope.core.workers.client:842) INFO: ================= client 22 received finish message =================
2025-09-10 12:15:29 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:15:29 (federatedscope.core.monitors.monitor:268) INFO: In worker #22, the system-related metrics are: {'id': 22, 'fl_end_time_minutes': 192.67545721666664, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367552, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-10 12:15:29 (federatedscope.core.workers.client:842) INFO: ================= client 23 received finish message =================
2025-09-10 12:15:29 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:15:29 (federatedscope.core.monitors.monitor:268) INFO: In worker #23, the system-related metrics are: {'id': 23, 'fl_end_time_minutes': 192.63258161666667, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-10 12:15:29 (federatedscope.core.workers.client:842) INFO: ================= client 24 received finish message =================
2025-09-10 12:15:29 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:15:29 (federatedscope.core.monitors.monitor:268) INFO: In worker #24, the system-related metrics are: {'id': 24, 'fl_end_time_minutes': 192.58428486666665, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367552, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-10 12:15:29 (federatedscope.core.workers.client:842) INFO: ================= client 25 received finish message =================
2025-09-10 12:15:29 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:15:29 (federatedscope.core.monitors.monitor:268) INFO: In worker #25, the system-related metrics are: {'id': 25, 'fl_end_time_minutes': 192.54123375, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367552, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-10 12:15:29 (federatedscope.core.workers.client:842) INFO: ================= client 26 received finish message =================
2025-09-10 12:15:29 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:15:29 (federatedscope.core.monitors.monitor:268) INFO: In worker #26, the system-related metrics are: {'id': 26, 'fl_end_time_minutes': 192.49313605, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-10 12:15:29 (federatedscope.core.workers.client:842) INFO: ================= client 27 received finish message =================
2025-09-10 12:15:29 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:15:29 (federatedscope.core.monitors.monitor:268) INFO: In worker #27, the system-related metrics are: {'id': 27, 'fl_end_time_minutes': 192.45016286666666, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-10 12:15:29 (federatedscope.core.workers.client:842) INFO: ================= client 28 received finish message =================
2025-09-10 12:15:29 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:15:29 (federatedscope.core.monitors.monitor:268) INFO: In worker #28, the system-related metrics are: {'id': 28, 'fl_end_time_minutes': 192.40755015000002, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367552, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-10 12:15:29 (federatedscope.core.workers.client:842) INFO: ================= client 29 received finish message =================
2025-09-10 12:15:29 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:15:29 (federatedscope.core.monitors.monitor:268) INFO: In worker #29, the system-related metrics are: {'id': 29, 'fl_end_time_minutes': 192.364869, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367552, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-10 12:15:29 (federatedscope.core.workers.client:842) INFO: ================= client 30 received finish message =================
2025-09-10 12:15:29 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:15:29 (federatedscope.core.monitors.monitor:268) INFO: In worker #30, the system-related metrics are: {'id': 30, 'fl_end_time_minutes': 192.31727476666669, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-10 12:15:29 (federatedscope.core.workers.client:842) INFO: ================= client 31 received finish message =================
2025-09-10 12:15:29 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:15:29 (federatedscope.core.monitors.monitor:268) INFO: In worker #31, the system-related metrics are: {'id': 31, 'fl_end_time_minutes': 192.27438141666667, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367552, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-10 12:15:29 (federatedscope.core.workers.client:842) INFO: ================= client 32 received finish message =================
2025-09-10 12:15:29 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:15:29 (federatedscope.core.monitors.monitor:268) INFO: In worker #32, the system-related metrics are: {'id': 32, 'fl_end_time_minutes': 192.23197718333333, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-10 12:15:29 (federatedscope.core.workers.client:842) INFO: ================= client 33 received finish message =================
2025-09-10 12:15:29 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:15:29 (federatedscope.core.monitors.monitor:268) INFO: In worker #33, the system-related metrics are: {'id': 33, 'fl_end_time_minutes': 192.18889959999998, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367552, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-10 12:15:29 (federatedscope.core.workers.client:842) INFO: ================= client 34 received finish message =================
2025-09-10 12:15:29 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:15:29 (federatedscope.core.monitors.monitor:268) INFO: In worker #34, the system-related metrics are: {'id': 34, 'fl_end_time_minutes': 192.14063135, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-10 12:15:29 (federatedscope.core.workers.client:842) INFO: ================= client 35 received finish message =================
2025-09-10 12:15:29 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:15:29 (federatedscope.core.monitors.monitor:268) INFO: In worker #35, the system-related metrics are: {'id': 35, 'fl_end_time_minutes': 192.09796476666668, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-10 12:15:29 (federatedscope.core.workers.client:842) INFO: ================= client 36 received finish message =================
2025-09-10 12:15:29 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:15:29 (federatedscope.core.monitors.monitor:268) INFO: In worker #36, the system-related metrics are: {'id': 36, 'fl_end_time_minutes': 192.05533406666666, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367552, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-10 12:15:29 (federatedscope.core.workers.client:842) INFO: ================= client 37 received finish message =================
2025-09-10 12:15:29 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:15:29 (federatedscope.core.monitors.monitor:268) INFO: In worker #37, the system-related metrics are: {'id': 37, 'fl_end_time_minutes': 192.01337226666666, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-10 12:15:29 (federatedscope.core.workers.client:842) INFO: ================= client 38 received finish message =================
2025-09-10 12:15:29 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:15:29 (federatedscope.core.monitors.monitor:268) INFO: In worker #38, the system-related metrics are: {'id': 38, 'fl_end_time_minutes': 191.971438, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367552, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-10 12:15:29 (federatedscope.core.workers.client:842) INFO: ================= client 39 received finish message =================
2025-09-10 12:15:29 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:15:29 (federatedscope.core.monitors.monitor:268) INFO: In worker #39, the system-related metrics are: {'id': 39, 'fl_end_time_minutes': 191.92813218333333, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367552, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-10 12:15:29 (federatedscope.core.workers.client:842) INFO: ================= client 40 received finish message =================
2025-09-10 12:15:29 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:15:29 (federatedscope.core.monitors.monitor:268) INFO: In worker #40, the system-related metrics are: {'id': 40, 'fl_end_time_minutes': 191.86435170000001, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367552, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-10 12:15:29 (federatedscope.core.workers.client:842) INFO: ================= client 41 received finish message =================
2025-09-10 12:15:30 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:15:30 (federatedscope.core.monitors.monitor:268) INFO: In worker #41, the system-related metrics are: {'id': 41, 'fl_end_time_minutes': 191.82215368333334, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367552, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-10 12:15:30 (federatedscope.core.workers.client:842) INFO: ================= client 42 received finish message =================
2025-09-10 12:15:30 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:15:30 (federatedscope.core.monitors.monitor:268) INFO: In worker #42, the system-related metrics are: {'id': 42, 'fl_end_time_minutes': 191.77866143333333, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-10 12:15:30 (federatedscope.core.workers.client:842) INFO: ================= client 43 received finish message =================
2025-09-10 12:15:30 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:15:30 (federatedscope.core.monitors.monitor:268) INFO: In worker #43, the system-related metrics are: {'id': 43, 'fl_end_time_minutes': 191.73690523333335, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367552, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-10 12:15:30 (federatedscope.core.workers.client:842) INFO: ================= client 44 received finish message =================
2025-09-10 12:15:30 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:15:30 (federatedscope.core.monitors.monitor:268) INFO: In worker #44, the system-related metrics are: {'id': 44, 'fl_end_time_minutes': 191.696259, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-10 12:15:30 (federatedscope.core.workers.client:842) INFO: ================= client 45 received finish message =================
2025-09-10 12:15:30 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:15:30 (federatedscope.core.monitors.monitor:268) INFO: In worker #45, the system-related metrics are: {'id': 45, 'fl_end_time_minutes': 191.65467865, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367552, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-10 12:15:30 (federatedscope.core.workers.client:842) INFO: ================= client 46 received finish message =================
2025-09-10 12:15:30 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:15:30 (federatedscope.core.monitors.monitor:268) INFO: In worker #46, the system-related metrics are: {'id': 46, 'fl_end_time_minutes': 191.61252396666666, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367552, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-10 12:15:30 (federatedscope.core.workers.client:842) INFO: ================= client 47 received finish message =================
2025-09-10 12:15:30 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:15:30 (federatedscope.core.monitors.monitor:268) INFO: In worker #47, the system-related metrics are: {'id': 47, 'fl_end_time_minutes': 191.56606125000002, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367552, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-10 12:15:30 (federatedscope.core.workers.client:842) INFO: ================= client 48 received finish message =================
2025-09-10 12:15:30 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:15:30 (federatedscope.core.monitors.monitor:268) INFO: In worker #48, the system-related metrics are: {'id': 48, 'fl_end_time_minutes': 191.52403426666666, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367552, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-10 12:15:30 (federatedscope.core.workers.client:842) INFO: ================= client 49 received finish message =================
2025-09-10 12:15:30 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:15:30 (federatedscope.core.monitors.monitor:268) INFO: In worker #49, the system-related metrics are: {'id': 49, 'fl_end_time_minutes': 191.48174506666666, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-10 12:15:30 (federatedscope.core.workers.client:842) INFO: ================= client 50 received finish message =================
2025-09-10 12:15:30 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:15:30 (federatedscope.core.monitors.monitor:268) INFO: In worker #50, the system-related metrics are: {'id': 50, 'fl_end_time_minutes': 191.43948901666667, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-10 12:15:30 (federatedscope.core.workers.client:842) INFO: ================= client 51 received finish message =================
2025-09-10 12:15:30 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:15:30 (federatedscope.core.monitors.monitor:268) INFO: In worker #51, the system-related metrics are: {'id': 51, 'fl_end_time_minutes': 191.39740293333335, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367552, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-10 12:15:30 (federatedscope.core.workers.client:842) INFO: ================= client 52 received finish message =================
2025-09-10 12:15:30 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:15:30 (federatedscope.core.monitors.monitor:268) INFO: In worker #52, the system-related metrics are: {'id': 52, 'fl_end_time_minutes': 191.35516336666666, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-10 12:15:30 (federatedscope.core.workers.client:842) INFO: ================= client 53 received finish message =================
2025-09-10 12:15:30 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-10 12:15:30 (federatedscope.core.monitors.monitor:268) INFO: In worker #53, the system-related metrics are: {'id': 53, 'fl_end_time_minutes': 191.31294556666666, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 1367544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-10 12:15:30 (federatedscope.core.monitors.monitor:441) INFO: We will compress the file eval_results.raw into a .gz file, and delete the old one
2025-09-10 12:15:30 (federatedscope.core.monitors.monitor:359) INFO: After merging the system metrics from all works, we got avg: defaultdict(None, {'id': 'sys_avg', 'sys_avg/fl_end_time_minutes': 192.47555429691354, 'sys_avg/total_model_size': '478.65M', 'sys_avg/total_flops': '0.0', 'sys_avg/total_upload_bytes': '0.0', 'sys_avg/total_download_bytes': '1.28M', 'sys_avg/global_convergence_round': 0.0, 'sys_avg/local_convergence_round': 0.0, 'sys_avg/global_convergence_time_minutes': 0.0, 'sys_avg/local_convergence_time_minutes': 0.0})
2025-09-10 12:15:30 (federatedscope.core.monitors.monitor:360) INFO: After merging the system metrics from all works, we got std: defaultdict(None, {'id': 'sys_std', 'sys_std/fl_end_time_minutes': 0.6890668582040869, 'sys_std/total_model_size': '65.75M', 'sys_std/total_flops': '0.0', 'sys_std/total_upload_bytes': '0.0', 'sys_std/total_download_bytes': '167.91K', 'sys_std/global_convergence_round': 0.0, 'sys_std/local_convergence_round': 0.0, 'sys_std/global_convergence_time_minutes': 0.0, 'sys_std/local_convergence_time_minutes': 0.0})
