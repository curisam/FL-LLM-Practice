import gc
import torch
import torch.nn as nn
from collections import OrderedDict
from peft import get_peft_model, TaskType, PeftModel

import accelerate
from accelerate import dispatch_model, infer_auto_device_map, \
    load_checkpoint_and_dispatch
from accelerate.utils import get_balanced_memory

from transformers import (OPTForCausalLM, GPT2LMHeadModel, BloomForCausalLM,
                          LlamaForCausalLM, LlamaForSequenceClassification,
                          Qwen2ForCausalLM, GemmaForCausalLM)


from federatedscope.llm.misc.accel_utils import in_distributed_mode, allow_device_map_auto_by_env





MODEL_UNIT = {
    LlamaForCausalLM: ['LlamaDecoderLayer'],
    LlamaForSequenceClassification: ['LlamaDecoderLayer'],
    BloomForCausalLM: ['BloomBlock'],
    GPT2LMHeadModel: ['GPT2Block'],
    OPTForCausalLM: ['OPTDecoderLayer'],
    Qwen2ForCausalLM: ['Qwen2DecoderLayer'],
    GemmaForCausalLM: ['GemmaDecoderLayer']
}

import logging
import sys

sys.setrecursionlimit(100000)

logger = logging.getLogger(__name__)


def enable_adapter(model, package, adapter, **kwargs):#package: peft, adapter:loraë¡œ ë“¤ì–´ê°.
    adapter = adapter.lower()
    if package == 'peft':
        """
        PEFT: https://github.com/huggingface/peft
        Support methods:
            LoRA
            Prefix Tuning
            P-Tuning
            Prompt Tuning
            AdaLoRA
        """
        if adapter == 'lora': ########## ì´ê±°ì— í•´ë‹¹ ##############
            """
            get_peft_modelì´ ì›ë˜ LMì„ PeftModelë¡œ ê°ì‹¸ë©´ì„œ ì§€ì •ëœ ëª¨ë“ˆ(q/k/v/o, gate/up/down_proj)ì— 
            LoRA ì–´ëŒ‘í„°(A/B í–‰ë ¬)ë¥¼ â€œë¼ì›Œ ë„£ì—ˆê³ â€, ê·¸ ê²°ê³¼ state_dict í‚¤ì— base_model.model.   ...lora_A/B...ê°€ ìƒê¸°ê³ , 
            ë² ì´ìŠ¤ ê°€ì¤‘ì¹˜ëŠ” ë™ê²°, LoRA ê°€ì¤‘ì¹˜ë§Œ í•™ìŠµ/ì „ì†¡ ëŒ€ìƒìœ¼ë¡œ ë°”ë€ ê²ƒì´ì•¼.           
            
            """

            """
            ì•ë¶€ë¶„(prefix): PEFTê°€ ë˜í•‘í•˜ë©´ì„œ ê²½ë¡œì— base_model.model. ì´ ë¶™ì–´ìš”.

            LoRA íŒŒë¼ë¯¸í„°(suffix): íƒ€ê¹ƒ ëª¨ë“ˆ ì´ë¦„ ë’¤ì— .lora_A.<ì–´ëŒ‘í„°ì´ë¦„>.weight, **.lora_B.<ì–´ëŒ‘í„°ì´ë¦„>.weight**ê°€ ë¶™ì–´ìš”.
            ì–´ëŒ‘í„° ì´ë¦„ì„ ë”°ë¡œ ì•ˆ ì£¼ë©´ ê¸°ë³¸ì´ **default**ë¼ì„œ
            ...q_proj.lora_A.default.weight, ...q_proj.lora_B.default.weight ì´ëŸ° ì‹ìœ¼ë¡œ ìƒê¹ë‹ˆë‹¤.

            ì •ë¦¬ ì˜ˆì‹œ:

            ë² ì´ìŠ¤ ê°€ì¤‘ì¹˜(ë™ê²°):
            base_model.model.model.layers.0.self_attn.q_proj.weight

            LoRA ì¶”ê°€ ê°€ì¤‘ì¹˜(í•™ìŠµ):
            base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight
            base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight
            
            """

            """
            
            1. ì €ì¥/ì „ì†¡ ê´€ì 

            PEFTëŠ” ë³´í†µ LoRA ê°€ì¤‘ì¹˜ë§Œ ê°€ë³ê²Œ ì €ì¥/ê³µìœ í•  ìˆ˜ ìˆì–´.

            ë„¤ FL íŒŒì´í”„ë¼ì¸ì—ì„œ â€œí•™ìŠµ ê°€ëŠ¥í•œ ì´ë¦„ë§Œâ€ í•„í„°í•˜ë©´ ìì—°ìŠ¤ëŸ½ê²Œ
            *.lora_A.*, *.lora_B.*ë§Œ ì„œë²„ë¡œ ë³´ë‚´ê²Œ ë¨(ë² ì´ìŠ¤ëŠ” ë™ê²°ì´ë¯€ë¡œ ì œì™¸).


            2. í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°ê°€ ë°”ë€œ

            ê¸°ë³¸ ê°€ì¤‘ì¹˜(...weight, ...bias)ëŠ” ë™ê²°(requires_grad=False),
            LoRA A/Bë§Œ í•™ìŠµ(requires_grad=True).

            bias='none'ì´ë¯€ë¡œ biasëŠ” ì¡´ì¬í•˜ë˜ í•™ìŠµì€ ì•ˆ í•¨(ë™ê²° ìƒíƒœë¡œ ë‚¨ìŒ).
            
            """

            from peft import LoraConfig
            peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, **kwargs)
            #LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, inference_mode=False, r=8, target_modules={'o_proj', 'k_proj', 'v_proj', 'q_proj', 'down_proj', 'gate_proj', 'up_proj'}, lora_alpha=16, lora_dropout=0.05, fan_in_fan_out=False, bias='none', modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={})
            model = get_peft_model(model, peft_config) #PEFT í´ë˜ìŠ¤ ê°ì²´. state_dictë¥¼ í• ë•Œ backbone, active ì—¬ë¶€ì™€ ìƒê´€ì—†ì´ ë“±ë¡ëœ adapter ì „ë¶€ê°€ í¬í•¨ë˜ì–´ì„œ ë‚˜ì˜´.
            #ì´ ì¤‘ í•™ìŠµ ëŒ€ìƒì¸ ê²ƒ â†’ active adapterë§Œ requires_grad=Trueë¡  ë‚˜ì˜´.
            #ë§Œì•½ active adapterë§Œ ì €ì¥/ë¡œë“œí•˜ê³  ì‹¶ìœ¼ë©´: model.save_pretrained(save_directory, selected_adapters=["style_a"]) ğŸ‘‰ ì´ë ‡ê²Œ í•˜ë©´ ì§€ì •í•œ adapterë§Œ ë”°ë¡œ ì €ì¥ ê°€ëŠ¥.
        elif adapter == 'prefix':
            from peft import PrefixTuningConfig
            peft_config = PrefixTuningConfig(task_type=TaskType.CAUSAL_LM,
                                             **kwargs)
            model = get_peft_model(model, peft_config)
        elif adapter == 'prompt':
            from peft import PromptTuningConfig
            peft_config = PromptTuningConfig(task_type=TaskType.CAUSAL_LM,
                                             **kwargs)
            model = get_peft_model(model, peft_config)
        elif adapter == 'p-tuning':
            from peft import PromptEncoderConfig
            peft_config = PromptEncoderConfig(task_type=TaskType.CAUSAL_LM,
                                              **kwargs)
            model = get_peft_model(model, peft_config)
        else:
            raise NotImplementedError

        model.print_trainable_parameters() #trainable params: 4,399,104 || all params: 498,172,032 || trainable%: 0.8830491712549612 ì¶œë ¥ìœ¼ë¡œ ë.
        return model, peft_config

    if package == 'adapterhub':
        """
        AdapterHub: https://docs.adapterhub.ml/model_overview.html
        Support methods:
            Bottleneck Adapters
            Prefix Tuning
            LoRA
            Compacter
            Adapter Fusion
            Invertible Adapters
            Parallel block
        """
        # TODO:  After supporting adapterhub, we will move the following
        #   parameters in yaml file for users' convenient
        if adapter == 'lora':
            from transformers.adapters import LoRAConfig

            config = LoRAConfig(r=8, alpha=16)
            model.add_adapter("lora_adapter", config=config)
            model.train_adapter(['lora_adapter'])
        elif adapter == 'bottleneck':
            from transformers.adapters import AdapterConfig

            config = AdapterConfig(mh_adapter=True,
                                   output_adapter=True,
                                   reduction_factor=16,
                                   non_linearity="relu")
            model.add_adapter("bottleneck_adapter", config=config)
            model.train_adapter(['bottleneck_adapter'])
        elif adapter == 'lang':
            from transformers.adapters import PfeifferInvConfig

            config = PfeifferInvConfig()
            model.add_adapter("lang_adapter", config=config)
            model.train_adapter(['lang_adapter'])
        elif adapter == 'prefix':
            from transformers.adapters import PrefixTuningConfig

            config = PrefixTuningConfig(flat=False, prefix_length=30)
            model.add_adapter("prefix_tuning", config=config)
            model.train_adapter(['prefix_tuning'])
        elif adapter == 'compacter':
            from transformers.adapters import CompacterConfig

            config = CompacterConfig()
            model.add_adapter("dummy", config=config)
            model.train_adapter(['dummy'])
        elif adapter == 'ia_3':
            from transformers.adapters import IA3Config

            config = IA3Config()
            model.add_adapter("ia3_adapter", config=config)
            model.train_adapter(['ia3_adapter'])
        elif adapter == 'union':
            from transformers.adapters import AdapterConfig, ConfigUnion

            # TODO: configure these args in cfg
            config = ConfigUnion(
                AdapterConfig(mh_adapter=True,
                              output_adapter=False,
                              reduction_factor=16,
                              non_linearity="relu"),
                AdapterConfig(mh_adapter=False,
                              output_adapter=True,
                              reduction_factor=2,
                              non_linearity="relu"),
            )
            model.add_adapter("union_adapter", config=config)
            model.train_adapter(['union_adapter'])
        elif adapter == 'mam':
            from transformers.adapters import \
                ConfigUnion, ParallelConfig, PrefixTuningConfig

            config = ConfigUnion(
                PrefixTuningConfig(bottleneck_size=800),
                ParallelConfig(),
            )
            model.add_adapter("mam_adapter", config=config)
            model.train_adapter(['mam_adapter'])
        else:
            raise NameError(
                f"There is no adapter named {adapter} in {package}")
        return model, config

    raise NotImplementedError


class AdapterModel(nn.Module):
    def __init__(self, model, use_adapter=False, *args, **kwargs):
        super().__init__()



        self.model = None
        try:
            self.model_unit = MODEL_UNIT[type(model)]
        except:
            self.model_unit = None

        if use_adapter:
            adapter_package = kwargs.pop('adapter_package', 'peft') #peft
            adapter_method = kwargs.pop('adapter_method', 'lora') #lora

            self.model, self.peft_config = \
                enable_adapter(model,
                               adapter_package,
                               adapter_method,
                               **kwargs) #self.model: PEFT í´ë˜ìŠ¤.
            self.adapter_names = ['default']

            # â˜… ì¶”ê°€: ì‹œì‘í•  ë•Œë¶€í„° ë¹„í™œì„± ì–´ëŒ‘í„°ëŠ” CPUë¡œ ë‚´ë¦¬ê¸°
            self.set_active_adapter('default')


        else:
            self.model = model 


    def get_input_embeddings(self):
        return self.model.get_input_embeddings() #self.modelì€ PEFT í´ë˜ìŠ¤

    def forward(self, disable_adapter=False, *args, **kwargs): #LoRA ì˜í–¥ ì—†ì´ ë² ì´ìŠ¤ ì„±ëŠ¥ì„ ë³´ê³  ì‹¶ê±°ë‚˜, íŠ¹ì • í‰ê°€(í—¬ë¦„/ì •í•©ì„± ë“±)ì—ì„œ ì–´ëŒ‘í„° íš¨ê³¼ë¥¼ ë°°ì œí•˜ê³  ì‹¶ì„ ë•Œ ìœ ìš©.
        if isinstance(self.model, PeftModel) and disable_adapter: #disable_adapter=True: ë² ì´ìŠ¤ë¡œë§Œ forward.
            with self.model.disable_adapter():
                return self.model(*args, **kwargs)

        return self.model.forward(*args, **kwargs) #self.modelì€ PEFT í´ë˜ìŠ¤

    def generate(self, disable_adapter=False, *args, **kwargs): #ì¼ë‹¨ pass. ì´ê²ƒë„ LoRA ì˜í–¥ ì—†ì´ ë² ì´ìŠ¤ ì„±ëŠ¥ì„ ë³´ê³  ì‹¶ê±°ë‚˜, íŠ¹ì • í‰ê°€(í—¬ë¦„/ì •í•©ì„± ë“±)ì—ì„œ ì–´ëŒ‘í„° íš¨ê³¼ë¥¼ ë°°ì œí•˜ê³  ì‹¶ì„ ë•Œ ìœ ìš©.
        try:
            if isinstance(self.model, PeftModel) and disable_adapter:
                with self.model.disable_adapter():
                    res = self.model.generate(*args, **kwargs)

            else:
                res = self.model.generate(*args, **kwargs) #self.modelì€ PEFT í´ë˜ìŠ¤
        except RuntimeError as e:
            # When does evaluation in HELM,
            # half precision will cause RuntimeError,
            # the following solves it
            if 'do_sample' in kwargs.keys():
                del kwargs['do_sample']
                if isinstance(self.model, PeftModel) and disable_adapter:
                    with self.model.disable_adapter():
                        res = self.model.generate(*args, **kwargs)
                else:
                    res = self.model.generate(*args, **kwargs) #self.modelì€ PEFT í´ë˜ìŠ¤
            else:
                raise RuntimeError(e) 
        return res

    #í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ë§Œ(ëŒ€ë¶€ë¶„ LoRA) ì¶”ë ¤ì„œ ë°˜í™˜.
    def state_dict(self, return_trainable=True, *args, **kwargs): #ê¸°ì¡´ PEFT í´ë˜ìŠ¤: backbone + ë¹„í™œì„± í¬í•¨í•œ ëª¨ë“  adapter ê°€ì¤‘ì¹˜ í¬í•¨ í•˜ì—¬ ë°˜í™˜.
        if return_trainable:
            return self.get_trainable_state_dict() #requires_grad=Trueì¸ íŒŒë¼ë¯¸í„° í¬í•¨ì¸ ê²ƒ í˜¹ì€ self.adapter_namesì— ìˆëŠ” ì–´ëŒ‘í„° ì´ë¦„ì´ íŒŒë¼ë¯¸í„° ì´ë¦„ ë¬¸ìì—´ì— í¬í•¨ë˜ë©´, requires_grad=Falseë¼ë„ í¬í•¨.
        return self.model.state_dict(*args, **kwargs) #backbone, ëª¨ë“  LoRA adapter  ì „ë¶€ ë‚´ë³´ëƒ„. ì´ ìì²´ë¡œë„ â€œí’€ ê°€ì¤‘ì¹˜(ë°±ë³¸+ì–´ëŒ‘í„°) ì €ì¥ ê°€ëŠ¥.

        # âœ PeftModelì˜ ì „ì²´ state_dictë¥¼ ê·¸ëŒ€ë¡œ ë°˜í™˜ (ë°±ë³¸ + ëª¨ë“  ì–´ëŒ‘í„° í‚¤ í¬í•¨). ì¦‰, PEFT êµ¬ì¡°ë¥¼ ìœ ì§€í•œ  'í’€ ê°€ì¤‘ì¹˜' ì²´í¬í¬ì¸íŠ¸ë¥¼ ì €ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì´ìœ ë¡œ PEFT ëª¨ë¸ í´ë˜ìŠ¤ì—ë§Œ ëª¨ë¸ ì—…ë¡œë“œê°€ ê°€ëŠ¥. LoRA í‚¤ê°€ ì‚¬ë¼ì§„ â€œìˆœìˆ˜ HF ëª¨ë¸â€ì—ëŠ” ì–´ëŒ‘í„°ê°€ ì—…ë¡œë“œê°€ ë ìˆ˜ê°€ ì—†ëŠ” ìƒí™©.

        #    ë§Œì•½ LoRAë¥¼ ë² ì´ìŠ¤ì— ì‹¤ì œë¡œ í•©ì³ì„œ(merge) 'ìˆœìˆ˜ HF ëª¨ë¸' ì²´í¬í¬ì¸íŠ¸ë¥¼ ë§Œë“¤ê³  ì‹¶ë‹¤ë©´,
        #    save_model(..., merge_adapter=True) ë˜ëŠ” copy.deepcopy í›„ merge_and_unload().state_dict()ì„ ì‚¬ìš©í•  ê²ƒ.
        #    (merge í›„ì—ëŠ” lora_* í‚¤ê°€ ì‚¬ë¼ì§€ê³ , PEFT ì—†ì´ë„ AutoModelForCausalLM.from_pretrained(...)ë¡œ ë°”ë¡œ ë¡œë“œ ê°€ëŠ¥


        ### ì˜ˆì‹œ
        ### merge ì „(PEFT êµ¬ì¡° ìœ ì§€):
        """
        base_model.model.layers.0.self_attn.q_proj.weight          # ë² ì´ìŠ¤
        base_model.model.layers.0.self_attn.q_proj.lora_A.default.weight
        base_model.model.layers.0.self_attn.q_proj.lora_B.default.weight        
        """

        ### merge í›„(ìˆœìˆ˜ HF):
        """
        model.layers.0.self_attn.q_proj.weight                     # LoRAê°€ í•©ì‚°ë˜ì–´ ë°˜ì˜ë¨
        lora_A/B í‚¤ëŠ” ì—†ìŒ
        """

    def load_state_dict(self, state_dict, strict=False):
        return self.model.load_state_dict(state_dict, strict=False) #ì „ë‹¬ë°›ì€ state_dictë¥¼ ë¶€ë¶„ ë¡œë”©(missing/unexpected í—ˆìš©)ìœ¼ë¡œ ì£¼ì…. self.modelì€ PEFT í´ë˜ìŠ¤ë¼ .state_dictë¡œ ì–»ì–´ì§„  ë°”íƒ•ìœ¼ë¡œ ëª¨ë¸ì´ ì—…ë¡œë“œ ì˜ ë¨.


    def get_trainable_state_dict(self): #í˜„ì¬ ëª¨ë¸(self.model)ì—ì„œ ì „ì†¡/ì €ì¥ì„ ìœ„í•œ í•™ìŠµ ëŒ€ìƒ íŒŒë¼ë¯¸í„°ë§Œ ë½‘ì•„ OrderedDictë¡œ ë°˜í™˜.
        """
        ê¸°ë³¸ ê·œì¹™:

        requires_grad=Trueì¸ íŒŒë¼ë¯¸í„° í¬í•¨ (ë³´í†µ LoRA A/B, modules_to_save ë“±)

        ë©€í‹° ì–´ëŒ‘í„° ë³´ì •: self.adapter_namesì— ìˆëŠ” ì–´ëŒ‘í„° ì´ë¦„ì´ íŒŒë¼ë¯¸í„° ì´ë¦„ ë¬¸ìì—´ì— í¬í•¨ë˜ë©´, requires_grad=Falseë¼ë„ í¬í•¨. í™œì„±í™”ë˜ì§€ ì•Šì€ ì–´ëŒ‘í„°ì˜ íŒŒë¼ë¯¸í„°ê¹Œì§€ í¬í•¨ ê°€ëŠ¥.       
        """
        grad_params = []
        for name, param in self.model.named_parameters():
            if param.requires_grad: #1ì°¨ ê¸°ì¤€: requires_grad=Trueì¸ íŒŒë¼ë¯¸í„°ë§Œ ìˆ˜ì§‘. â†’ LoRA A/B, modules_to_save ë“±ì´ ì—¬ê¸°ì— ë“¤ì–´ê°.
                grad_params.append(name)
            # Special case for multiple adapters
            for adap_name in self.adapter_names: #ë©€í‹° ì–´ëŒ‘í„° ë³´ì •: íŠ¹ì • ì–´ëŒ‘í„° ì´ë¦„ì´ ê²½ë¡œì— ë“¤ì–´ìˆìœ¼ë©´ requires_grad=Falseì—¬ë„ í¬í•¨.

                #ì´ìœ : ë¼ìš´ë“œ/ìƒí™©ì— ë”°ë¼ ì–´ë–¤ ì–´ëŒ‘í„°ëŠ” ë¹„í™œì„±(ë™ê²°) ìƒíƒœì—¬ë„ ì „ì†¡/ë³´ê´€ì´ í•„ìš”í•  ìˆ˜ ìˆìŒ.
                if (adap_name in name) and (name not in grad_params): #param.requires_grad=False but adap_nameì€ ìˆëŠ” ê²½ìš°.
                    grad_params.append(name)
                    break #breakë¡œ ì¤‘ë³µ ë°©ì§€. self.adapter_names ì¤‘ í•˜ë‚˜ì—ì„œ appendë˜ì—ˆìœ¼ë©´ ë‹¤ë¥¸ê±´ ê·¸ëƒ¥ ë°”ë¡œ pass.

        model_state_dict = self.model.state_dict()
        new_state_dict = OrderedDict()
        for k, v in model_state_dict.items():
            if k in grad_params:
                new_state_dict[k] = v
        return new_state_dict

    def save_model(self,
                   path,
                   state=0,
                   merge_adapter=False,
                   return_trainable=True): #ì²´í¬í¬ì¸íŠ¸ë¥¼ íŒŒì¼ë¡œ ì €ì¥. 
        if merge_adapter and isinstance(self.model, PeftModel):
            merged_model = self.model.merge_and_unload() #LoRAë¥¼ ë² ì´ìŠ¤ì— í•©ì³ ìˆœìˆ˜ HF ëª¨ë¸ ê°€ì¤‘ì¹˜ë¡œ ì €ì¥
            ckpt = {'cur_round': state, 'model': merged_model.state_dict()}
        elif return_trainable:
            ckpt = {'cur_round': state, 'model': self.state_dict()} #í•™ìŠµ ëŒ€ìƒë§Œ ì €ì¥ (get_trainable_state_dict() ê²½ìœ )
        else:
            ckpt = {'cur_round': state, 'model': self.model.state_dict()} #self.model.state_dict() ê·¸ëŒ€ë¡œ ì €ì¥
        torch.save(ckpt, path)



    def sharding(self): #Accelerateì˜ infer_auto_device_map + dispatch_modelë¥¼ ì´ìš©í•´ ë‹¨ì¼ ë…¸ë“œ ë‹¤ì¤‘ GPUì—ì„œ ëª¨ë¸ì„ ë””ë°”ì´ìŠ¤ë³„ë¡œ ìë™ ìƒ¤ë”©.
        if in_distributed_mode() or not allow_device_map_auto_by_env(): #DDPì—ì„œëŠ” ë ˆì´ì–´ë¥¼ ìª¼ê°œì§€ ì•Šê³ , ëª¨ë¸ ì „ì²´ê°€ ê° GPUì— ë³µì œë˜ì–´ ì˜¬ë¼ê°‘ë‹ˆë‹¤.
            # ë¶„ì‚°(DDP/ë©€í‹° í”„ë¡œì„¸ìŠ¤) ëª¨ë“œë©´ ì—¬ê¸°ì„  ìƒ¤ë”©ì„ ì•ˆ í•¨. (DDPë‘ Accelerateì˜ intra-process ìƒ¤ë”©ì„ ì„ìœ¼ë©´ ê¼¬ì¼ ìˆ˜ ìˆì–´ì„œ)
            self.device_map = None
            return

        if not hasattr(self, 'device_map'):

            #GPUë³„ ë©”ëª¨ë¦¬ ì˜ˆì‚° ê³„ì‚°
            max_memory = get_balanced_memory(
                self.model,
                max_memory=None,
                no_split_module_classes=self.model_unit,
                low_zero=False,
            ) #GPUë³„ ë©”ëª¨ë¦¬ ì˜ˆì‚° ê³„ì‚°. no_split_module_classes=self.model_unit: ì§€ì •í•œ ëª¨ë“ˆ(ì˜ˆ: LlamaDecoderLayer)ì€ í•œ ì¥ë¹„ ì•ˆì—ì„œ í†µì§¸ë¡œ ë°°ì¹˜(ë ˆì´ì–´ ì¤‘ê°„ì—ì„œ ìª¼ê°œì§€ì§€ ì•Šê²Œ).
            from accelerate import infer_auto_device_map, dispatch_model

            # â€œì–´ë–¤ ëª¨ë“ˆì„ ì–´ëŠ GPUì—?â€ ìë™ ì¶”ë¡ . ëª¨ë¸ì˜ ì„œë¸Œëª¨ë“ˆ â†’ GPU ì¸ë±ìŠ¤ ë§¤í•‘ì„ ë§Œë“ ë‹¤.
            """
            {
                'model.embed_tokens': 0,
                'model.layers.0': 0, 'model.layers.1': 0, ... 'model.layers.11': 0,
                'model.layers.12': 1, ... 'model.layers.23': 1,
                'model.norm': 1,
                'lm_head': 1,
            }
            
            LoRA(PEFT) ì£¼ì˜: LoRA A/BëŠ” í•´ë‹¹ Linear ëª¨ë“ˆì— ë¶™ì–´ ìˆìœ¼ë¯€ë¡œ ê·¸ ëª¨ë“ˆì´ ê°€ëŠ” GPUë¡œ ê°™ì´ ì´ë™í•œë‹¤. ë”°ë¡œ ì°¢ì–´ì§€ì§€ ì•Šì•„.
            """
            self.device_map = infer_auto_device_map(
                self.model,
                max_memory=max_memory,
                no_split_module_classes=self.model_unit,
            ) 


            #ì‹¤ì œë¡œ GPUì— ë‚˜ëˆ  ì˜¬ë¦¬ê¸°. ìœ„ì—ì„œ ë§Œë“  ë§¤í•‘ëŒ€ë¡œ íŒŒë¼ë¯¸í„°/ë²„í¼ë¥¼ í•´ë‹¹ GPUë¡œ ì´ë™í•˜ê³ , êµì°¨-ë””ë°”ì´ìŠ¤ í˜¸ì¶œ í›…ì„ ë‹¬ì•„ì¤€ë‹¤. (í•œ í”„ë¡œì„¸ìŠ¤ ì•ˆì—ì„œ ë‹¤ì¤‘ GPU ì¶”ë¡ ì´ ê°€ëŠ¥í•´ì§)
            self.model = dispatch_model(self.model, device_map=self.device_map)

            ####ì‚¬ìš© ì˜ˆì‹œ
            """
            2 GPUsê°€ ìˆë‹¤ê³  ê°€ì • (cuda:0, cuda:1)
            adap = AdapterModel(
                base_model, 
                use_adapter=True, 
                adapter_package='peft', 
                adapter_method='lora', 
                r=8, lora_alpha=16
            )
            adap.sharding()
            print(adap.device_map)
            # ì˜ˆ) {'model.embed_tokens': 0, 'model.layers.0': 0, ..., 'model.layers.11': 0,
            #      'model.layers.12': 1, ..., 'model.layers.23': 1, 'model.norm': 1, 'lm_head': 1}

            ì›í•˜ë©´ ê²€ì¦ìš©ìœ¼ë¡œ:
            adap.print_model_map()
            # model.layers.0.self_attn.q_proj.weight -> cuda:0
            # model.layers.12.self_attn.q_proj.weight -> cuda:1
            # ...
            # (LoRA íŒŒë¼ë¯¸í„°ë“¤ë„ ê°™ì€ ë””ë°”ì´ìŠ¤ì— ë¶™ì–´ ìˆëŠ”ì§€ í™•ì¸ ê°€ëŠ¥)
            
            """



    def print_model_map(self): #íŒŒë¼ë¯¸í„°ë³„ë¡œ í• ë‹¹ëœ ë””ë°”ì´ìŠ¤ë¥¼ ë¡œê¹…. ìƒ¤ë”©/ë””ë°”ì´ìŠ¤ ë§µì´ ì œëŒ€ë¡œ ì ìš©ëëŠ”ì§€ ëˆˆìœ¼ë¡œ ê²€ì¦í•  ë•Œ ìœ ìš©.

        #self.model.named_parameters(): ë°±ë³¸ + ëª¨ë“  LoRA ì–´ëŒ‘í„°(í™œì„±/ë¹„í™œì„±) + modules_to_saveê¹Œì§€ ì „ë¶€ ë‚˜ì—´
        for i in self.model.named_parameters(): #i[0]: íŒŒë¼ë¯¸í„° ì´ë¦„ (str), i[1]: íŒŒë¼ë¯¸í„° í…ì„œ (torch.nn.Parameter).
            logger.info(f"{i[0]} -> {i[1].device}")

    def merge_and_unload(self): #ì´ ë°˜í™˜ê°’ì€ ìƒˆ ëª¨ë¸ ê°ì²´ê³ , í˜„ì¬ self.modelì€ ë°”ë€Œì§€ ì•ŠëŠ” ì ì— ì£¼ì˜(ì´ í•¨ìˆ˜ëŠ” â€œë°˜í™˜â€ë§Œ í•¨).
        if isinstance(self.model, PeftModel) and \
                callable(self.model.merge_and_unload): #self.modelì€ PEFT í´ë˜ìŠ¤. #PeftModelì´ë©´ LoRAë¥¼ ë² ì´ìŠ¤ì— í•©ì³ ìˆœìˆ˜ HF ëª¨ë¸ì„ ë°˜í™˜.
            return self.model.merge_and_unload()
        else:
            return self.model #ì•„ë‹ˆë©´ ì›ë˜ ëª¨ë¸ ê·¸ëŒ€ë¡œ.
         
    def append_adapters(self, adapter_names, peft_config=None): #ì—¬ëŸ¬ ê°œì˜ ì–´ëŒ‘í„°ë¥¼ ë™ì¼í•œ peft_configë¡œ ì¶”ê°€í•  ì¤€ë¹„.ì™¸ë¶€ì—ì„œ ë³„ë„ config ì£¼ì§€ ì•Šìœ¼ë©´ í˜„ì¬ ë³´ìœ í•œ self.peft_config ì¬ì‚¬ìš©.
        assert isinstance(self.model, PeftModel)
        peft_config = self.peft_config if peft_config is None else peft_config
        for name in adapter_names:
            self.model.add_adapter(name, peft_config)
            self.adapter_names.append(name)

        # â˜… ì¶”ê°€: ì¶”ê°€í•œ ì§í›„ í˜„ì¬ í™œì„± ì–´ëŒ‘í„° ê¸°ì¤€ìœ¼ë¡œ ë‹¤ì‹œ ì •ë¦¬
        current = getattr(self.model, 'active_adapter', 'default')
        self.set_active_adapter(current)


    # ---------------------- ì—¬ê¸°ë¶€í„° ì¶”ê°€ ìœ í‹¸ ----------------------
    def _iter_lora_modules(self):
        for _, m in self.model.named_modules():
            if hasattr(m, "lora_A") or hasattr(m, "lora_embedding_A"):
                yield m

    def _module_device(self, m: nn.Module):
        for p in m.parameters(recurse=False):
            return p.device
        for p in m.parameters():
            return p.device
        return next(self.model.parameters()).device

    def _list_adapter_names_safe(self):
        if hasattr(self, "adapter_names") and len(self.adapter_names) > 0:
            return list(self.adapter_names)

        names = set()
        if hasattr(self.model, "peft_config") and isinstance(self.model.peft_config, dict):
            names.update(list(self.model.peft_config.keys()))

        for m in self._iter_lora_modules():
            if hasattr(m, "lora_A"):
                names.update(list(getattr(m, "lora_A").keys()))
            if hasattr(m, "lora_embedding_A"):
                names.update(list(getattr(m, "lora_embedding_A").keys()))

        if not names:
            active = getattr(self.model, "active_adapter", None)
            if isinstance(active, str) and active:
                names.add(active)
            else:
                names.add("default")
        return sorted(list(names))

    def _move_adapter_on_module(self, m: nn.Module, adapter: str, device: torch.device):
        if hasattr(m, "lora_A") and adapter in getattr(m, "lora_A", {}):
            m.lora_A[adapter].to(device)
        if hasattr(m, "lora_B") and adapter in getattr(m, "lora_B", {}):
            m.lora_B[adapter].to(device)
        if hasattr(m, "lora_embedding_A") and adapter in getattr(m, "lora_embedding_A", {}):
            m.lora_embedding_A[adapter].to(device)
        if hasattr(m, "lora_embedding_B") and adapter in getattr(m, "lora_embedding_B", {}):
            m.lora_embedding_B[adapter].to(device)
        if hasattr(m, "lora_dropout"):
            try:
                if adapter in m.lora_dropout:
                    m.lora_dropout[adapter].to(device)
            except Exception:
                pass

    def offload_inactive_adapters(self,
                                active,
                                offload_device: torch.device = torch.device("cpu"),
                                freeze_offloaded: bool = True,
                                empty_cache: bool = True):
        if isinstance(active, str):
            active_set = {active}
        else:
            active_set = set(active)

        all_names = set(self._list_adapter_names_safe())
        active_set = {a for a in active_set if a in all_names}
        if not active_set and all_names:
            active_set = {next(iter(all_names))}

        for m in self._iter_lora_modules():
            adapters_here = set()
            if hasattr(m, "lora_A"):
                adapters_here.update(list(getattr(m, "lora_A").keys()))
            if hasattr(m, "lora_embedding_A"):
                adapters_here.update(list(getattr(m, "lora_embedding_A").keys()))

            module_dev = self._module_device(m)
            for an in adapters_here:
                tgt = module_dev if an in active_set else offload_device
                self._move_adapter_on_module(m, an, tgt)

        if freeze_offloaded:
            for n, p in self.model.named_parameters():
                is_adapter_param = (".lora_" in n) or (".modules_to_save." in n)
                if not is_adapter_param:
                    continue
                on_active = any(an in n for an in active_set)
                p.requires_grad = on_active

        if empty_cache and torch.cuda.is_available():
            torch.cuda.empty_cache()
            gc.collect()
    # ---------------------- ì¶”ê°€ ìœ í‹¸ ë ----------------------

    def set_active_adapter(self, adapter_name):  # ê¸°ì¡´ êµ¬í˜„ ë®ì–´ì“°ê¸°
        names = set(self._list_adapter_names_safe())
        assert adapter_name in names, f"Unknown adapter: {adapter_name} (known={names})"
        self.model.set_adapter(adapter_name)  # PEFT í˜¸ì¶œ
        try:
            self.offload_inactive_adapters(active=adapter_name,
                                        offload_device=torch.device("cpu"),
                                        freeze_offloaded=True,
                                        empty_cache=True)
        except Exception as e:
            import logging
            logging.getLogger(__name__).warning(f"[auto-offload skipped] {e}")




    # def set_active_adapter(self, adapter_name):#ìœ íš¨ì„± ê²€ì‚¬ í›„ í•´ë‹¹ ì–´ëŒ‘í„°ë¥¼ í™œì„± ìƒíƒœë¡œ ì „í™˜. í™œì„± ì–´ëŒ‘í„°ë§Œ í•™ìŠµ/ì¶”ë¡ ì— ì‚¬ìš©ë¨(PEFT ë‚´ë¶€ ë™ì‘).
    #     assert adapter_name in self.adapter_names
    #     self.model.set_adapter(adapter_name)  #self.modelì€ PEFT í´ë˜ìŠ¤

    def get_active_adapter(self):
        return self.model.active_adapter #í˜„ì¬ í™œì„± ì–´ëŒ‘í„° ì´ë¦„ì„ ë¦¬í„´.


    @property
    def config(self):
        return self.model.config

    @property
    def layers(self):
        _layers = []
        for module in self.model.modules():
            if isinstance(module, nn.ModuleList):
                # This one should be encoders/decoders
                _layers.append(module)

        if len(_layers) == 1:
            return _layers[0]
        return _layers

    def set_layers(self, layers):
        if isinstance(self.layers, nn.ModuleList) and isinstance(
                layers, nn.ModuleList):
            self.layers._modules = layers._modules

        elif isinstance(layers, list) and isinstance(self.layers, list):
            # This consists of multiple ModuleLists
            assert len(self.layers) == len(layers)
            for src, tgt in zip(self.layers, layers):
                assert isinstance(tgt, nn.ModuleList)
                src._modules = tgt._modules

        else:
            raise ValueError(
                'Layers cannot be set due to the mismatched type. ')

    @property
    def trainable_param_name_pattern(self):
        if isinstance(self.model, PeftModel):
            return self.model.active_adapter
        return None

    def set_trainable_modules(self, modules=None):
        # First, set all modules to untrainable
        for module in self.model.modules():
            module.requires_grad_(False)

        # Second, search for the capable modules
        if modules is None:
            # Set the encoders/decoders to be trainable
            modules = self.layers

        if isinstance(modules, nn.ModuleList):
            # Make it to the list
            trainable_modules = [modules]

        elif isinstance(modules, list):
            trainable_modules = modules

        else:
            raise ValueError(f'{modules} cannot be trainable because '
                             f'{type(modules)}.')

        pattern = self.trainable_param_name_pattern
        for module in trainable_modules:
            for layer in module:
                for name, param in layer.named_parameters():
                    if pattern is None or pattern in name:
                        param.requires_grad = True

    # TODO: Fix `__getattr__`
    # def __getattr__(self, item):
    #     return getattr(self.model, item)




class LLMDataParallel(nn.DataParallel):
    def __init__(self, adap_model, device_ids=None, output_device=None, dim=0):
        assert isinstance(adap_model, AdapterModel)
        super().__init__(adap_model.model,
                         device_ids=device_ids,
                         output_device=output_device,
                         dim=dim)
        self.model = adap_model

    def get_input_embeddings(self):
        return self.model.get_input_embeddings()

    def generate(self, *args, **kwargs):
        return self.model.generate(*args, **kwargs)

    def state_dict(self, return_trainable=True, *args, **kwargs):
        return self.model.state_dict(return_trainable, *args, **kwargs)

    def load_state_dict(self, state_dict, strict=False):
        return self.model.load_state_dict(state_dict, strict)

    def save_model(self, path, state=0):
        self.model.save_model(path, state)