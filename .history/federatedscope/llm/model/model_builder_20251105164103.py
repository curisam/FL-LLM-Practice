import os

from federatedscope.llm.model.adapter_builder import AdapterModel
from federatedscope.core.configs.config import global_cfg
import torch

import logging

from federatedscope.llm.misc.accel_utils import should_use_device_map_auto

from federatedscope.llm.misc.debug_utils import log_tok_model_sync

logger = logging.getLogger(__name__)


def get_model_from_huggingface(model_name, config, **kwargs): #kwargs={}
    from transformers import AutoModelForCausalLM

    if len(config.llm.cache.model):
        kwargs['cache_dir'] = config.llm.cache.model

    if config.train.is_enable_half: ############ ì ìš© #######
        kwargs['torch_dtype'] = torch.bfloat16

    if config.model.llm_type == 'SequenceClassification':
        from transformers import AutoModelForSequenceClassification
        if len(config.model.llm_kwargs) > 0:
            kwargs.update(config.model.llm_kwargs[0])
        return AutoModelForSequenceClassification.from_pretrained(
            model_name, **kwargs)
    else:############### ì´ê±¸ë¡œ ì ìš© ###############################
        return AutoModelForCausalLM.from_pretrained(model_name, **kwargs) #LoRA ì ìš© ì•ˆë¨.


def get_model_from_modelscope(model_name, config, **kwargs):
    from modelscope import AutoModelForCausalLM

    if len(config.llm.cache.model):
        kwargs['cache_dir'] = config.llm.cache.model

    return AutoModelForCausalLM.from_pretrained(model_name, **kwargs)


def get_llm(config, load_from_prev_ckpt=False, **kwargs):
    from federatedscope.llm.dataloader import get_tokenizer

    model_config = config.model
    model_name, model_hub = model_config.type.split('@')

    # --- [1] ğŸ”’ device_map='auto' ë°©ì–´ ---
    if 'device_map' in kwargs and kwargs['device_map'] == 'auto': #False
        if not should_use_device_map_auto():
            print("[get_llm] âš ï¸ device_map='auto' ë¬´ì‹œë¨ (DDP ëª¨ë“œì´ë¯€ë¡œ unsafe)")
            kwargs['device_map'] = None


    if config.model.load_from_local_pretrained_fs_config != '': #False
        # load model from local pretrained model
        pretrained_cfg = global_cfg.clone()
        pretrained_cfg.merge_from_file(
            config.model.load_from_local_pretrained_fs_config)
        assert pretrained_cfg.model.type.split('@')[0] == model_name, \
            'Two models cannot match. Failed to load from pretrained.'
        pretrained_model = get_llm(pretrained_cfg, **kwargs)
        if config.model.load_from_local_pretrained_model_path != '':
            path = config.model.load_from_local_pretrained_model_path
            ckpt = torch.load(path, map_location='cpu')
            logger.info('Successfully import the pretrained model '
                        f'from the checkpoint {path}. ')
            pretrained_model.load_state_dict(ckpt['model'])
        model = pretrained_model.merge_and_unload()
        logger.info(f'Merge and unload to {type(model)}...')
    elif model_hub == 'huggingface_llm':############# ì´ ë¶€ë¶„ì´ ê±¸ë¦¼, ì—¬ê¸°ì„œëŠ” LoRA ì•ˆë¶™ì„. ##############
        model = get_model_from_huggingface(model_name=model_name,
                                           config=config,
                                           **kwargs) 
    elif model_hub == 'modelscope_llm':
        model = get_model_from_modelscope(model_name=model_name,
                                          config=config,
                                          **kwargs)
    else:
        raise NotImplementedError(f'Not support LLM {model_name} in'
                                  f' {model_hub}.')

    # Resize LLM model based on settings
    tokenizer, num_new_tokens = \
        get_tokenizer(model_name, config.data.root, config.llm.tok_len)
    

    if model_config.llm_type == 'SequenceClassification': #False
        model.config.pad_token_id = tokenizer.pad_token_id


    model.resize_token_embeddings(len(tokenizer))

    log_tok_model_sync(tokenizer, model, tag="after-build")


    if num_new_tokens > 0:  
        input_embeddings = model.get_input_embeddings().weight.data
        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(
            dim=0, keepdim=True)
        input_embeddings[-num_new_tokens:] = input_embeddings_avg

        if model_config.llm_type != 'SequenceClassification':
            output_embeddings = model.get_output_embeddings().weight.data
            output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(
                dim=0, keepdim=True)
            output_embeddings[-num_new_tokens:] = output_embeddings_avg

    args = config.llm.adapter.args[0] if len(
        config.llm.adapter.args[0]) > 0 else {} #{'adapter_package': 'peft', 'adapter_method': 'lora', 'r': 8, 'lora_alpha': 16, 'lora_dropout': 0.05, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']}
    
    model = AdapterModel(model, use_adapter=config.llm.adapter.use, **args)#model:AdapterModel í´ë˜ìŠ¤  model.modelì€ PEFT í´ë˜ìŠ¤.



    if config.llm.adapter.count > 1: #3ìœ¼ë¡œ ì ìš©ë˜ì–´ì„œ ê±¸ë¦¼, ì‹¤ì§ˆì ìœ¼ë¡œ forwardì— ì“°ì´ëŠ” ê²ƒì€ 1ê°œë¿
        #adapter_names=[f"Adapter_{i}" for i in range(count)] â†’ ì˜ˆ: ["Adapter_0","Adapter_1","Adapter_2"]ë¥¼ ë§Œë“  ë‹¤ìŒ
        #ê° ì´ë¦„ì— ëŒ€í•´ **ë™ì¼í•œ LoRA ì„¤ì •ì„ ê°€ì§„ ì—¬ëŸ¬ â€œì´ë¦„ ìˆëŠ” ì–´ëŒ‘í„°â€**ë¥¼ ëª¨ë¸ ì•ˆì— ì¶”ê°€.
        #state_dictì—” ê° ì–´ëŒ‘í„° ì´ë¦„ ê¸°ì¤€ìœ¼ë¡œ lora_A.<adapter_name>.weight, lora_B.<adapter_name>.weightê°€ ì¶”ê°€ë¨.
        #**forwardëŠ” í•­ìƒ í•˜ë‚˜(ë˜ëŠ” set_adapterë¡œ ì§€ì •í•œ ë¦¬ìŠ¤íŠ¸)**ë§Œ ì ìš©. ë‚˜ë¨¸ì§€ ì–´ëŒ‘í„°ëŠ” â€œë¶™ì–´ë§Œ ìˆê³ â€ ì ìš©ë˜ì§€ ì•Šì•„ìš”.

        """
        í™œì„± ì–´ëŒ‘í„°(= forwardì— ì“°ì´ëŠ” ê²ƒ) ë°”ê¾¸ëŠ” ë²•
        PEFT ëª¨ë¸ì—ì„œ:      
                
        # í˜„ì¬ í™œì„± ì–´ëŒ‘í„° í™•ì¸
        print(model.active_adapter)        # ì˜ˆ: "default"

        # í™œì„± ì–´ëŒ‘í„° ë°”ê¾¸ê¸°
        model.set_adapter("Adapter_1")
        print(model.active_adapter)        # "Adapter_1"

        ì´ë ‡ê²Œ ë°”ê¾¸ë©´ ì´í›„ forwardì— Adapter_1ì˜ LoRAê°€ ì“°ì´ê³ , default/Adapter_0 ë“±ì€ ì ìš©ë˜ì§€ ì•ŠìŒ.

        ì—¬ëŸ¬ ì–´ëŒ‘í„°ë¥¼ ë™ì‹œì— ì“°ê³  ì‹¶ë‹¤ë©´? 
        PEFTëŠ” ë¦¬ìŠ¤íŠ¸ë¥¼ ì¤˜ì„œ ì—¬ëŸ¬ ì–´ëŒ‘í„°ë¥¼ í•©ì‚°í•˜ëŠ” ê²ƒë„ ì§€ì›í•´ìš”(ë‹¨, ë™ì¼ êµ¬ì¡°/í˜¸í™˜ ì „ì œ):    
        model.set_adapter(["Adapter_0", "Adapter_1"])  # ë‘ LoRA ë¸íƒ€ë¥¼ í•©ì³ì„œ ì‚¬ìš©
      
        
        """

        """
        modelì´ PEFT CLASSì¼ë•Œ
        model.peft_configëŠ” ëª¨ë¸ ì•ˆì— â€œì–´ë–¤ ì–´ëŒ‘í„°ë“¤ì´ ì–´ë–¤ ì„¤ì •(LoRA r/alpha/target_modules/bias ë“±)ìœ¼ë¡œ ë¶™ì–´ìˆëŠ”ì§€â€ë¥¼ ë‹´ì€ ë”•ì…”ë„ˆë¦¬
        í‚¤=ì–´ëŒ‘í„° ì´ë¦„("default", "Adapter_1"â€¦), ê°’=ê·¸ ì–´ëŒ‘í„°ì˜ LoraConfig(í˜¹ì€ ë‹¤ë¥¸ PEFT config).

        í˜„ì¬ ì–´ëŒ‘í„° ëª©ë¡ í™•ì¸: list(model.peft_config.keys())

        ê° ì–´ëŒ‘í„° ì„¸ë¶€ ì„¤ì • ì¡°íšŒ

        cfg = model.peft_config["default"]
        print(cfg.r, cfg.lora_alpha, cfg.lora_dropout, cfg.target_modules, cfg.bias)
        
        """


        model.append_adapters(adapter_names=[
            f'Adapter_{i}' for i in range(config.llm.adapter.count)
        ])#"default", "Adapter_0", "Adapter_1", "Adapter_2"ì˜ adapterê°€ ìˆê²Œ ë¨. Adapter classì¸ modelì€ append_adapters ì¡´ì¬.




    # 2) ckpt ë¶ˆëŸ¬ì™€ ê·¸ëŒ€ë¡œ ë¡œë“œ (ì—¬ë¶„ ì–´ëŒ‘í„°ëŠ” ìë™ ë¬´ì‹œ)
    ckpt_path = getattr(model_config, "load_from_local_pretrained_model_path", None)  


    if ckpt_path:
        ckpt = torch.load(ckpt_path, map_location="cpu")
        sd   = ckpt.get("model", ckpt)  # ì¼ë¶€ëŠ” ë°”ë¡œ state_dictì¼ ìˆ˜ ìˆìŒ

        res = model.load_state_dict(sd, strict=False)  # í•µì‹¬: strict=False
        try:
            #mis: modelì—ëŠ” ì•˜ì§€ë§Œ sdì—ëŠ” ì—†ëŠ” keyì˜ ìˆ˜
            #unexp: sdì—ëŠ” ìˆì§€ë§Œ modelì—ëŠ” ì—†ëŠ” keyì˜ ìˆ˜.
            miss = len(res.missing_keys); unexp = len(res.unexpected_keys)
        except:
            miss = unexp = -1
        logger.info(f"[Warmup-Init] loaded from {ckpt_path} "
                    f"(round={ckpt.get('cur_round','?')}) | "
                    f"missing={miss} unexpected={unexp}")
        

    # ê²€ì‚¬í•  ì–´ëŒ‘í„° ì´ë¦„(ì˜ˆì‹œ)
    name_a = "Adapter_0"
    name_b = "Adapter_1"

    # ì•ˆì „í•œ í…ì„œ ì¶”ì¶œ í—¬í¼ (ëª¨ë“ˆ, Parameter, Tensor ëª¨ë‘ ì²˜ë¦¬)
    def _get_weight_tensor(mod):
        if hasattr(mod, "weight") and isinstance(mod.weight, (torch.nn.Parameter, torch.Tensor)):
            return mod.weight
        if isinstance(mod, (torch.nn.Parameter, torch.Tensor)):
            return mod
        params = list(mod.parameters()) if hasattr(mod, "parameters") else []
        if len(params):
            return params[0]
        # ì¼ë¶€ PEFT ì €ì¥ êµ¬ì¡°ëŠ” .dataë‚˜ .weightê°€ ì•„ë‹Œ ê²½ìš°ê°€ ìˆìœ¼ë¯€ë¡œ ì‹œë„
        for attr in ("data", "_tensor", "_param"):
            v = getattr(mod, attr, None)
            if isinstance(v, (torch.nn.Parameter, torch.Tensor)):
                return v
        raise RuntimeError(f"Cannot find weight tensor in module {type(mod)}")

    def _compare_and_report(m, name_a, name_b):
        found = False
        for param_map_name in ("lora_A", "lora_B", "lora_embedding_A", "lora_embedding_B"):
            if not hasattr(m, param_map_name):
                continue
            d = getattr(m, param_map_name)
            if not isinstance(d, dict):
                continue
            if name_a in d and name_b in d:
                found = True
                mod_a = d[name_a]
                mod_b = d[name_b]

                try:
                    a_w = _get_weight_tensor(mod_a)
                    b_w = _get_weight_tensor(mod_b)
                except Exception as e:
                    print(f"[{param_map_name}] failed to extract tensor: {e}")
                    continue

                try:
                    a_cpu = a_w.detach().cpu()
                except Exception:
                    a_cpu = a_w.cpu()
                try:
                    b_cpu = b_w.detach().cpu()
                except Exception:
                    b_cpu = b_w.cpu()

                print(f"module: {type(m)} | map: {param_map_name}")
                print(" - keys:", list(d.keys()))
                print(" - same object?", mod_a is mod_b)
                try:
                    same_storage = hasattr(a_w, 'storage') and hasattr(b_w, 'storage') and a_w.storage().data_ptr() == b_w.storage().data_ptr()
                except Exception:
                    same_storage = False
                print(" - same storage?", same_storage)

                if a_cpu.shape == b_cpu.shape:
                    try:
                        values_equal = torch.equal(a_cpu, b_cpu)
                    except Exception:
                        values_equal = False
                    try:
                        max_diff = float((a_cpu - b_cpu).abs().max().item())
                    except Exception:
                        max_diff = float('nan')
                else:
                    values_equal = False
                    max_diff = float('nan')

                print(" - values equal?", values_equal)
                print(" - max diff:", max_diff)
                print(" - a device/dtype:", getattr(a_w, 'device', 'n/a'), getattr(a_w, 'dtype', 'n/a'),
                      " b device/dtype:", getattr(b_w, 'device', 'n/a'), getattr(b_w, 'dtype', 'n/a'))
                try:
                    print(" - a mean/std:", float(a_cpu.mean().item()), float(a_cpu.std().item()))
                    print(" - b mean/std:", float(b_cpu.mean().item()), float(b_cpu.std().item()))
                except Exception:
                    pass
                # í•œ í•­ëª©ë§Œ ë¹„êµí•˜ë©´ ì¶©ë¶„í•˜ë¯€ë¡œ ë°˜í™˜
                return True
        return found

    # ìˆœíšŒí•˜ë©´ì„œ ì²« ë²ˆì§¸ë¡œ ì°¾ëŠ” ì¼ì¹˜ í•­ëª©ì„ ë³´ê³ í•¨
    for m in model._iter_lora_modules():
        try:
            reported = _compare_and_report(m, name_a, name_b)
            if reported:
                break
        except Exception as e:
            print(f"Error while comparing adapters in module {type(m)}: {e}")
            continue







    return model #model:AdapterModel í´ë˜ìŠ¤  model.modelì€ PEFT í´ë˜ìŠ¤.



