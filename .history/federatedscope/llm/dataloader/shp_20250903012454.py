import os
import json
import copy
import pickle
import datasets

from federatedscope.core.splitters.generic.lda_splitter import LDASplitter
from federatedscope.core.data.utils import download_url
from federatedscope.llm.dataloader.dataloader import load_jsonls, load_jsonl
from federatedscope.llm.dataset.llm_dataset import LLMComparisonDataset, \
    LLMDataset

# --- í™˜ê²½ ë³€ìˆ˜ ê¸°ë°˜ í—¬í¼ í•¨ìˆ˜ ---
def is_main_process_env():
    return os.environ.get("LOCAL_RANK", "0") == "0"
# --------------------------------

SHP_PROMPT_DICT = {
    "shp": ("Below is an instruction that describes a task. "
            "Write a response that appropriately completes the request.\n\n"
            "### Instruction:\n{instruction}\n\n"
            "### Response:"),
    "shp_cmp": ("Below is a query followed by two responses. Pick a "
                "helpful response that is precise, concise, and casual. "
                "State your choice with a single capital letter, "
                "i.e., \"A\" if RESPONSE A is better, "
                "\"B\" if RESPONSE B is better.\n\n"
                "### QUERY: {instruction}\n"
                "### RESPONSE A: {output_A}\n"
                "### RESPONSE B: {output_B}\n"
                "### YOUR CHOICE:"),
    "mix_cmp": ("Below is an instruction that describes a task. "
                "There are two responses that complete the request. "
                "Pick an appropriate response and state your choice with "
                "a single capital letter, i.e., "
                "\"A\" if RESPONSE A is better and more appropriate, "
                "\"B\" if RESPONSE B is better and more appropriate.\n\n"
                "### Instruction:\n{instruction}\n\n"
                "### RESPONSE A: {output_A}\n"
                "### RESPONSE B: {output_B}\n"
                "### YOUR CHOICE:")
}


def _download_shp_cmpr(data_root):
    """
    (ì „ì²´ ê°œìš”)
    ì…ë ¥: data_root (ë°ì´í„°ë¥¼ ì €ì¥í•  ê²½ë¡œ)

    ì¶œë ¥: list_train_dict, list_val_dict, list_test_dict (ì„¸ ê°€ì§€ splitì˜ ìƒ˜í”Œ ë¦¬ìŠ¤íŠ¸)

    ìƒ˜í”Œ êµ¬ì¡°: ê° ìƒ˜í”Œì€ ë”•ì…”ë„ˆë¦¬ë¡œ,

    {
        "instruction": ...,  # í”„ë¡¬í”„íŠ¸ (ì§ˆë¬¸/ì§€ì‹œë¬¸)
        "output_A": ...,     # í›„ë³´ ì‘ë‹µ A
        "output_B": ...,     # í›„ë³´ ì‘ë‹µ B
        "choice": ...,       # ì‚¬ëŒì´ ì„ íƒí•œ ì •ë‹µ (0/1)
        "category": ...      # ì›ë˜ ë„ë©”ì¸ (reddit ë“±)
    }
        
    """


    #íŒŒì¼ ê²½ë¡œ ì„¤ì •.
    train_fp, val_fp, test_fp = [
        os.path.join(data_root, 'shp_cmpr_train.jsonl'),
        os.path.join(data_root, 'shp_cmpr_val.jsonl'),
        os.path.join(data_root, 'shp_cmpr_test.jsonl')
    ]

    dataloader_kwargs = {
        'instruction': 'instruction', # í”„ë¡¬í”„íŠ¸ (ì§ˆë¬¸/ì§€ì‹œë¬¸)
        'output_A': 'output_A', # í›„ë³´ ì‘ë‹µ A
        'output_B': 'output_B', # í›„ë³´ ì‘ë‹µ B
        'choice': 'choice', # ì‚¬ëŒì´ ì„ íƒí•œ ì •ë‹µ (0/1)
        'category': 'category' # ì›ë˜ ë„ë©”ì¸ (reddit ë“±)
    }
    if os.path.exists(train_fp) and os.path.exists(val_fp) and \
            os.path.exists(test_fp):
        list_train_dict = load_jsonl(train_fp, **dataloader_kwargs)
        list_val_dict = load_jsonl(val_fp, **dataloader_kwargs)
        list_test_dict = load_jsonl(test_fp, **dataloader_kwargs)

    else:
        dataset = datasets.load_dataset("stanfordnlp/SHP")
        list_train_dict, list_val_dict, list_test_dict = [], [], []
        tag_fp = {
            'train': (train_fp, list_train_dict),
            'validation': (val_fp, list_val_dict),
            'test': (test_fp, list_test_dict)
        }
        for tag, (fp, list_data_dict) in tag_fp.items():
            file = open(fp, 'w')
            for hist, ref_A, ref_B, choice, domain in \
                zip(dataset[tag]['history'],
                    dataset[tag]['human_ref_A'],
                    dataset[tag]['human_ref_B'],
                    dataset[tag]['labels'],
                    dataset[tag]['domain']):
                record = {
                    'instruction': hist,
                    'output_A': ref_A,
                    'output_B': ref_B,
                    'choice': choice,
                    'category': domain.split('_')[0]
                }
                file.write(f'{json.dumps(record)}\n')
                list_data_dict.append(record)
            file.close()

    return list_train_dict, list_val_dict, list_test_dict


def _download_shp(data_root):
    train_fp, val_fp, test_fp = [
        os.path.join(data_root, 'shp_rlhf_train.jsonl'),
        os.path.join(data_root, 'shp_rlhf_val.jsonl'),
        os.path.join(data_root, 'shp_rlhf_test.jsonl')
    ]

    dataloader_kwargs = {'instruction': 'instruction', 'category': 'category'}
    if os.path.exists(train_fp) and os.path.exists(val_fp) and \
            os.path.exists(test_fp):
        list_train_dict = load_jsonl(train_fp, **dataloader_kwargs)
        list_val_dict = load_jsonl(val_fp, **dataloader_kwargs)
        list_test_dict = load_jsonl(test_fp, **dataloader_kwargs)

    else:
        dataset = datasets.load_dataset("stanfordnlp/SHP")
        instructions = []
        list_train_dict, list_val_dict, list_test_dict = [], [], []
        tag_fp = {
            'train': (train_fp, list_train_dict),
            'validation': (val_fp, list_val_dict),
            'test': (test_fp, list_test_dict)
        }
        for tag, (fp, list_data_dict) in tag_fp.items():
            file = open(fp, 'w')
            for hist, domain in zip(dataset[tag]['history'],
                                    dataset[tag]['domain']):
                if hist not in instructions:
                    instructions.append(hist)
                    record = {
                        'instruction': hist,
                        'category': domain.split('_')[0]
                    }
                    file.write(f'{json.dumps(record)}\n')
                    list_data_dict.append(record)
            file.close()

    return list_train_dict, list_val_dict, list_test_dict


def shp_dataset(data_root, num_clients, tokenizer):
    #ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°


    list_train_dict, list_val_dict, list_test_dict = \
        _download_shp_cmpr(data_root)

    # First, disjoint by post instructions
    list_train_instructions, _, _ = _download_shp(data_root)


   #ì›ë˜ ì¹´í…Œê³ ë¦¬ â†’ ìˆ«ì ì¸ë±ìŠ¤ë¡œ ë³€í™˜
    """
    ê° instruction ìƒ˜í”Œì˜ ì›ë˜ category(ì˜ˆ: Reddit ì„œë¸Œë ˆë”§ ì´ë¦„)ë¥¼ ì •ìˆ˜ ì¸ë±ìŠ¤ë¡œ ë§¤í•‘.

    sample['categories']ë¼ëŠ” ìƒˆ í•„ë“œì— ê·¸ ì¸ë±ìŠ¤ë¥¼ ê¸°ë¡.
    ğŸ‘‰ ë””ë¦¬í´ë ˆ ë¶„í• ê¸°(LDASplitter)ê°€ ì¹´í…Œê³ ë¦¬ ë¶„í¬ë¥¼ ì“¸ ìˆ˜ ìˆê²Œ ì¤€ë¹„.
    """
    cat_idx_map = {}
    for sample in list_train_instructions:
        if sample['category'] not in cat_idx_map:
            cat_idx_map[sample['category']] = len(cat_idx_map)
        sample['categories'] = cat_idx_map[sample['category']]


    # ë””ë¦¬í´ë ˆ ë¶„í• ë¡œ instructionì„ í´ë¼ì´ì–¸íŠ¸ì— í• ë‹¹

    """
    LDASplitter: instruction ìƒ˜í”Œë“¤ì„ num_clientsê°œì˜ í´ë¼ì´ì–¸íŠ¸ì— ë¹„IID(ë¶ˆê· ì¼)í•˜ê²Œ ë¶„ë°°. alpha=0.3ì€ í•œ í´ë¼ì´ì–¸íŠ¸ê°€ íŠ¹ì • ì£¼ì œì— ë” ì¹˜ìš°ì¹˜ë„ë¡ ë§Œë“¦.

    ê²°ê³¼ë¬¼ inst_split_listëŠ” [[í´ë¼ì´ì–¸íŠ¸0 ìƒ˜í”Œë“¤], [í´ë¼ì´ì–¸íŠ¸1 ìƒ˜í”Œë“¤], ...].

    ì´ê±¸ í’€ì–´ì„œ inst_client_mapì´ë¼ëŠ” dict(â€œì–´ë–¤ instruction â†’ ì–´ë–¤ client_idâ€)ë¥¼ ë§Œë“¦.
    
    """
    splitter = LDASplitter(num_clients, alpha=0.3)
    inst_split_list = splitter(list_train_instructions)
    inst_client_map = {}
    for idx, sublist in enumerate(inst_split_list):
        for sample in sublist:
            inst_client_map[sample['instruction']] = idx


    #ë¹„êµ ë°ì´í„°ì…‹ì— í´ë¼ì´ì–¸íŠ¸ ë¼ë²¨ ì…íˆê¸°
    """
    ë¹„êµ ë°ì´í„°(train)ì˜ ê° ìƒ˜í”Œì€ ì›ë˜ categoryë¥¼ domainì— ë”°ë¡œ ì €ì¥.

    ëŒ€ì‹  categoryì—ëŠ” Client_0, Client_1 ê°™ì€ í´ë¼ì´ì–¸íŠ¸ IDë¥¼ ê¸°ë¡.
    ğŸ‘‰ ì¦‰, ê°™ì€ instructionì„ ê³µìœ í•˜ëŠ” ë¹„êµ ìƒ˜í”Œë“¤ì€ í•œ í´ë¼ì´ì–¸íŠ¸ ì†Œìœ ë¡œ ë¬¶ì„.

    """

    # Update their categories and force the data splitter as meta
    for sample in list_train_dict:
        sample['domain'] = sample['category']
        sample['category'] = \
            f"Client_{inst_client_map[sample['instruction']]}"

    # í† í° ê¸¸ì´ ì œí•œ í•„í„°ë§
    """
    instruction + output_A + output_B ì„¸ ë¬¸ì¥ì„ í† í°í™”í•´ì„œ ê¸¸ì´ í•©ì‚°.

    512 í† í° ì´í•˜ì¸ ìƒ˜í”Œë§Œ train ë°ì´í„°ì— ë‚¨ê¹€.
    ğŸ‘‰ í•™ìŠµ ì‹œ ì…ë ¥ ê¸¸ì´ ì´ˆê³¼ë¡œ ìƒê¸°ëŠ” ë©”ëª¨ë¦¬ ë¬¸ì œ ë°©ì§€.

    """
    new_list_train_dict = []
    for sample in list_train_dict:
        len_inst = len(tokenizer(sample['instruction'])['input_ids'])
        len_resA = len(tokenizer(sample['output_A'])['input_ids'])
        len_resB = len(tokenizer(sample['output_B'])['input_ids'])
        if len_inst + len_resA + len_resB <= 512:
            new_list_train_dict.append(sample)
    list_train_dict = new_list_train_dict

    # í´ë¼ì´ì–¸íŠ¸ë³„ ë„ë©”ì¸ ë¶„í¬ ì¶œë ¥

    """
    ê° í´ë¼ì´ì–¸íŠ¸(Client_0, Client_1, â€¦)ì— ì–´ë–¤ domain(ì›ë˜ ì¹´í…Œê³ ë¦¬)ì´ ì–¼ë§ˆë‚˜ ë¶„í¬í–ˆëŠ”ì§€ ì¶œë ¥.

    ê²°ê³¼ë¥¼ ë³´ë©´ í´ë¼ì´ì–¸íŠ¸ë³„ ë°ì´í„° ë¶„í¬ê°€ ë¹„IID(ë¶ˆê· ë“±)í•˜ê²Œ ì˜ ë‚˜ë‰˜ì—ˆëŠ”ì§€ í™•ì¸ ê°€ëŠ¥.

    ì£¼ì˜: range(num_clients + 1)ë¼ ë§ˆì§€ë§‰ì— ë¹ˆ client í•˜ë‚˜ ë” ì°í ìˆ˜ë„ ìˆìŒ.    
    
    """
    for client_id in range(num_clients + 1):
        print(f'Client {client_id}:')
        num_sample_by_domains = dict()
        for sample in new_list_train_dict:
            if sample['category'] == f'Client_{client_id}':
                if sample['domain'] not in num_sample_by_domains:
                    num_sample_by_domains[sample['domain']] = 0
                num_sample_by_domains[sample['domain']] += 1
        print(num_sample_by_domains)


    # ìµœì¢… trainì€ í´ë¼ì´ì–¸íŠ¸ ë¼ë²¨+í† í° í•„í„°ë§ ì ìš©ë¨. val/testëŠ” ê·¸ëŒ€ë¡œ ì›ë³¸ ë¶„í• ì„ ìœ ì§€.
    return list_train_dict, list_val_dict, list_test_dict


def load_rlhf_dataset(data_root,
                      tokenizer,
                      max_num_test=-1,
                      raw_no_prompt=False):
    _, list_val_dict, list_test_dict = \
        _download_shp(data_root)

    # reorganize the training data for RLHF
    list_train_dict = list_val_dict + list_test_dict
    list_val_dict = list_test_dict[:len(list_test_dict) // 2]
    list_test_dict = list_test_dict[len(list_test_dict) // 2:]

    if max_num_test > 0:
        return (list_train_dict, list_val_dict[:max_num_test],
                list_test_dict[:max_num_test])
    else:
        return list_train_dict, list_val_dict, list_test_dict


def load_safe_dataset():
    ds = datasets.load_dataset("PKU-Alignment/PKU-SafeRLHF-prompt")
    list_train_dict = [{
        'instruction': prompt
    } for prompt in ds['train']['prompt']]

    return list_train_dict, None, None


def load_comparison_dataset(data_root, tokenizer, config, max_num_test=-1):
    token_name = os.path.basename(tokenizer.name_or_path)
    num_clients = config.federate.client_num
    train_fp, val_fp, test_fp = [
        os.path.join(data_root, f'{token_name}_train_{num_clients}.pickle'),
        os.path.join(data_root, f'{token_name}_val.pickle'),
        os.path.join(data_root, f'{token_name}_test.pickle')
    ]

    if os.path.exists(train_fp) and os.path.exists(val_fp) and os.path.exists(
            test_fp):
        with open(train_fp, 'rb') as f_train, open(val_fp, 'rb') as f_val, \
                open(test_fp, 'rb') as f_test:
            train_dataset = pickle.load(f_train)
            val_dataset = pickle.load(f_val)
            test_dataset = pickle.load(f_test)

    else:
        list_train_dict, list_val_dict, list_test_dict = \
            shp_dataset(data_root, num_clients, tokenizer)

        # load dataset, which should be tuple
        train_dataset = LLMComparisonDataset(
            list_train_dict,
            tokenizer,
            prompt_input=SHP_PROMPT_DICT['shp'],
            prompt_no_input=SHP_PROMPT_DICT['shp'],
            output_A='output_A',
            output_B='output_B',
            choice='choice')
        val_dataset = LLMComparisonDataset(
            list_val_dict,
            tokenizer,
            prompt_input=SHP_PROMPT_DICT['shp'],
            prompt_no_input=SHP_PROMPT_DICT['shp'],
            output_A='output_A',
            output_B='output_B',
            choice='choice')
        test_dataset = LLMComparisonDataset(
            list_test_dict,
            tokenizer,
            prompt_input=SHP_PROMPT_DICT['shp'],
            prompt_no_input=SHP_PROMPT_DICT['shp'],
            output_A='output_A',
            output_B='output_B',
            choice='choice')

        # Store these three lists to a pickle file
        with open(train_fp, 'wb') as f_train, \
                open(val_fp, 'wb') as f_val, \
                open(test_fp, 'wb') as f_test:
            pickle.dump(train_dataset, f_train)
            pickle.dump(val_dataset, f_val)
            pickle.dump(test_dataset, f_test)

    # shrink val and test dataset
    if max_num_test > 0:
        val_dataset.win_dataset.input_ids = \
            val_dataset.win_dataset.input_ids[:max_num_test]
        val_dataset.lose_dataset.input_ids = \
            val_dataset.lose_dataset.input_ids[:max_num_test]
        test_dataset.win_dataset.input_ids = \
            test_dataset.win_dataset.input_ids[:max_num_test]
        test_dataset.lose_dataset.input_ids = \
            test_dataset.lose_dataset.input_ids[:max_num_test]

    dataset = (train_dataset, val_dataset, test_dataset)

    return dataset


def load_shp_best_dataset(data_root, tokenizer, config, max_num_test=-1):
    train_dataset, val_dataset, test_dataset = \
        load_comparison_dataset(data_root, tokenizer, config, max_num_test)
    # Use the win_dataset only
    dataset = (train_dataset.win_dataset, val_dataset.win_dataset,
               test_dataset.win_dataset)
    return dataset


def load_shp_cmp_dataset_by_choice(data_root,
                                   tokenizer,
                                   config,
                                   max_num_test=-1): #ì´ê±°ì— í•´ë‹¹.
    token_name = os.path.basename(tokenizer.name_or_path)
    num_clients = config.federate.client_num

    train_fp, val_fp, test_fp = [
        os.path.join(data_root,
                     f'{token_name}_train_choice_{num_clients}.pickle'),
        os.path.join(data_root, f'{token_name}_val_choice.pickle'),
        os.path.join(data_root, f'{token_name}_test_choice.pickle')
    ]

    # ë™ê¸°í™”ë¥¼ ìœ„í•œ ì™„ë£Œ íŒŒì¼(completion file) ê²½ë¡œ
    completion_file_path = os.path.join(data_root, f'{token_name}_shp.complete')

    if is_main_process_env():

        # ë©”ì¸ í”„ë¡œì„¸ìŠ¤ëŠ” ìºì‹œê°€ ìœ íš¨í•œì§€ í™•ì¸í•˜ê³ , ìœ íš¨í•˜ì§€ ì•Šìœ¼ë©´ ì¬ìƒì„±
        if not os.path.exists(completion_file_path):
            logger.info("Main process: Completion file not found. Generating data...")

        else:
            # ... (ë°ì´í„° ìƒì„± ë¡œì§ì€ ê¸°ì¡´ê³¼ ë™ì¼: shp_dataset, ë ˆì´ë¸” ë³€í™˜, LLMDataset ìƒì„±)
            list_train_dict, list_val_dict, list_test_dict = shp_dataset(data_root, num_clients, tokenizer)

            # ... (ë ˆì´ë¸” ë³€í™˜) ...
            # map the choice to "A" and "B" instead of 0 and 1. 

            #ë ˆì´ë¸”(Choice) ë³€í™˜. ë™ì‘: LLMì´ ë‹µë³€ì„ ìƒì„±í•˜ê¸° ì‰½ë„ë¡, ìˆ«ì ë ˆì´ë¸” 0, 1ì„ ë¬¸ìì—´ " A", " B"ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
            #ì˜ˆì‹œ:
            #### choiceê°€ 0ì´ì—ˆë˜ ìƒ˜í”Œì€ chr(0 + ord("A")) -> chr(65) -> "A"ê°€ ë˜ê³ , ì•ì— ê³µë°±ì´ ë¶™ì–´ ìµœì¢…ì ìœ¼ë¡œ " A"ê°€ ë©ë‹ˆë‹¤.
            #### choiceê°€ 1ì´ì—ˆë˜ ìƒ˜í”Œì€ chr(1 + ord("A")) -> chr(66) -> "B"ê°€ ë˜ê³ , ìµœì¢…ì ìœ¼ë¡œ " B"ê°€ ë©ë‹ˆë‹¤.  

            for list_dict in [list_train_dict, list_test_dict, list_val_dict]:
                for sample in list_dict:
                    sample['choice'] = " " + chr(sample['choice'] + ord("A"))

            # ... (LLMDataset ê°ì²´ 3ê°œ ìƒì„±) ...

            #ì „ì²˜ë¦¬ëœ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸(list_train_dict ë“±)ë¥¼ LLMDataset í´ë˜ìŠ¤ì— ì „ë‹¬í•˜ì—¬ ìµœì¢… ë°ì´í„°ì…‹ ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
            ####prompt_input (í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿)ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
            """Below is a query followed by two responses. Pick a helpful response that is precise, concise, and casual. 
            State your choice with a single capital letter, i.e., \"A\" if RESPONSE A is better, \"B\" if RESPONSE B is better.\n\n ### QUERY: {instruction}\n 
            ### RESPONSE A: {output_A}\n ### RESPONSE B: {output_B}\n ### YOUR CHOICE: """
            #### ê° ìƒ˜í”Œ ë”•ì…”ë„ˆë¦¬ì˜ ë‚´ìš©(instruction, output_A, output_B)ì„ ì´ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì— ì±„ì›Œ ë„£ì–´ ì™„ì „í•œ ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ ë§Œë“­ë‹ˆë‹¤.
            #### tokenizerë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ ì…ë ¥ í…ìŠ¤íŠ¸ì™€ íƒ€ê²Ÿ í…ìŠ¤íŠ¸(" A" ë˜ëŠ” " B")ë¥¼ í† í°í™”(ìˆ«ì ì‹œí€€ìŠ¤ë¡œ ë³€í™˜)í•˜ì—¬ input_ids, attention_mask, labels ë“±ì„ ìƒì„±í•©ë‹ˆë‹¤.
            ####ì´ ëª¨ë“  ì •ë³´ë¥¼ ë‹´ê³  ìˆëŠ” ë°ì´í„°ì…‹ ê°ì²´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
            train_dataset = LLMDataset(list_train_dict,
                                    tokenizer,
                                    prompt_input=SHP_PROMPT_DICT['shp_cmp'],
                                    prompt_no_input=SHP_PROMPT_DICT['shp_cmp'],
                                    output_tag='choice')
            val_dataset = LLMDataset(list_val_dict,
                                    tokenizer,
                                    prompt_input=SHP_PROMPT_DICT['shp_cmp'],
                                    prompt_no_input=SHP_PROMPT_DICT['shp_cmp'],
                                    output_tag='choice')
            test_dataset = LLMDataset(list_test_dict,
                                    tokenizer,
                                    prompt_input=SHP_PROMPT_DICT['shp_cmp'],
                                    prompt_no_input=SHP_PROMPT_DICT['shp_cmp'],
                                    output_tag='choice')

            # Store these three lists to a pickle file  ìµœì¢… LLMDataset ê°ì²´ë“¤ì„ pickleì„ ì‚¬ìš©í•´ íŒŒì¼ë¡œ ì €ì¥
            # ë°ì´í„° íŒŒì¼ ì €ì¥.
            with open(train_fp, 'wb') as f_train, \
                    open(val_fp, 'wb') as f_val, \
                    open(test_fp, 'wb') as f_test:
                pickle.dump(train_dataset, f_train)
                pickle.dump(val_dataset, f_val)
                pickle.dump(test_dataset, f_test)

            logger.info("Main process: Caching completed in shp.")
            # ëª¨ë“  ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ëë‚˜ë©´ ì™„ë£Œ íŒŒì¼ ìƒì„±
            with open(completion_file_path, 'w') as f:
                f.write('done')

    # ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ë“¤ì€ ì™„ë£Œ íŒŒì¼ì´ ìƒì„±ë  ë•Œê¹Œì§€ ëŒ€ê¸°
    else:
        local_rank = os.environ.get("LOCAL_RANK", "?")
        logger.info(f"Process {local_rank}: Waiting for completion file...")
        while not os.path.exists(completion_file_path):
            time.sleep(2)
        logger.info(f"Process {local_rank}: Completion file found.")



    # ì´ì œ ëª¨ë“  í”„ë¡œì„¸ìŠ¤ëŠ” ë©”ì¸ í”„ë¡œì„¸ìŠ¤ê°€ ëª¨ë“  ì‘ì—…ì„ ì™„ë£Œí–ˆìŒì„ í™•ì‹ í•˜ê³ 
    # ì•ˆì „í•˜ê²Œ íŒŒì¼ì„ ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    with open(train_fp, 'rb') as f_train, open(val_fp, 'rb') as f_val, \
            open(test_fp, 'rb') as f_test:
        train_dataset = pickle.load(f_train)
        val_dataset = pickle.load(f_val)
        test_dataset = pickle.load(f_test)

    # shrink val and test dataset
    if max_num_test > 0: #ë°ì´í„° ì‚¬ì´ì¦ˆ ì¤„ì—¬ì„œ ë°˜í™˜. LLMDatasetì— ì ‘ê·¼í•˜ì—¬ ì ìš©.
        val_dataset.input_ids = val_dataset.input_ids[:max_num_test]
        test_dataset.input_ids = test_dataset.input_ids[:max_num_test]

    dataset = (train_dataset, val_dataset, test_dataset)

    return dataset


def load_alpacafarm_human_for_eval(data_root, tokenizer):
    token_name = os.path.basename(tokenizer.name_or_path)
    path = os.path.join(data_root,
                        f'{token_name}_alpacafarm_human_choice.pickle')
    if os.path.exists(path):
        with open(path, 'rb') as f:
            test_dataset = pickle.load(f)
    else:
        ds = datasets.load_dataset("tatsu-lab/alpaca_farm",
                                   "alpaca_human_preference")["preference"]
        list_data_dict = []
        for row in ds.iter(batch_size=1):
            record = {
                "instruction": row["instruction"][0],
                "output_A": row["output_1"][0],
                "output_B": row["output_2"][0],
                "choice": {
                    1: 'A',
                    2: 'B'
                }[row["preference"][0]],
            }
            if row["input"][0]:
                record["instruction"] += f'\n\n{row["input"][0]}'
            list_data_dict.append(record)

        test_dataset = LLMDataset(list_data_dict,
                                  tokenizer,
                                  prompt_input=SHP_PROMPT_DICT['shp_cmp'],
                                  prompt_no_input=SHP_PROMPT_DICT['shp_cmp'],
                                  output_tag='choice')

        with open(path, 'wb') as f:
            pickle.dump(test_dataset, f)

    return test_dataset
