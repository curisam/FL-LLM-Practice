import torch
import logging
import gc
import math

try:
    import deepspeed
    from deepspeed import DeepSpeedEngine
except:
    deepspeed = None
    DeepSpeedEngine = None

import accelerate
from accelerate import Accelerator, DistributedDataParallelKwargs


import torch.distributed as dist
from torch.utils.data import DataLoader, Sampler
from torch.utils.data.distributed import DistributedSampler
from transformers import AdamW

# from torch.optim import AdamW

from federatedscope.register import register_trainer
from federatedscope.core.trainers import GeneralTorchTrainer
from federatedscope.core.trainers.context import CtxVar, lifecycle
from federatedscope.core.trainers.enums import MODE, LIFECYCLE
from federatedscope.core.monitors.monitor import Monitor
from federatedscope.core.data.wrap_dataset import WrapDataset
from federatedscope.core.auxiliaries.dataloader_builder import get_dataloader
from federatedscope.core.auxiliaries.ReIterator import ReIterator
from federatedscope.core.auxiliaries.optimizer_builder import get_optimizer
from federatedscope.core.auxiliaries.scheduler_builder import get_scheduler
from federatedscope.llm.model.adapter_builder import AdapterModel
from federatedscope.llm.dataloader.dataloader import get_tokenizer



from federatedscope.core.auxiliaries.decorators import use_diff

from federatedscope.llm.misc.debug_utils import log_tok_model_sync



import os, json



logger = logging.getLogger(__name__)

import sys

sys.setrecursionlimit(100000)


# ëª¨ë“  ë­í¬ ë™ê¸°í™”ìš© ë°°ë¦¬ì–´
from federatedscope.llm.utils_dist import barrier_all



def _get_dist_info():
    if dist.is_available() and dist.is_initialized():
        return True, dist.get_world_size(), dist.get_rank()
    try:
        ws = int(os.environ.get('WORLD_SIZE', '1'))
        rk = int(os.environ.get('RANK', '0'))
        return (ws > 1), ws, rk
    except Exception:
        return False, 1, 0

class EvalShardSampler(Sampler):
    def __init__(self, dataset_len: int, rank: int, world_size: int):
        self.indices = list(range(rank, int(dataset_len), int(world_size)))
    def __iter__(self): return iter(self.indices)
    def __len__(self): return len(self.indices)



class LLMTrainer(GeneralTorchTrainer): #**Large Language Model (LLM)**ì„ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ í™•ì¥ëœ ë²„ì „.

#ì¦‰, ì¼ë°˜ì ì¸ Trainerë¡œëŠ” ë„ˆë¬´ ëŠë¦¬ê±°ë‚˜ ë©”ëª¨ë¦¬ë¥¼ ë„ˆë¬´ ë§ì´ ì¡ì•„ë¨¹ëŠ” LLMì„ í•™ìŠµí•˜ë ¤ë©´,ë‹¤ìŒ ë‘ ê°€ì§€ê°€ í•„ìš”í•¨.
#### 1. íš¨ìœ¨ì ì¸ ë©”ëª¨ë¦¬ ê´€ë¦¬ì™€ GPU ë¶„ì‚° í•™ìŠµ â†’ DeepSpeed -> ì‹¤ì œë¡œ ì ìš© ì•ˆí•¨.
#### 2. Mixed precision (ì˜ˆ: bf16)ê³¼ gradient accumulation â†’ Accelerate-> ì´ê²Œ ì‹¤ì œë¡œ ì ìš©ë¨.


    def __init__(self,
                 model,
                 data,
                 device,
                 config,
                 only_for_eval=False,
                 monitor=None):
        num_train_batch = len(data['train'])#train dataloaderì˜ length. micro batch size ê¸°ì¤€ ì •í•´ì§„ ê²ƒ.


        """

        ìš°ë¦¬ê°€ ìƒê°í•˜ëŠ” ê²ƒì€ effective_batch

        effective batch size = batch_size Ã— grad_accum_step (1-gpu)

        effectiveÂ batchÂ size = per_device_batch_size Ã— grad_accum_step Ã— world_sizeÂ (í”„ë¡œì„¸ìŠ¤Â ìˆ˜) (ë‹¤ì¤‘ gpu, DDP)



        ì´ë¥¼ ê³ ë ¤í•˜ì—¬ grad_accum_step, world_size ë“±ì„ ê³ ë ¤í•˜ë©´ ëœë‹¤.


        LLM í•™ìŠµìš©: llm.grad_accum_step â†’ LLM forward/backward ë°˜ë³µ íšŸìˆ˜

        ì¼ë°˜ í•™ìŠµìš©: grad.grad_accum_count â†’ non-LLM forward/backward ë°˜ë³µ íšŸìˆ˜

        
        """



        #Gradient Accumulation Step ê³„ì‚°â†’ í•œ ë²ˆì˜ optimizer.step()ì„ ì—¬ëŸ¬ mini-batchì— ê±¸ì³ ìˆ˜í–‰í•  ìˆ˜ ìˆë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤. 
        self.grad_accum_step = min(
            num_train_batch,
            max(config.llm.grad_accum_step, config.grad.grad_accum_count)) #config.llm.grad_accum_step
        

        super().__init__(model, data, device, config, only_for_eval, monitor)
        model_name, _ = config.model.type.split('@')

        #tokenizer ì¤€ë¹„
        self.tokenizer, _ = get_tokenizer(model_name, config.data.root,
                                          config.llm.tok_len)
        
        self.eval_metrics = config.eval.metrics #[loss, acc]

        self.local_only = bool(getattr(config.eval, "local_only", False))

        self.ctx.current_round_num = 0
        self.ctx.cur_round_i = 0


        if config.llm.accelerator.use:   # âœ… ì¡°ê±´ë¬¸ ì¶”ê°€
            ddp_kwargs = DistributedDataParallelKwargs(
                find_unused_parameters=True, # ì–´ëŒ‘í„° ìŠ¤ì™‘ ë“± unused íŒŒë¼ë¯¸í„° ëŒ€ë¹„
                gradient_as_bucket_view=False, # bf16 ë²„í‚·ë·° ì´ìŠˆ íšŒí”¼
                broadcast_buffers=False
            )
            mp = getattr(config.llm, "mixed_precision", "bf16")
            self.accelerator = Accelerator(
                gradient_accumulation_steps=self.grad_accum_step,
                mixed_precision=mp,
                kwargs_handlers=[ddp_kwargs],
            )
            self.device = self.accelerator.device
        else:
            self.accelerator = None
            self.device = device

        # __init__ ëë¶€ë¶„(Accelerator ìƒì„± ì´í›„)
        self._prepared_once = False

        # SDPA ì»¤ë„ ì„ íƒ: í”Œë˜ì‹œ/ë©”ëª¨ë¦¬íš¨ìœ¨ off, ìˆ˜í•™ ì»¤ë„ on
        try:
            import torch
            if hasattr(torch.backends, "cuda") and hasattr(torch.backends.cuda, "sdp_kernel"):
                torch.backends.cuda.sdp_kernel(enable_flash=False, enable_mem_efficient=False, enable_math=True)
        except Exception:
            pass


        #ì‹¤ì œ ë¼ë²¨ì€ " A</s>"= "â–A"+"</s>" or " B</s>"="â–B"+"</s>"ì¸ ìƒí™©. ìµœì¢… ëª©í‘œ: "â–A", "â–B" ì´ë ‡ê²Œ 2ì°¨ì›ìœ¼ë¡œ Classifierë¥¼ ì¶•ì†Œ.
        try:
            # (ì¤‘ìš”) "choices" ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ë¥¼ í† í¬ë‚˜ì´ì¦ˆí•˜ì—¬
            # ë§ˆì§€ë§‰ í† í° IDë§Œ ì¶”ì¶œ â†’ í•´ë‹¹ í† í° ë“±ì¥ ì‹œ ê·¸ ìœ„ì¹˜ë¥¼ ì„ íƒì§€ í´ë˜ìŠ¤ ë¼ë²¨ë¡œ ì‚¬ìš©
            choices_list = [] #"â–A", "â–B"ì˜ token id ì¶”ì¶œì´ ëª©í‘œ.
            for choice in config.trainer.choices: #config.trainer.choices=["A", "B"]
                # ì•ì— ': 'ë¥¼ ë¶™ì´ëŠ” ì´ìœ : í”„ë¡¬í”„íŠ¸ í˜•ì‹ì—ì„œ ì‘ë‹µ í‘œê¸° ì§í›„ í† í°ì„ ì•ˆì •ì ìœ¼ë¡œ ë½‘ê¸° ìœ„í•¨. add_special_tokens=False:BOS/EOS ê°™ì€ ìŠ¤í˜ì…œ í† í°ì´ ë¼ì–´ë“¤ë©´ ì•ˆ ë˜ë¯€ë¡œ.
                token_ids = self.tokenizer(f': {choice}', add_special_tokens=False)['input_ids']
                #": A"ëŠ” ë³´í†µ [:, â–A]ì²˜ëŸ¼ 2ê°œ ì´ìƒ í† í°ìœ¼ë¡œ ìª¼ê°œì§‘ë‹ˆë‹¤. ì´ ì¤‘ì— **ì‹¤ì œë¡œ ë¶„ë¥˜ë¥¼ ê°ˆë¼ì£¼ëŠ” ê±´ ë§ˆì§€ë§‰ í† í°(= â–A)**ì´ì£ . ê·¸ë˜ì„œ [-1]ë§Œ ë½‘ìŠµë‹ˆë‹¤.
                if not token_ids: raise ValueError(f"Tokenizer returned empty list for choice: '{choice}'")
                choices_list.append(token_ids[-1])
            # CPU í…ì„œë¡œ ë³´ê´€, ë¼ìš´ë“œ ì‹œì‘ ì‹œ ë””ë°”ì´ìŠ¤ë¡œ ì˜®ê¹€
            self.choices_cpu = torch.tensor(choices_list, dtype=torch.long)
            logger.info(f'Choice token IDs: {self.choices_cpu.tolist()}')
        except Exception as e:
            logger.error(f"Error during trainer initialization: {e}")
            raise ValueError('Failed to initialize trainer.choices.')

        self._ct_ft = int(self.cfg.federate.total_round_num) == 1 # CT-FT ëª¨ë“œ(ë¼ìš´ë“œ=1)
        self._mid_eval_every = int(getattr(self.cfg.eval, "every_n_train_steps", -1)) if self._ct_ft else -1
        self._global_updates = 0

        # â¬‡ï¸ Early-Stop ìƒíƒœê°’ (CT-FTì¼ ë•Œë§Œ ìœ íš¨)
        self._es_enabled   = self._ct_ft and bool(getattr(self.cfg.eval, "early_stop_on_test_acc", True))
        self._es_patience  = int(getattr(self.cfg.eval, "early_stop_patience", 10))
        self._es_min_delta = float(getattr(self.cfg.eval, "early_stop_min_delta", 0.0))
        self._es_best      = float("-inf")
        self._es_wait      = 0
        self._es_triggered = False


        self._mid_eval_pending = False   
        self._mid_eval_running = False

        self._epoch_i_cache_for_train_loop = 0
        self._num_epoch_cache_for_train_loop = 1
        self._n_last_cache = None

    def _reset_and_build_dataloader(self, split_name): #í•™ìŠµ/í‰ê°€ì— ë“¤ì–´ê°€ê¸° ì§ì „ë§ˆë‹¤ ë¡œë”ë¥¼ ë‹¤ì‹œ ë§Œë“¤ê³ (ìƒ¤ë”© ë°˜ì˜)
        data_key = f"{split_name}_data"
        loader_key = split_name
        ctx_loader_key = f"{split_name}_loader"

        client_data_obj = self.ctx.data

        # ê¸°ì¡´ ë¡œë” ì œê±°. ê°™ì€ ë¼ìš´ë“œ/ë£¨í‹´ ë°˜ë³µ ì‹œ ë‚¡ì€ ì´í„°ë ˆì´í„°/ìƒ˜í”ŒëŸ¬ ìƒíƒœê°€ ì„ì´ì§€ ì•Šë„ë¡ ê¸°ì¡´ ë¡œë”ë¥¼ ê¹¨ë—ì´ ì§€ì›€.
        if loader_key in client_data_obj:
            del client_data_obj[loader_key]

        # ì›ë³¸ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ë¡œë” ìƒì„±
        if hasattr(client_data_obj, data_key) and getattr(client_data_obj, data_key) is not None:
            dataset = WrapDataset(getattr(client_data_obj, data_key))
            base_loader = get_dataloader(dataset, self.cfg, split_name) #sharding ì´ì „ì˜ ìƒí™©. ìƒ¤ë”© ì „(sampler=ì—†ê±°ë‚˜ ê¸°ë³¸).


            #dist_ready: torch.distributed í”„ë¡œì„¸ìŠ¤ ê·¸ë£¹ì´ ì´ˆê¸°í™”ë˜ì–´ ìˆëŠ”ì§€ ì—¬ë¶€(bool)

            #world_size: ì „ì²´ í”„ë¡œì„¸ìŠ¤ ìˆ˜(ì˜ˆ: 4)

            #rank: í˜„ì¬ í”„ë¡œì„¸ìŠ¤ì˜ ì „ì—­ ë­í¬(0,1,2,3 â€¦)

            dist_ready, world_size, rank = _get_dist_info()


            if world_size > 1:
                if split_name == "train":

                   # DistributedSampler (í›ˆë ¨ìš©)
                    #### ëª©ì : ê° ì—í­ë§ˆë‹¤ ì…”í”Œí•˜ê³ , ê° rankì— ë™ì¼ ê°œìˆ˜ì˜ ìƒ˜í”Œì„ ë°°ë¶„.

                    #### ì‘ë™:
                    ######## ì—í­ë§ˆë‹¤ set_epoch(e)ë¡œ ì‹œë“œ ê³ ì • â†’ ì „ rankê°€ ë™ì¼í•œ í¼ë®¤í…Œì´ì…˜ì„ ê³µìœ .

                    ######## ì „ì²´ ì¸ë±ìŠ¤ë¥¼ ì„ì€ ë’¤ ë™ì¼í•œ ê¸¸ì´ë¡œ ê· ë“± ë¶„í• .

                    ######## drop_last=Falseë©´ ê¸¸ì´ê°€ ë‚˜ëˆ„ì–´ë–¨ì–´ì§€ì§€ ì•Šì„ ë•Œ ì•ìª½ ì¸ë±ìŠ¤ë¥¼ ì¬ì‚¬ìš©(íŒ¨ë”©) í•´ì„œ num_samples = ceil(N / world_size)ë¥¼ ë§ì¶¤ â†’ í•œ ì—í­ ë‚´ ì¤‘ë³µ ê°€ëŠ¥(í›ˆë ¨ì—ì„œëŠ” í—ˆìš©/ê¶Œì¥).

                    #### ê¸¸ì´:

                    ######## len(sampler) = num_samples = ceil(N / world_size) (drop_last=False)

                    ######## íŠ¹ì§•: ì…”í”Œ/íŒ¨ë”©/ê· ë“±ë¶„í• /set_epoch ì§€ì› â†’ í›ˆë ¨ ì¹œí™”ì .


                    # num_samples = ceil(10/4)=3, total_size=12 â†’ ì•ì—ì„œ 2ê°œ íŒ¨ë”©(ì¤‘ë³µ)

                    # rank0: [0,1,2]
                    # rank1: [3,4,5]
                    # rank2: [6,7,8]
                    # rank3: [9,0,1] âŸµ íŒ¨ë”©ëœ ì¤‘ë³µ

                    # ì¥ì : ê° rank 3ê°œë¡œ ê· ë“± / ë‹¨ì : ì¤‘ë³µ ì¡´ì¬(í›ˆë ¨ OK)

                    sampler = DistributedSampler(
                        base_loader.dataset,
                        num_replicas=world_size,
                        rank=rank,
                        shuffle=True,
                    )
                    self._train_dist_sampler = sampler

                    loader = DataLoader(
                        dataset=base_loader.dataset,
                        batch_size=base_loader.batch_size,
                        sampler=sampler,
                        shuffle=False,
                        num_workers=getattr(base_loader, "num_workers", 0),
                        pin_memory=getattr(base_loader, "pin_memory", False),
                        drop_last=getattr(base_loader, "drop_last", False),
                        collate_fn=getattr(base_loader, "collate_fn", None),
                        persistent_workers=False, 
                    )
                else:
                    # EvalShardSampler

                    # rank0: [0,4,8] (3ê°œ)
                    # rank1: [1,5,9] (3ê°œ)
                    # rank2: [2,6] (2ê°œ)
                    # rank3: [3,7] (2ê°œ)

                    # ì¥ì : ì¤‘ë³µ ì—†ìŒ, ì „ì²´ ì •í™•íˆ 10ê°œ í‰ê°€ / ë‹¨ì : ê° rank ê¸¸ì´ê°€ ê· ë“±í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ(í‰ê°€ OK)

                    sampler = EvalShardSampler(len(base_loader.dataset), rank, world_size)

                    loader = DataLoader(
                        dataset=base_loader.dataset,
                        batch_size=base_loader.batch_size,
                        sampler=sampler,
                        shuffle=False,
                        num_workers=0,
                        pin_memory=False,
                        drop_last=False,
                        collate_fn=getattr(base_loader, "collate_fn", None),
                        persistent_workers=False, 
                    )

                sharded = True
                local_count = len(sampler) #rank ë³„ë¡œ ì²˜ë¦¬í•  ìƒ˜í”Œ(ì¸ë±ìŠ¤)ì˜ ê°œìˆ˜
            else:
                loader = base_loader
                sharded = False
                local_count = len(base_loader.dataset)

            client_data_obj[loader_key] = loader
            setattr(self.ctx, ctx_loader_key, loader)

            logger.info(
                f"Dataloader for '{split_name}' has been reset and recreated. "
                f"(sharded={sharded}, "
                f"world_size={1 if not sharded else world_size}, "
                f"rank={0 if not sharded else rank}, "
                f"local_count={local_count}, "
                f"total={len(base_loader.dataset)})"
            )

            try:
                tok = getattr(self, "tokenizer", None) or getattr(self.ctx, "tokenizer", None)
                mdl = getattr(self, "model", None) or getattr(self.ctx, "model", None)
                if tok is not None and mdl is not None:
                    log_tok_model_sync(tok, mdl, tag=f"after-reset-{split_name}")
            except Exception:
                pass
            
            barrier_all()
            if torch.cuda.is_available():                      
                torch.cuda.synchronize()





    def _set_round_ctx(self, rnd):
        rnd = int(rnd)
        # ì»¨í…ìŠ¤íŠ¸ì™€ íŠ¸ë ˆì´ë„ˆ ì–‘ìª½ì— aliasë¥¼ ëª¨ë‘ ì±„ì›Œì„œ ì–´ë””ì„œë“  ì°¸ì¡°ë˜ê²Œ
        for tgt in (self.ctx, self):
            setattr(tgt, "current_round_num", rnd)
            setattr(tgt, "current_round", rnd)
            setattr(tgt, "cur_round_i", rnd)
            setattr(tgt, "round_idx", rnd)


    def _unwrap(self, m):
        """Accelerate/DDP ë˜í•‘ëœ ëª¨ë¸ì—ì„œ ì›ë³¸ HF ëª¨ë¸ì„ ì•ˆì „í•˜ê²Œ ì–»ëŠ”ë‹¤."""
        if m is None:
            return None
        if getattr(self, "accelerator", None) is not None:
            try:
                m = self.accelerator.unwrap_model(m)
            except Exception:
                pass
        while hasattr(m, "module"):
            m = m.module
        return m



    def register_default_hooks_train(self):
        super().register_default_hooks_train()

        #ì¶”ê°€ì ìœ¼ë¡œ memory ì ˆì•½ìš© hook ë“±ë¡
        self.register_hook_in_train(self._hook_on_fit_end_free_space,
                                    "on_fit_end")

    def register_default_hooks_ft(self):
        super().register_default_hooks_ft()
        #ì¶”ê°€ì ìœ¼ë¡œ memory ì ˆì•½ìš© hook ë“±ë¡:
        self.register_hook_in_ft(self._hook_on_fit_end_free_space,
                                 "on_fit_end")

    def register_default_hooks_eval(self):
        super().register_default_hooks_eval()
        #ì¶”ê°€ì ìœ¼ë¡œ memory ì ˆì•½ìš© hook ë“±ë¡:
        self.register_hook_in_eval(self._hook_on_fit_end_free_space,
                                   "on_fit_end")




    @use_diff
    def train(self, target_data_split_name="train", hooks_set=None, round_num=-1):

        logger.info(f"[mid-eval] every_n_train_steps={self._mid_eval_every}")

        self._set_round_ctx(round_num)  # â† ì—¬ê¸°!
        # [ìˆ˜ì •] current_roundë¥¼ ë§¨ ë¨¼ì € ì •ì˜í•˜ì—¬ NameError í•´ê²°
        current_round = round_num

        # â˜… CT-FT & baseline ì˜µì…˜ì´ë©´, íŒŒì¸íŠœë‹ ì‹œì‘ ì „ì— 1íšŒ í‰ê°€
        if self._ct_ft and self._mid_eval_every > 0 and bool(getattr(self.cfg.eval, "baseline_before_ft", True)):
            self._mid_eval_once()




        scheduler_cfg = getattr(self.cfg.train, "scheduler", None)
        if scheduler_cfg:
            initial_lr = self.cfg.train.optimizer.lr
            milestones = getattr(scheduler_cfg, "milestones", [])
            gamma = getattr(scheduler_cfg, "gamma", 1.0)
            
            num_decays = sum(1 for milestone in sorted(milestones) if current_round >= milestone)
            new_lr = initial_lr * (gamma ** num_decays)
            
            self.ctx.new_lr_to_be_set = new_lr
            logger.info(
                f"[Stateless LR Controller] In Round #{current_round}, planning to set LR to {new_lr:.2e}"
            )

        self._reset_and_build_dataloader(target_data_split_name)
        try:
            # ì—¬ê¸°ê¹Œì§€ ì¤€ë¹„ ë â†’ ì‹¤ì œ í•™ìŠµ/í‰ê°€ëŠ” super().train ì•ˆì—ì„œ ëª¨ë‘ ì§„í–‰
            return super().train(target_data_split_name, hooks_set)
        finally:
            # âœ… ì´ í˜¸ì¶œì—ì„œ í•„ìš”í•œ ì¼ ì „ë¶€ ëë‚œ ì´í›„ì—ë§Œ ì •ë¦¬
            for k in ('train_loader', 'train_loader_iter', 'optimizer', 'scheduler', 'grad_scaler', 'scaler'):
                if hasattr(self.ctx, k):
                    delattr(self.ctx, k)

    def evaluate(self, target_data_split_name="test", hooks_set=None):
        hooks_set = hooks_set or self.hooks_in_eval
        self._reset_and_build_dataloader(target_data_split_name)
        with torch.no_grad():
            if self.ctx.check_split(target_data_split_name, skip=True):
                self._run_routine(MODE.TEST, hooks_set, target_data_split_name)
            else:
                self.ctx.eval_metrics = dict()
        return self.ctx.eval_metrics


    @lifecycle(LIFECYCLE.EPOCH) #í•œ ì—í­(epoch) ë‹¨ìœ„ë¡œ ë°˜ë³µ ì‹¤í–‰. ëë‚˜ë©´ ì—í­ìš© ì„ì‹œë³€ìˆ˜(CtxVar(..., "epoch")) ì¼ê´„ ì‚­ì œ
    def _run_epoch(self, hooks_set, run_step=-1):


        if run_step == -1:
            run_step = getattr(self.ctx, f"num_{self.ctx.cur_split}_epoch")#ì´ epoch ìˆ˜. batchë“  epoch ëª¨ë“œì´ë“  total data ë£¨í”„ ëª‡ ë²ˆ ë„ëŠ”ì§€ ê³„ì‚°ë¨. test/valì¼ë•ŒëŠ” 1.
        for epoch_i in range(run_step):
            self.ctx.cur_epoch_i = CtxVar(epoch_i, "epoch")

            for hook in hooks_set["on_epoch_start"]:
                hook(self.ctx)

            self._run_batch(hooks_set)#llm trainerë¡œ override. run_step=-1ë¡œ ëœë‹¤ëŠ” ê²ƒ ìœ ì˜. ë§ˆì§€ë§‰ epochì˜ ë¶€ì¡±í•œ batch updateë„ ê³ ë ¤.

            # â˜… [ADD] ë°°ì¹˜ ë£¨í”„ì—ì„œ íŠ¸ë¦¬ê±°ë˜ë©´ ì—í­ë„ ì¦‰ì‹œ ì¢…ë£Œ
            if getattr(self, "_es_triggered", False):
                break


            for hook in hooks_set["on_epoch_end"]:
                hook(self.ctx)




    @lifecycle(LIFECYCLE.BATCH)
    #ì—…ë°ì´íŠ¸ 1íšŒë¥¼ ì–´ë–»ê²Œ ë§Œë“œëŠ”ì§€ë¥¼ ê²°ì •.
    def _run_batch(self, hooks_set, run_step=-1): #í•œ epoch ë‚´ì—ì„œ batch 1ê°œ ë„ëŠ” ê±¸ ì—¬ëŸ¬ë²ˆ ë°˜ë³µ. run_epochì—ì„œëŠ” run_step=-1 ì ìš©. ë‹¤ë¥¸ ê³³ì—ì„œëŠ” run_step=1 ì ìš©.
        # 1) grad_accum_step
        if self.ctx.cur_mode in [MODE.TRAIN, MODE.FINETUNE]:
            """
            Train ë‹¨ê³„:

            gradient: ìë™ ì§‘ê³„ (DDP).

            loss/metrics: print/loggingìš©ìœ¼ë¡œëŠ” ì§‘ê³„ ì²˜ë¦¬ ìˆìŒ (aggregate).

            raw ê¸°ë¡ì€ rank0ë§Œ ë‚¨ëŠ” êµ¬ì¡°ì¼ ìˆ˜ ìˆìŒ.

            Eval ë‹¨ê³„:

            loss/metrics: ë°˜ë“œì‹œ accelerator.gather_for_metrics() â†’ monitor.aggregate() ê±°ì³ì„œ global average ê¸°ë¡.
            """
            grad_accum_step = self.grad_accum_step

        else:
            #loss.backward() ìì²´ë¥¼ í•˜ì§€ ì•ŠìŒ â†’ gradient ì—†ìŒ. 
            # ë”°ë¼ì„œ: 1. optimizer.step() í˜¸ì¶œ ì—†ìŒ. 
            # 2. gradient accumulation ê°œë… ìì²´ê°€ ë™ì‘ ì•ˆ í•¨. 
            # 3. DDP(DistributedDataParallel)ëŠ” forward ì‹œì ì—ë§Œ í†µì‹ (all-gather) í•„ìš” â†’ backward ì—†ìŒ â†’ all-reduceë„ ì—†ìŒ.

 
            
            grad_accum_step = 1  
 

        split = self.ctx.cur_split
        loader = self.ctx.get(f"{split}_loader")
        if loader is None:
            return



        # DDPì¼ ë•Œ ë§¤ epochë§ˆë‹¤ ëª¨ë“  ë­í¬ê°€ ë™ì¼í•œ ì…”í”Œ ì‹œë“œ ì²´ê³„ë¥¼ ê³µìœ í•˜ê¸° ìœ„í•´ ë§¤ ì—í­ë§ˆë‹¤ set_epoch. 
        # ê° í”„ë¡œì„¸ìŠ¤ê°€ ê°€ì§„ DataLoader í•˜ë‚˜ê°€ DistributedSampler(rank, num_replicas) ë•Œë¬¸ì— â€˜ìê¸° ëª«â€™ë§Œ ì½ê²Œ í•¨.

        """
        ê° ë­í¬ëŠ” ìê¸° shardì—ì„œ ë‹¤ìŒ ë§ˆì´í¬ë¡œë°°ì¹˜ë¥¼ ìˆœì„œëŒ€ë¡œ ì†Œëª¨.

        effective batch: í•œ â€œì—…ë°ì´íŠ¸ 1íšŒâ€ ë™ì•ˆ

        ë­í¬ë³„ë¡œ per_process_batch_size Ã— grad_accum_step ìƒ˜í”Œ ì†Œë¹„

        ì „ ë­í¬ í•©ì¹˜ë©´ per_process_batch_size Ã— grad_accum_step Ã— world_size (= ê¸€ë¡œë²Œ ìœ íš¨ë°°ì¹˜)

        """

        if split == "train":
            sampler = getattr(loader, "sampler", None)
            try:
                from torch.utils.data.distributed import DistributedSampler
                if isinstance(sampler, DistributedSampler): #True
                    epoch_seed = int(getattr(self.ctx, "cur_epoch_i", 0))
                    sampler.set_epoch(epoch_seed)
            except Exception:
                pass


        iter_name = f"{split}_loader_iter"

        if self.ctx.cur_mode in [MODE.TRAIN, MODE.FINETUNE]:
            if run_step == -1: #train ë–„ ì¼ë°˜ì ìœ¼ë¡œ ì´ê²ƒì— ê±¸ë¦¼.
                num_batches = getattr(self.ctx, f"num_{split}_batch") ##sharding ê³ ë ¤í•˜ì§€ ì•Šê³  ì¼ë°˜ì ì¸ epochë‹¹ micro batch ê¸°ì¤€ ì´  batch ê°œìˆ˜. ë‹¤ë§Œ batch_or_epoch == "batch"ì´ë©´ iteration ìˆ˜ì™€ ë¹„êµí–ˆì„ ë•Œ min ê°’ìœ¼ë¡œ ì„¤ì •ë¨.

                logger.info(f"[run-batch-setup] split={split}, "
                            f"len(loader)={len(loader)}, num_batches(ctx)={num_batches}, "
                            f"grad_accum_step={grad_accum_step}, "
                            f"will_run_step(loops)={num_batches * (grad_accum_step if (self.accelerator is not None) else 1)}")




                if hasattr(self, 'accelerator') and self.accelerator is not None:
                    # (microbatch size*grad_accum_step) ì‚¬ì´ì¦ˆë¥¼ num_batchesë§Œí¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•¨

                    #ë£¨í”„ ë³¸ë¬¸: with self.accelerator.accumulate: ì•ˆì—ì„œ ë§ˆì´í¬ë¡œë°°ì¹˜ 1ê°œ ì²˜ë¦¬
                    #ê²°ê³¼: ë§ˆì´í¬ë¡œë°°ì¹˜ ì´ ì†Œë¹„ëŸ‰ì´ NÃ—gê°€ ë˜ì–´, ë°ì´í„°ì…‹ì„ gë°° ë” ë
                    run_step = num_batches * max(1, grad_accum_step)

                    self.ctx.num_train_batch_last_epoch = self.ctx.num_train_batch_last_epoch * max(1, grad_accum_step)

                else: #ì•ˆìª½ì—ì„œ for k in range(grad_accum_step)ë¡œ ë§ˆì´í¬ë¡œë°°ì¹˜ grad_accum_stepê°œ ì²˜ë¦¬ â†’ ì—…ë°ì´íŠ¸ 1íšŒ
                    # run_step = math.ceil(num_batches / max(1, grad_accum_step))
                    run_step = num_batches


             # ì´í„°ë ˆì´í„°: í•„ìš”ì‹œ ì¬ì‹œì‘(ë°ì´í„°ê°€ ë” ì ì€ ê²½ìš°ë¥¼ ëŒ€ë¹„)
            if not hasattr(self.ctx, iter_name):
                setattr(self.ctx, iter_name, iter(loader))
            data_loader_iter = getattr(self.ctx, iter_name)

        else:
            # âœ… TEST, VAL: ìƒ¤ë”©ëœ ë¡œë” ê¸¸ì´ë§Œí¼ë§Œ 1íšŒ(grad_accum_step = 1)
            if run_step == -1:
                run_step = len(loader) #microbatch size ê¸°ì¤€ ê¸¸ì´.
            # ì´í„°ë ˆì´í„°: ë§¤ í‰ê°€ ë£¨í‹´ë§ˆë‹¤ ìƒˆë¡œ ì‹œì‘, StopIteration â†’ ì¢…ë£Œ
            data_loader_iter = iter(loader)
            setattr(self.ctx, iter_name, data_loader_iter)

        exhausted = False
        for update_i in range(run_step):
            self.ctx.cur_batch_i = CtxVar(update_i, LIFECYCLE.BATCH)
            # âœ… ë§¤ ë°°ì¹˜ë§ˆë‹¤ ê¸°ë³¸ê°’ ì„¸íŒ…
            self.ctx.skip_this_batch = CtxVar(False, LIFECYCLE.BATCH)
            e = getattr(self.ctx, "cur_epoch_i", None)
            if e is not None:
                self._epoch_i_cache_for_train_loop = int(e)
            ne = getattr(self.ctx, "num_train_epoch", None)
            if ne is not None:
                self._num_epoch_cache_for_train_loop = int(ne)
            nlast = getattr(self.ctx, "num_train_batch_last_epoch", None)
            if nlast is not None:
                self._n_last_cache = int(nlast)
            elif self._n_last_cache is None:
                # í´ë°±: splitë³„ ë°°ì¹˜ ìˆ˜
                self._n_last_cache = int(getattr(self.ctx, f"num_{split}_batch", 0))

            if hasattr(self, 'accelerator') and self.accelerator is not None: #accelerator ëª¨ë“œì—ì„œ ì‹¤í–‰. 
                # âœ… grad_accum_step ë£¨í”„ ì œê±°: accumulate ë¸”ë¡ë§Œ ì‚¬ìš©
                try:
                    self.ctx.data_batch = next(data_loader_iter)
                except StopIteration:
                    if self.ctx.cur_mode in [MODE.TRAIN, MODE.FINETUNE]:
                        # TRAIN: ìŠ¤í…ì´ ë‚¨ì•„ ìˆìœ¼ë©´ ë°ì´í„° ì¬ì‹œì‘
                        data_loader_iter = iter(loader)
                        setattr(self.ctx, iter_name, data_loader_iter)
                        try:
                            self.ctx.data_batch = next(data_loader_iter)
                        except StopIteration:
                            exhausted = True
                            break
                    else:
                        # EVAL: ì¬ì‹œì‘í•˜ì§€ ì•Šê³  ì¢…ë£Œ
                        exhausted = True
                        break

                #AccelerateëŠ” ë‚´ë¶€ ì¹´ìš´í„°ë¡œ self.grad_accum_step-1ë²ˆì€ no_sync(í†µì‹ /step ì—†ìŒ), self.grad_accum_stepë²ˆì§¸ì—ë§Œ all-reduce + step.
                #ë”°ë¼ì„œ ì—…ë°ì´íŠ¸ 1íšŒ = ì´ ë¸”ë¡ì´ self.grad_accum_stepë²ˆ ì‹¤í–‰ëœ ì‹œì .
                with self.accelerator.accumulate(self.ctx.model): #ë§ˆì´í¬ë¡œë°°ì¹˜ 1ê°œë¥¼ ì²˜ë¦¬í•˜ëŠ” ë‹¨ìœ„ ë¸”ë¡.
                    for hook in hooks_set["on_batch_start"]:
                        hook(self.ctx)
                    for hook in hooks_set["on_batch_forward"]:
                        hook(self.ctx)
                    for hook in hooks_set["on_batch_backward"]:
                        hook(self.ctx)
                    for hook in hooks_set["on_batch_end"]:
                        hook(self.ctx)

                # â† ì—¬ê¸°! í•œ ë°°ì¹˜ì˜ ë¼ì´í”„ì‚¬ì´í´ì´ ëë‚œ ì•ˆì „ ì§€ì 
                if self.ctx.cur_mode in (MODE.TRAIN, MODE.FINETUNE) and self._mid_eval_pending:
                    self._mid_eval_pending = False
                    self._mid_eval_once()

                # â˜… [ADD] ì¡°ê¸°ì¢…ë£Œì´ë©´ ë£¨í”„ íƒˆì¶œ
                if self.ctx.cur_mode in (MODE.TRAIN, MODE.FINETUNE) and getattr(self, "_es_triggered", False):
                    exhausted = True
                    break
            else:
                for k in range(grad_accum_step):
                    self.ctx.cur_batch_i = CtxVar(update_i * grad_accum_step + k,
                                                LIFECYCLE.BATCH)
                    try:
                        self.ctx.data_batch = next(data_loader_iter)
                    except StopIteration:
                        if self.ctx.cur_mode in [MODE.TRAIN, MODE.FINETUNE]:
                            data_loader_iter = iter(loader)
                            setattr(self.ctx, iter_name, data_loader_iter)
                            try:
                                self.ctx.data_batch = next(data_loader_iter)
                            except StopIteration:
                                exhausted = True
                                break
                        else:
                            exhausted = True
                            break

                    for hook in hooks_set["on_batch_start"]:
                        hook(self.ctx)
                    for hook in hooks_set["on_batch_forward"]:
                        hook(self.ctx)
                    for hook in hooks_set["on_batch_backward"]:
                        hook(self.ctx)
                    for hook in hooks_set["on_batch_end"]:
                        hook(self.ctx)
                # â† ì—¬ê¸°! ëˆ„ì ì´ ëë‚˜ optimizer.step()ê°€ ë°œìƒí–ˆì„ ë•Œë§Œ í”Œë˜ê·¸ê°€ ì¼œì§
                if self.ctx.cur_mode in (MODE.TRAIN, MODE.FINETUNE) and self._mid_eval_pending:
                    self._mid_eval_pending = False
                    self._mid_eval_once()
                # â˜… [ADD] ì¡°ê¸°ì¢…ë£Œì´ë©´ ë£¨í”„ íƒˆì¶œ
                if self.ctx.cur_mode in (MODE.TRAIN, MODE.FINETUNE) and getattr(self, "_es_triggered", False):
                    exhausted = True
                    break

            if exhausted:
                break

            # if self.ctx.cur_mode in [MODE.TRAIN, MODE.FINETUNE] and self.ctx.cur_epoch_i == self.ctx.num_train_epoch - 1: 
            #     logger.info(f"[break-check] update_i={update_i}, "
            #                 f"num_train_batch_last_epoch={self.ctx.num_train_batch_last_epoch}")

            #     if update_i >= self.ctx.num_train_batch_last_epoch - 1: 
            #         break    

            if self.ctx.cur_mode in (MODE.TRAIN, MODE.FINETUNE): # í•™ìŠµ/íŒŒì¸íŠœë‹ ëª¨ë“œ í™•ì¸ ë° ë§ˆì§€ë§‰ ì—í­ì¸ì§€ í™•ì¸
                cur_epoch_i = int(getattr(self.ctx, "cur_epoch_i",
                                        self._epoch_i_cache_for_train_loop))
                num_train_epoch = int(getattr(self.ctx, "num_train_epoch",
                                            self._num_epoch_cache_for_train_loop))
                n_last = int(getattr(self.ctx, "num_train_batch_last_epoch",
                                    self._n_last_cache if self._n_last_cache is not None
                                    else getattr(self.ctx, f"num_{split}_batch", 0)))

                if cur_epoch_i == max(0, num_train_epoch - 1): #ë§ˆì§€ë§‰ ì—í­ì˜ ì‹¤ì œ ìŠ¤í…(ì—…ë°ì´íŠ¸) ìˆ˜ ì´ˆê³¼ ì—¬ë¶€ íŒì •
                    if update_i >= max(0, n_last - 1):
                        break #ì¡°ê¸° ì¢…ë£Œ(break) 
 
    # def _hook_on_fit_start_numerical_precision(self, ctx):
    #     if self.cfg.train.is_enable_half:
    #         if not ctx.cfg.llm.deepspeed.use and \
    #            not ctx.cfg.llm.accelerator.use:
    #             ctx.model.to(torch.bfloat16)


    def _hook_on_fit_start_init(self, ctx):  #accelerate ë° deepspeed ì§€ì› ì¶”ê°€

        """ì„¸ ê°€ì§€ ê²½ìš°ë¡œ ë‚˜ë‰©ë‹ˆë‹¤:
        âœ… Accelerator ì‚¬ìš© ì‹œ

        self.accelerator.prepare(...) â†’ model, optimizer, dataloader, scheduler ëª¨ë‘ ê°ì‹¸ì„œ Mixed precision, DDP ë“±ì„ ì²˜ë¦¬í•´ì¤Œ
        
        âœ… Deepspeed ì‚¬ìš© ì‹œ ->ì´ê±´ í•´ë‹¹ì•ˆí•˜ë‹ˆ ì•ˆë´ë„ ë  ê²ƒ ê°™ìŒ.
        deepspeed.initialize(...)â†’ deepspeed ë°©ì‹ìœ¼ë¡œ ëª¨ë¸ì„ ë¶„ì‚°/íš¨ìœ¨ì ìœ¼ë¡œ í•™ìŠµ ê°€ëŠ¥í•˜ë„ë¡ ì´ˆê¸°í™”


        âœ… ê¸°ë³¸ PyTorchë§Œ ì‚¬ìš©í•˜ëŠ” ê²½ìš°
        â†’ ê·¸ëƒ¥ .to(device) í›„ ì˜µí‹°ë§ˆì´ì € ì„¸íŒ…
        
        """

        # --- ë¡œë” ë³µì›: ì´ì „ ë¼ìš´ë“œì—ì„œ ì§€ì› ë˜ train/val/test ë¡œë”ë¥¼ ë‹¤ì‹œ ë°”ì¸ë”© ---
        # self.ctx.data ì— ë”°ë¼ ë‘ ì¼€ì´ìŠ¤ë¥¼ ëª¨ë‘ ì§€ì›:
        #   1) ì´ë¯¸ DataLoader ë¥¼ ë“¤ê³  ìˆëŠ” dict í˜•íƒœ: self.ctx.data['train'|'val'|'test']
        #   2) raw dataset ì„ ë“¤ê³  ìˆëŠ” ì»¨í…Œì´ë„ˆ: ctx.get(f"{split}_data") ë˜ëŠ” self.ctx.data.<split>_data
        for split in ["train", "val", "test"]:
            loader_key = f"{split}_loader"
            if getattr(ctx, loader_key, None) is None:
                # (1) dictì— DataLoaderê°€ ì§ì ‘ ë“¤ì–´ì˜¨ ê²½ìš°
                if isinstance(self.ctx.data, dict) and split in self.ctx.data:
                    setattr(ctx, loader_key, self.ctx.data[split])
                else:
                    # (2) raw datasetì—ì„œ ìƒˆë¡œ ë¡œë”ë¥¼ ë§Œë“ ë‹¤
                    raw = ctx.get(f"{split}_data", None)
                    if raw is None and hasattr(self.ctx.data, f"{split}_data"):
                        raw = getattr(self.ctx.data, f"{split}_data")
                    if raw is not None:
                        dl = get_dataloader(WrapDataset(raw), self.cfg, split)
                        setattr(ctx, loader_key, ReIterator(dl))
        # (ì•ˆì „) ì´ì „ ë¼ìš´ë“œ ì”ì—¬ ì´í„°ë ˆì´í„°ê°€ ë‚¨ì•„ìˆì§€ ì•Šë„ë¡ ë³´ì¥
        for it_name in ["train_loader_iter", "val_loader_iter", "test_loader_iter"]:
            if hasattr(ctx, it_name):
                delattr(ctx, it_name)


        # ë¦¬ì…‹ ì§í›„, ë‹¤ìŒ ë‹¨ê³„ë¡œ ë„˜ì–´ê°€ê¸° ì „ì— ë­í¬ ë™ê¸°í™”
        if self.accelerator is not None:
            self.accelerator.wait_for_everyone()
            barrier_all()
            torch.cuda.synchronize()
        
        if ctx.cfg.llm.accelerator.use:
            # âœ… AcceleratorëŠ” ì´ë¯¸ __init__ì—ì„œ 1íšŒ ìƒì„±ë¨
            ctx.device = self.accelerator.device
            base = self._unwrap(self.model)

            if not self._prepared_once:
                # ìµœì´ˆ 1íšŒë§Œ ëª¨ë¸ prepare
                ctx.model = self.accelerator.prepare(base)
                self.model = ctx.model
                self._prepared_once = True


            else:
                # ì¤€ë¹„ëœ ëª¨ë¸ ì¬ì‚¬ìš© (re-prepare ê¸ˆì§€)
                ctx.model = self.model

            if ctx.cur_mode in [MODE.TRAIN, MODE.FINETUNE]:
                ctx.optimizer = get_optimizer(self._unwrap(ctx.model), **ctx.cfg[ctx.cur_mode].optimizer)
                ctx.scheduler = None
        else:
            ctx.model = self.model.to(ctx.device)
            if ctx.cur_mode in [MODE.TRAIN, MODE.FINETUNE]:
                ctx.optimizer = get_optimizer(ctx.model, **ctx.cfg[ctx.cur_mode].optimizer)
                ctx.scheduler = None

        # âœ… ê²€ì¦ë§Œ ìœ ì§€(ì¶”ê°€/ë¦¬ì‚¬ì´ì¦ˆëŠ” ì—†ìŒ)
        try:
            tok = getattr(self, "tokenizer", None)
            base = self._unwrap(ctx.model)
            if tok is not None and base is not None:
                tlen = len(tok)
                elen = base.get_input_embeddings().weight.size(0)
                if tlen != elen:
                    raise RuntimeError(f"[Guard] tokenizer/embedding mismatch: {tlen} vs {elen}")
        except Exception as e:
            logger.warning(f"[Tokenizer guard] {e}")


        if ctx.cur_mode in [MODE.TRAIN, MODE.FINETUNE]:
            ctx.model.train()
        else:  # MODE.TEST or MODE.VAL
            ctx.model.eval()


        # prepare statistics
        ctx.loss_batch_total = CtxVar(0., LIFECYCLE.ROUTINE)
        ctx.loss_regular_total = CtxVar(0., LIFECYCLE.ROUTINE)
        ctx.num_samples = CtxVar(0, LIFECYCLE.ROUTINE)
        ctx.ys_true = CtxVar([], LIFECYCLE.ROUTINE)
        ctx.ys_prob = CtxVar([], LIFECYCLE.ROUTINE)

        ctx.sample_seen = CtxVar(0, LIFECYCLE.ROUTINE)
        ctx.sample_correct_accum = CtxVar(0, LIFECYCLE.ROUTINE)



        if hasattr(ctx, 'ys_pred'): 
            ctx.ys_pred = CtxVar([], LIFECYCLE.ROUTINE)


        if hasattr(ctx, "new_lr_to_be_set"):
            new_lr = ctx.new_lr_to_be_set
            if hasattr(ctx, 'optimizer') and ctx.optimizer is not None:
                opt = ctx.optimizer
                for param_group in opt.param_groups:
                    param_group['lr'] = new_lr
                logger.info(f"Successfully applied new LR {new_lr:.2e} to the optimizer.")
                del ctx.new_lr_to_be_set
     
        current_round = int(getattr(ctx, "current_round_num", getattr(ctx, "cur_state", 0)))
 
        # train/val/test ì„±ëŠ¥ ì¸¡ì •ìš© ì¹´ìš´í„° ë¦¬ì…‹
        for sp in ["train", "val", "test"]:
            setattr(ctx, f"num_samples_{sp}", 0)
            setattr(ctx, f"loss_total_{sp}", 0.0)
            setattr(ctx, f"correct_{sp}", 0)

        self.choices = self.choices_cpu.to(ctx.device, dtype=torch.long, non_blocking=True) #non_blocking=TrueëŠ” pinned memory í™˜ê²½ì—ì„œ async copy í—ˆìš© â†’ ì„±ëŠ¥ ìµœì í™”.


        try:
            # GeneralTorchTrainerê°€ ê³„ì‚°í•´ ë‘” ì»¨í…ìŠ¤íŠ¸ ê°’ í™•ì¸
            boe = getattr(self.ctx, "batch_or_epoch", None)
            it  = getattr(self.ctx, "train_iter", None)
            nb  = getattr(self.ctx, "num_train_batch", None)
            nb_last = getattr(self.ctx, "num_train_batch_last_epoch", None)
            ne  = getattr(self.ctx, "num_train_epoch", None)
            logger.info(f"[fit-start] batch_or_epoch={boe}, train_iter={it}, "
                        f"num_train_batch={nb}, num_train_batch_last_epoch={nb_last}, "
                        f"num_train_epoch={ne}, grad_accum_step={self.grad_accum_step}, "
                        f"accum_in_accel={getattr(self.accelerator,'gradient_accumulation_steps',None)}")
            

            
        except Exception as e:
            logger.warning(f"[fit-start] debug read failed: {e}")


        # === [PATCH] batch ëª¨ë“œì—ì„œë§Œ ìŠ¤í… ê¸°ë°˜ ìŠ¤ì¼€ì¤„ì„ ê°•ì œ ===
        try:
            if ctx.cur_mode in [MODE.TRAIN, MODE.FINETUNE] \
            and getattr(ctx.cfg.train, "batch_or_epoch", "batch") == "batch":
                lus = int(ctx.cfg.train.local_update_steps)  # ì˜ˆ: 800
                # ì˜ë¯¸ìƒ í•œ ì—í­ìœ¼ë¡œ ê³ ì •
                ctx.num_train_epoch = 1
                # "num_batches"ë¥¼ ê³§ë°”ë¡œ ì›í•˜ëŠ” optimizer step ê°œìˆ˜ë¡œ ì§€ì •
                ctx.num_train_batch = lus
                ctx.num_train_batch_last_epoch = lus
                logger.info(f"[force-step-schedule] epoch=1, num_batches={lus}, "
                            f"grad_accum_step={self.grad_accum_step} "
                            f"(=> total micro-batches = {lus * max(1, self.grad_accum_step)})")
        except Exception as e:
            logger.warning(f"[force-step-schedule] skip due to: {e}")



 
    #local processì—ì„œë§Œ ì¼ì–´ë‚˜ëŠ” ì¼. ê° rankê°€ ìê¸° shardëœ ë¯¸ë‹ˆë°°ì¹˜ë¥¼ forward í•˜ê³  lossë¥¼ ê³„ì‚°í•˜ëŠ” ê²ƒ.
    def _hook_on_batch_forward(self, ctx): #ctx.data_batch â†’ input_ids, attention_mask, labelsë¡œ ëª…ì‹œ ì²˜ë¦¬ + ëª¨ë¸ í˜¸ì¶œ ë°©ì‹ ë³€ê²½. ê¸°ì¡´ toprch trainerëŠ” CNN, MLP, BERT ìš©ì´ë¼ LLMìš©ì€ ì•„ë‹ˆë¼ì„œ OVERRIDE í•´ì•¼í•¨.

        """ 
        (ê¸°ì¡´)->ì¼ë°˜ ëª¨ë¸ (ì˜ˆ: CNN, MLP, BERT ë“±)ìš©
        x, label = [_.to(ctx.device) for _ in ctx.data_batch]
        pred = ctx.model(x)
        loss = ctx.criterion(pred, label)

        ğŸ” íŠ¹ì§•
        ctx.data_batchëŠ” (x, label) íŠœí”Œ í˜•íƒœ

        model(x)ë§Œìœ¼ë¡œ ì˜ˆì¸¡ ê°€ëŠ¥

        loss = criterion(pred, label)ë¡œ ì†ì‹¤ ê³„ì‚°

        ë§¤ìš° ë‹¨ìˆœí•˜ê³  ì§ê´€ì ì¸ êµ¬ì¡°



        (OVERRIDE ë²„ì „)->LLM í•™ìŠµìš© override


        input_ids = ctx.data_batch['input_ids']
        labels = ctx.data_batch['labels']
        attention_mask = ctx.data_batch['attention_mask']
        ...
        outputs = model(input_ids=..., labels=..., attention_mask=...)


        ğŸ” ì£¼ìš” ë³€ê²½ ì´ìœ  ìš”ì•½
        ë³€ê²½ ìš”ì†Œ	ì´ìœ 
        data_batchê°€ dictë¡œ ë°”ë€œ	LLMì€ (input_ids, attention_mask, labels) í˜•íƒœë¡œ í•™ìŠµ
        model(input_ids=..., labels=...) ì‚¬ìš©	Huggingface LLMì€ forward()ì—ì„œ lossê¹Œì§€ ê°™ì´ ë°˜í™˜í•¨
        ctx.model_engine(...) ë¶„ê¸°	DeepSpeed ë“± ì‚¬ìš©í•  ê²½ìš° Acceleratorë‚˜ Engineì´ ëª¨ë¸ì„ wrapping
        skip_this_batch ì¶”ê°€	LLM í•™ìŠµì—ì„œ NaN lossê°€ ë¹ˆë²ˆ â†’ ë°©ì–´ ë¡œì§ í•„ìš”


        """

 
        #LLM í•™ìŠµìš© Datasetì€ ë³´í†µ tokenized_dictë¥¼ ë°˜í™˜->{'input_ids': Tensor, 'labels': Tensor, 'attention_mask': Tensor}
        #acceleratorëŠ” ì…ë ¥ í…ì„œë¥¼ ìë™ìœ¼ë¡œ ë””ë°”ì´ìŠ¤ì— ì˜¬ë ¤ì¤˜ì„œ .to(ctx.device)ê°€ í•„ìš” ì—†ìŒ.


        #input_ids: [101,  42,  53,  78,   2]
        #labels:    [101,  42,  53,  78,   2]  # í˜¹ì€ ì¼ë¶€ -100 í¬í•¨
        #ì´ë ‡ê²Œ ê±°ì˜ ê°™ì§€ë§Œ, loss ê³„ì‚°ì—ì„œ ë¬´ì‹œí•  í† í°ì€ labelsì—ì„œë§Œ ë°”ë€œ.

        # ---- ì„ë² ë”© ê²½ê³„ ì´ˆê³¼ ê°€ë“œ ----
        try:
            base_model = self._unwrap(getattr(self.ctx, "model", None))
            if base_model is not None and "input_ids" in self.ctx.data_batch:
                emb_rows = base_model.get_input_embeddings().weight.shape[0]
                # ë¹ ë¥¸ ì¥ì¹˜ ë¬´ê´€ max (GPU í…ì„œë©´ .amax)
                mx = int(self.ctx.data_batch["input_ids"].amax().item())
                if mx >= emb_rows:
                    rk = getattr(self.accelerator, "process_index", "?") if getattr(self, "accelerator", None) else "?"
                    logger.error(f"[TOK_OVF] rank={rk} max_id={mx} >= emb_rows={emb_rows} | split={self.ctx.cur_split}")
                    raise RuntimeError(f"Token id overflow: {mx} >= {emb_rows}")
        except Exception:
            pass

        # 1) ì…ë ¥ ì¤€ë¹„
        input_ids = ctx.data_batch['input_ids'].to(ctx.device)
        labels = ctx.data_batch['labels'].to(ctx.device)
        attention_mask = ctx.data_batch['attention_mask'].to(ctx.device)

        # 2) ëª¨ë¸ ì‹¤í–‰ 
        if ctx.cfg.llm.accelerator.use: #ì°¸ê³ , #ctx.model ê¸°ë°˜ìœ¼ë¡œ outputs ë½‘ì•„ë‚¸ë‹¤.
            outputs = ctx.model(
                input_ids=input_ids,
                labels=labels,
                attention_mask=attention_mask,
            )
        elif ctx.cfg.llm.deepspeed.use: #ctx.model_engine ê¸°ë°˜ìœ¼ë¡œ outputs ë½‘ì•„ë‚¸ë‹¤.
            outputs = ctx.model_engine(
                input_ids=input_ids,
                labels=labels,
                attention_mask=attention_mask,
            )
        else: #ì°¸ê³ , #ctx.model ê¸°ë°˜ìœ¼ë¡œ outputs ë½‘ì•„ë‚¸ë‹¤.
            outputs = ctx.model(
                input_ids=input_ids,
                labels=labels,
                attention_mask=attention_mask,
            )

        logits = outputs.logits #ì¼ë°˜ì ìœ¼ë¡œ LLMì—ì„œëŠ” [batch_size, seq_len, vocab_size] :=[B,T,V] shapeì˜ tensor
        loss = outputs.loss #ì¼ë¶€ ëª¨ë¸ì—ì„œ reduction='none'ì´ë©´ shapeê°€ [batch_size] í˜¹ì€ [batch_size, seq_len]ì¼ ìˆ˜ë„ ìˆì§€ë§Œ, ëŒ€ë¶€ë¶„ì€ 1ê°œì˜ ìŠ¤ì¹¼ë¼ ê°’

        # NaN/Inf ëª¨ë‘ ë°©ì–´
        if not torch.isfinite(loss): #LM í•™ìŠµì—ì„œëŠ” ì¢…ì¢… NaN lossê°€ ë°œìƒí•  ìˆ˜ ìˆìŒ-> labelì´ ëª¨ë‘ -100 (ignore index). precision ë¬¸ì œ (e.g., bf16/float16). exploding gradients, bad initialization
            ctx.skip_this_batch = CtxVar(True, LIFECYCLE.BATCH) #ë‹¤ë¥¸ hookì—ì„œ ì´ ê°’ì´ Trueë©´ ì´ ë°°ì¹˜ë¥¼ ê±´ë„ˆëœ€. (ì˜ˆ: loss.backward() ìŠ¤í‚µ)
            logger.warning(f"Skip batch: non-finite loss={loss.item()}")
            return
        else:
            ctx.skip_this_batch = CtxVar(False, LIFECYCLE.BATCH)

 

        # 4) ì¼ë°˜ LM ë©”íŠ¸ë¦­ìš© ì €ì¥
        ctx.y_true = CtxVar(labels, LIFECYCLE.BATCH) #shape: [batch_size, seq_len]
        ctx.y_prob = CtxVar(logits, LIFECYCLE.BATCH) # shape: [batch_size, seq_len, vocab_size]
        ctx.loss_batch = CtxVar(loss, LIFECYCLE.BATCH)
        ctx.batch_size = CtxVar(len(labels), LIFECYCLE.BATCH)


    
        # 5) A/B ì„ íƒ ì •í™•ë„(ì²« ìœ íš¨ í† í°) ì¶”ê°€ ì§‘ê³„
        #    - self.choices: [C] (ì˜ˆ: 2) í˜•íƒœì˜ í† í° ID í…ì„œ (ë””ë°”ì´ìŠ¤ì— ì˜¬ë¼ì™€ ìˆì–´ì•¼ í•¨)
        #    - labelsì—ì„œ ' A' ë˜ëŠ” ' B'ê°€ ë“¤ì–´ê°„ ì²« ìœ„ì¹˜ë¥¼ ì°¾ê³ , ê·¸ ìœ„ì¹˜ì˜ ë¡œì§“ì—ì„œ ë‘ í´ë˜ìŠ¤ë§Œ ë¹„êµ
        try:
            choice_ids = self.choices  # shape [C], ì˜ˆ: tensor([id_A, id_B], device=...)
            # labelsê°€ choice ì¤‘ í•˜ë‚˜ì¸ ìœ„ì¹˜ ë§ˆìŠ¤í¬: [B, T]
            """
            labels[..., None]          # == labels.unsqueeze(-1) â†’ [B, T, 1]
            choice_ids[None, None, :]  # â†’ [1, 1, C]
            (labels[..., None] == choice_ids[None, None, :])# â†’ [B, T, C]
            """
            is_choice = (labels[..., None] == choice_ids[None, None, :]).any(dim=-1)# â†’ [B, T]
            has_choice = is_choice.any(dim=1)  # [B]

            if has_choice.any():
                B, T, V = logits.shape
                device = labels.device

                #ìƒ˜í”Œë³„ â€œì²« ë²ˆì§¸â€ choice ìœ„ì¹˜ ì°¾ê¸°
                """
                is_choice.int()ëŠ” True/False â†’ 1/0.
                argmax(dim=1)ëŠ” ê° ìƒ˜í”Œ(b)ì— ëŒ€í•´ ê°€ì¥ ë¨¼ì € 1ì´ ë‚˜ì˜¤ëŠ” të¥¼ ëŒë ¤ì¤Œ â†’ â€œì²« choice ìœ„ì¹˜â€.
                """
                first_idx = torch.argmax(is_choice.int(), dim=1)                  # [B]

                """
                has_choice==Falseì¸ ìƒ˜í”Œì€ ì œì™¸í•˜ê¸° ìœ„í•´, ì„ íƒ ë§ˆìŠ¤í¬ has_choiceë¡œ ì¸ë±ì‹±:
                    sel_b: ì‹¤ì œë¡œ choiceê°€ ìˆëŠ” ìƒ˜í”Œë“¤ì˜ ë°°ì¹˜ ì¸ë±ìŠ¤ë“¤ (Mê°œ)

                    sel_t: ê·¸ ìƒ˜í”Œ ê°ê°ì—ì„œì˜ â€œì²« choice í† í°â€ì˜ ìœ„ì¹˜ (ê¸¸ì´ M)
                """
                sel_b = torch.arange(B, device=device)[has_choice]                # [M]
                sel_t = first_idx[has_choice]                                     # [M]

                # ìˆ˜ì •
                sel_t_pred = sel_t - 1                          # ì˜ˆì¸¡ìš© ë¡œì§“ì˜ ì‹œê°„ì¶• ì¸ë±ìŠ¤
                valid = sel_t_pred >= 0                         # í˜¹ì‹œ 0ì¸ ì¼€ì´ìŠ¤ ë°©ì§€
                sel_b      = sel_b[valid]
                sel_t      = sel_t[valid]
                sel_t_pred = sel_t_pred[valid]


                # â¬‡ï¸ ë¹„ì–´ ìˆìœ¼ë©´ ì•ˆì „ ì¢…ë£Œ (argmaxê°€ ë¹ˆ í…ì„œì—ì„œ í„°ì§ˆ ìˆ˜ ìˆìŒ)
                if sel_b.numel() == 0:
                    ctx.sample_correct_batch = 0
                    ctx.sample_count_batch   = 0
                    return



                # Mê°œ ìƒ˜í”Œ ê°ê°ì— ëŒ€í•´ â€œì²« choice ìœ„ì¹˜ t=sel_tâ€ì—ì„œì˜ ì–´íœ˜ ì „ì²´(V) ë¡œì§“ì„ ë½‘ìŒ.
                logits_at   = logits[sel_b, sel_t_pred, :]   # [M, V]

                # í•´ë‹¹ ìœ„ì¹˜ì˜ ì •ë‹µ í† í° ID (id_A ë˜ëŠ” id_B)
                targets_tok = labels[sel_b, sel_t]                                # [M]


                # ì„ íƒì§€ ë‘(ì—¬ëŸ¬) í´ë˜ìŠ¤ ë¡œì§“ë§Œ ë½‘ê¸° â†’ [M, C]
                logits_choice = logits_at.index_select(dim=1, index=choice_ids)



                # (targets_tok == choice_ids) ë¹„êµë¡œ [M, C] bool ë§Œë“¤ê³ , argmaxë¡œ ìœ„ì¹˜(0 or 1)ë¥¼ ë½‘ìŒ.
                #   id_Aë©´ 0, id_Bë©´ 1ì´ ë¨.
                target_idx = (targets_tok.unsqueeze(1) == choice_ids.unsqueeze(0)).long().argmax(dim=1)  # [M]

                #ì˜ˆì¸¡ í´ë˜ìŠ¤
                pred_idx = torch.argmax(logits_choice, dim=-1)                    # [M]

                #ì •í™•ë„ ê³„ì‚°
                sample_correct = int((pred_idx == target_idx).sum().item()) #ì •ë‹µê³¼ ë¹„êµí•´ Mê°œ ì¤‘ ë§ì¶˜ ê°œìˆ˜
                sample_count   = int(target_idx.numel())
            else:
                sample_correct, sample_count = 0, 0

            ctx.sample_correct_batch = sample_correct
            ctx.sample_count_batch   = sample_count
        except Exception as e:
            # ë©”íŠ¸ë¦­ ì‹¤íŒ¨í•´ë„ í•™ìŠµì€ ê³„ì†
            logger.warning(f"[choice-metrics] skipped due to error: {e}")
            ctx.sample_correct_batch = 0
            ctx.sample_count_batch   = 0





    # grad accumulation loopëŠ” rankë³„ë¡œ localì—ì„œë§Œ ë™ì‘.

    # í•˜ì§€ë§Œ backwardë§ˆë‹¤ DDP hookì´ ìë™ìœ¼ë¡œ all-reduce ì‹¤í–‰.

    # ê²°êµ­ grad_accum_stepì€ â€œoptimizer.step() í˜¸ì¶œ ì‹œì ë§Œ ëŠ¦ì¶”ëŠ” ì¥ì¹˜â€ì„.

    def _hook_on_batch_backward(self, ctx): #accelerate, deepspeedì— ë”°ë¥¸ backward, step, zero_grad ì²˜ë¦¬ ë¶„ê¸°

        """
        Accelerator ë˜ëŠ” Deepspeed ì‚¬ìš© ì‹œ ì „ìš© backward/step ë¡œì§
            self.accelerator.backward()

            ctx.model_engine.backward() ë“±

        ê·¸ë ‡ì§€ ì•Šì€ ê²½ìš°, ì¼ë°˜ loss.backward()ì— grad_accum_step ê³ ë ¤í•˜ì—¬ ë‚˜ëˆ ì¤Œ

        """


        if bool(getattr(ctx, "skip_this_batch", False)): #ìŠ¤í‚µ í”Œë˜ê·¸(skip_this_batch)ê°€ ì¼œì ¸ ìˆìœ¼ë©´ ì•„ë¬´ ì‘ì—…ë„ í•˜ì§€ ì•Šê³  ë°”ë¡œ ë¦¬í„´
            return

        if ctx.cfg.llm.accelerator.use:
            # backwardë§ˆë‹¤ DDP hookì´ ìë™ìœ¼ë¡œ ê° processì˜ ê²°ê³¼ ë°”íƒ•ìœ¼ë¡œ all-reduce ì‹¤í–‰. Accelerateê°€ grad accumulationì„ ê´€ë¦¬í•˜ë¯€ë¡œ ì†ì‹¤ì„ ë‚˜ëˆ„ì§€ ì•ŠìŠµë‹ˆë‹¤.
            self.accelerator.backward(ctx.loss_task) 
            # âœ… ëˆ„ì  ê²½ê³„(sync_gradients=True)ì—ì„œë§Œ step/zero_grad ì‹¤í–‰
            #(i % grad_accum_step != 0)ì—ì„œëŠ” sync_gradients=False. ì¦‰, backwardëŠ” ì‹¤í–‰í•˜ì§€ë§Œ DDP all-reduce í†µì‹ ì€ ë°œìƒí•˜ì§€ ì•ŠìŒ â†’ ë¡œì»¬ì—ì„œë§Œ grad ëˆ„ì .
            # accumulation ë (i % grad_accum_step == 0)ì—ì„œëŠ” sync_gradients=True. ì´ë•Œë§Œ optimizer.step()ì´ ì‹¤í–‰ë˜ê³ , all-reduceê°€ ë°œìƒí•´ì„œ 4ê°œ í”„ë¡œì„¸ìŠ¤ì˜ grad í‰ê· ì´ ë§ì¶°ì§.


            if getattr(self.accelerator, "sync_gradients", True): 
                # (ë³´ì™„) ê²½ê³„ì—ì„œë§Œ gradient clipping ìˆ˜í–‰
                if getattr(ctx, "grad_clip", 0) and ctx.grad_clip > 0:
                    try:
                        self.accelerator.clip_grad_norm_(ctx.model.parameters(), ctx.grad_clip)
                    except Exception:
                        torch.nn.utils.clip_grad_norm_(ctx.model.parameters(), ctx.grad_clip)
                ctx.optimizer.step()
                if ctx.scheduler is not None:
                    ctx.scheduler.step()
                ctx.optimizer.zero_grad()

                # logger.info(f"[UPD] optimizer_step={self._global_updates+1} (sync_gradients=True), "
                #             f"num_train_batch_last_epoch={getattr(self.ctx,'num_train_batch_last_epoch',None)}")

                # â˜… ì—¬ê¸°ì„œ 'ë°”ë¡œ' í˜¸ì¶œ: pending/ë°”ê¹¥ì²´í¬ X
                self._global_updates += 1
                if self._ct_ft and self._mid_eval_every > 0 and (self._global_updates % self._mid_eval_every) == 0:
                    logger.info(f"[MID-EVAL-TRIGGER] (inline) at optimizer_step={self._global_updates}")
                    self._mid_eval_pending = True
        else:
            (ctx.loss_task / self.grad_accum_step).backward()

            if (ctx.cur_batch_i + 1) % self.grad_accum_step == 0: #ì¡°ê±´ë¶€ ì—…ë°ì´íŠ¸. cur_batch_i+1) % grad_accum_step == 0ì¼ ë•Œë§Œ
                if ctx.grad_clip > 0: #ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘. ->ctx.grad_clip > 0ì¼ ê²½ìš° clip_grad_norm_ ì ìš©
                    torch.nn.utils.clip_grad_norm_(ctx.model.parameters(),
                                                   ctx.grad_clip)
                ctx.optimizer.step()
                if ctx.scheduler is not None:
                    ctx.scheduler.step()
                ctx.optimizer.zero_grad()

                self._global_updates += 1
                if self._ct_ft and self._mid_eval_every > 0 and (self._global_updates % self._mid_eval_every) == 0:
                    self._mid_eval_pending = True


        # move the training data to cpu (ì•ˆì „í•˜ê²Œ í‚¤ í™•ì¸)
        db = getattr(ctx, "data_batch", None)
        if isinstance(db, dict):
            for k in ('input_ids', 'labels', 'attention_mask'):
                t = db.get(k, None)
                if torch.is_tensor(t):
                    db[k] = t.detach().to('cpu', non_blocking=True)


        # for k in ('input_ids', 'labels', 'attention_mask'):
        #     if k in ctx.data_batch:
        #         t = ctx.data_batch[k]
        #         ctx.data_batch[k] = t.detach().to('cpu', non_blocking=True)




    def _hook_on_batch_end(self, ctx): #ğŸ‘‰ í•œ ë°°ì¹˜(batch)ë¥¼ ëë‚¼ ë•Œ í˜¸ì¶œë˜ëŠ” í›„ì²˜ë¦¬ hook
        #1) ìŠ¤í‚µ ì²˜ë¦¬
        """
        ê¸°ë³¸ ë¡œì§ + loss == NaNì¼ ê²½ìš° batchë¥¼ retry

        if ctx.skip_this_batch:
            if ctx.cfg.llm.retry_on_nan_loss:
                self._run_batch(...)
        """
        if bool(getattr(ctx, "skip_this_batch", False)):
            if ctx.cfg.llm.retry_on_nan_loss:
                # Retry with new data in train and finetune
                if ctx.cur_mode == MODE.TRAIN:
                    self._run_batch(self.hooks_in_train, run_step=1)
                elif ctx.cur_mode == MODE.FINETUNE:
                    self._run_batch(self.hooks_in_ft, run_step=1)
            return

        #2) (ì˜µì…˜) ë°°ì¹˜ë³„ LR ë””ë²„ê¹…. ìŠ¤ì¼€ì¥´ëŸ¬ ì ìš© í™•ì¸ìš©.
        if ctx.cur_mode == MODE.TRAIN:
            opt = getattr(ctx, "optimizer", None)
            if opt is not None:
                current_lr = opt.param_groups[0]["lr"]
                round_idx = getattr(ctx, "current_round_num", "?")
                batch_idx = getattr(ctx, "batch_idx", "?")
                # logger.info(f"[LR BATCH CHECK] round={round_idx} batch={batch_idx} -> LR={current_lr:.2e}")

        
        #3) í†µê³„ ì§‘ê³„ì‹œ í‚¤ ì´ˆê¸°í™”
        # ëˆ„ì  ì§‘ê³„ í‚¤
        split = ctx.cur_split

        loss_total_key = f"loss_total_{split}"
        num_samples_key = f"num_samples_{split}"


        if not hasattr(ctx, loss_total_key):
            setattr(ctx, loss_total_key, 0.0)

        if not hasattr(ctx, num_samples_key):
            setattr(ctx, num_samples_key, 0)

        #4) ë°°ì¹˜ ë‹¨ìœ„ í†µê³„
        batch_raw_samples = int(ctx.batch_size) #ë°°ì¹˜ ë‚´ì˜ ì „ì²´ ìƒ˜í”Œ ìˆ˜
        batch_logic_seen = int(getattr(ctx, "sample_count_batch", 0)) #ë°°ì¹˜ ë‚´ì˜ ìœ íš¨í•œ ìƒ˜í”Œ ìˆ˜
        batch_logic_corr = int(getattr(ctx, "sample_correct_batch", 0)) #ë°°ì¹˜ ë‚´ì˜ ìœ íš¨í•œ ìƒ˜í”Œë“¤ ì¤‘ ë§ì¶˜ ê²ƒë“¤ì˜ ê°¯ìˆ˜

        #split ë™ì•ˆ ëˆ„ì  sample ìˆ˜ ë§Ÿ loss ì´í•©
        setattr(
            ctx,
            loss_total_key,
            getattr(ctx, loss_total_key) + ctx.loss_batch.item() * batch_raw_samples,
        )

        setattr(
            ctx,
            num_samples_key,
            getattr(ctx, num_samples_key) + batch_raw_samples,
        )


        #5) ì •í™•ë„ ëˆ„ì ,  ë…¼ë¦¬ì (ì„ íƒëœ) í† í° ê¸°ì¤€ ì •í™•ë„ ì§‘ê³„
        ctx.sample_seen = int(getattr(ctx, "sample_seen", 0)) + batch_logic_seen
        ctx.sample_correct_accum = int(getattr(ctx, "sample_correct_accum", 0)) + batch_logic_corr

        #6) Loss ì„¸ë¶€ ì§‘ê³„
        ctx.num_samples = int(getattr(ctx, "num_samples", 0)) + batch_raw_samples
        ctx.loss_batch_total = float(getattr(ctx, "loss_batch_total", 0.0)) + ctx.loss_batch.item() * batch_raw_samples
        ctx.loss_regular_total = float(getattr(ctx, "loss_regular_total", 0.0)) + float(ctx.get("loss_regular", 0.0))

        #7) ê²€ì¦/í…ŒìŠ¤íŠ¸: Prediction ì •ë³´ ì €ì¥
        if ctx.cur_mode in [MODE.TEST, MODE.VAL]:
            if not hasattr(ctx, "ys_true"):
                ctx.ys_true = []
            if not hasattr(ctx, "ys_pred"):
                ctx.ys_pred = []

            pred = torch.argmax(ctx.y_prob, dim=-1)
            ctx.ys_true.append(ctx.y_true)
            ctx.ys_pred.append(pred)

        # if ctx.cur_mode in (MODE.TRAIN, MODE.FINETUNE) and getattr(self, "_mid_eval_pending", False):
        #     logger.info(f"[MID-EVAL-TRIGGER] at optimizer_step={self._global_updates}")
        #     self._mid_eval_pending = False
        #     self._mid_eval_once()

    def _hook_on_fit_end(self, ctx): 

        """
        ë¼ìš´ë“œ ì¢…ë£Œ ì‹œì  ì •ë¦¬/ë¡œê·¸ í›….
        âœ… ì§‘ê³„(reduce)ëŠ” _run_routineì—ì„œ ì™„ë£Œë˜ì—ˆë‹¤ê³  ê°€ì •.
        - split(train/val/test)ë³„ ìµœì¢… metric(ctx.eval_metrics)ë§Œ ì½ì–´ ë¡œê·¸ ì¶œë ¥
        - Accelerator/ë©”ëª¨ë¦¬ ì •ë¦¬ ë“±ì€ ë‹¤ë¥¸ í›…ì— ë§¡ê¸°ê±°ë‚˜ ì—¬ê¸°ì„œ ê°€ë³ê²Œ ì²˜ë¦¬
        """        
        #1) split ìœ íš¨ì„± ì²´í¬
        split = getattr(ctx, "cur_split", None)
        if split not in ("train", "val", "test"):
            return

        # 2) accelerator ë° main ì—¬ë¶€
        acc = getattr(self, "accelerator", None)
        is_main = (acc.is_main_process if acc is not None else True)

        # 3) ğŸ”§ ì§‘ê³„ëŠ” í•˜ì§€ ì•Šê³ , _run_routineì—ì„œ ì±„ìš´ ìµœì¢… metricë§Œ ë¡œë“œ
        #    (train/evalì„ ê°™ì€ ë¼ìš´ë“œì—ì„œ ë‘˜ ë‹¤ ëŒë ¤ë„, splitë³„ í‚¤ë¡œ ê³µì¡´ ê°€ëŠ¥)
        m = getattr(ctx, "eval_metrics", None)
        if not isinstance(m, dict) or len(m) == 0:
            # âš ï¸ í˜¹ì‹œ ìƒìœ„ê°€ ì•„ì§ ì•ˆ ì±„ì› ê±°ë‚˜ ì‹±í¬ ë¬¸ì œë©´ ì¡°ìš©íˆ ë¦¬í„´
            return

        # 4) ë¡œê¹…ìš© ì•ˆì „ ì¶”ì¶œ (í‚¤ ì—†ì„ ìˆ˜ ìˆìœ¼ë‹ˆ get ì‚¬ìš©)
        total    = m.get(f"{split}_total", 0)
        loss_sum = m.get(f"{split}_loss", 0.0)
        avg_loss = m.get(f"{split}_avg_loss", 0.0)
        seen     = m.get(f"{split}_seen", 0)          # ğŸ†• seen/ correctë„ ë¡œê¹…
        correct  = m.get(f"{split}_correct", 0)
        accuracy  = m.get(f"{split}_acc", 0.0) 

        #7) metric ì €ì¥ ë° ë¡œê·¸ ì¶œë ¥ (ë©”ì¸ í”„ë¡œì„¸ìŠ¤ë§Œ)
        if is_main:
            logger.info(
                f"[{split}|final] total={total}, "
                f"loss_sum={float(loss_sum):.6f}, avg_loss={float(avg_loss):.6f}, "
                f"seen={int(seen)}, correct={int(correct)}, accuracy={float(accuracy):.6f}"
            )

    def _hook_on_fit_end_free_space(self, ctx):

        # A) ëª¨ë“  ë­í¬ê°€ í•™ìŠµ ë£¨í”„ë¥¼ ì™„ì „íˆ ë¹ ì ¸ë‚˜ì™”ìŒì„ ë³´ì¥
        barrier_all()

        # B) prepareëœ ê°ì²´ë¶€í„° ì œê±° (self.model ì›ë³¸ì€ ìœ ì§€)
        to_del = [
            'val_loader', 'test_loader',
            'val_loader_iter', 'test_loader_iter',
            'loss_batch','loss_task','loss_regular',
            'loss_batch_total', 'loss_regular_total', 
            'y_true', 'y_prob', 'ys_true', 'ys_pred',
            'data_batch', 'grad', 'model_engine', 'skip_this_batch',
            # ë°ì´í„°ë¡œë”ì™€ ì´í„°ë ˆì´í„°ë¥¼ ëª¨ë‘ ì‚­ì œí•˜ì—¬ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ë¥¼ ë°©ì§€

        ]

        deleted_attrs = []
        for k in to_del:
            if hasattr(ctx, k):
                delattr(ctx, k); deleted_attrs.append(k)
        if getattr(ctx, 'rank', 0) == 0:
            logger.info(f"[Memory Cleanup] Deleted ctx attrs: {deleted_attrs}")        

        
        try:
            tok = getattr(self, "tokenizer", None)
            mdl = getattr(self, "model", None) or getattr(self.ctx, "model", None)
            if tok is not None and mdl is not None:
                log_tok_model_sync(tok, self._unwrap(mdl), tag="before-accel-free_memory")
        except Exception:
            pass

        if getattr(self, "accelerator", None) is not None:
            import torch
            try:
                torch.cuda.synchronize()
                self.accelerator.free_memory()
                torch.cuda.synchronize()
            except Exception:
                pass
            try:
                if hasattr(torch.cuda, "ipc_collect"):
                    torch.cuda.ipc_collect()
            except Exception:
                pass
            logger.info("Accelerator memory has been freed (object preserved).")

 



        if hasattr(self, 'model'):
            for p in self.model.parameters():
                p.grad = None
        

        gc.collect()
        torch.cuda.empty_cache()

        # D) ì •ë¦¬ ì™„ë£Œ ë™ê¸°í™”
        barrier_all()

        try:
            if torch.cuda.is_available():
                rnd = getattr(self.ctx, "current_round_num",
                    getattr(self.ctx, "cur_round_i", "?"))
                res = torch.cuda.memory_reserved() // (1024**2)
                aloc = torch.cuda.memory_allocated() // (1024**2)
                logger.info(f"[VRAM] round={rnd} reserved={res}MB allocated={aloc}MB")
                torch.cuda.reset_peak_memory_stats()
        except Exception:
            pass




    def _hook_on_batch_forward_flop_count(self, ctx): #PASSí•´ë„ ë ë“¯

        """
        ëª¨ë¸ êµ¬ì¡°ì— ë”°ë¼ LLMì˜ input (input_ids, attention_mask)ì„ ì´ìš©í•´ FLOPsë¥¼ ê³„ì‚°.

        fvcore.nn.FlopCountAnalysisë¥¼ ì‚¬ìš©í•¨.
        """



        """
        The monitoring hook to calculate the flops during the fl course

        Note:
          For customized cases that the forward process is not only \
          based on ctx.model, please override this function (inheritance \
          case) or replace this hook (plug-in case)

          The modified attributes and according operations are shown below:
            ==================================  ===========================
            Attribute                           Operation
            ==================================  ===========================
            ``ctx.monitor``                     Track average flops
            ==================================  ===========================
        """

        # The process may occupy a large amount of video memory
        # if the garbage collection is not triggered in time
        # when there is plenty of video memory left. Set
        # `eval.count_flops = False` to avoid this.
        if not isinstance(ctx.monitor, Monitor):
            logger.warning(
                f"The trainer {type(self)} does contain a valid monitor, "
                f"this may be caused by initializing trainer subclasses "
                f"without passing a valid monitor instance."
                f"Please check whether this is you want.")
            return

        if self.cfg.eval.count_flops and ctx.monitor.flops_per_sample == 0:
            # calculate the flops_per_sample
            try:
                input_ids = ctx.data_batch['input_ids'].to(ctx.device)
                labels = ctx.data_batch['labels'].to(ctx.device)
                attention_mask = ctx.data_batch['attention_mask'].to(
                    ctx.device)
                from fvcore.nn import FlopCountAnalysis
                if isinstance(ctx.model, AdapterModel):
                    flops_one_batch = FlopCountAnalysis(
                        ctx.model.model,
                        inputs=(input_ids, attention_mask)).total()
                else:
                    flops_one_batch = FlopCountAnalysis(
                        ctx.model, inputs=(input_ids, attention_mask)).total()
                ctx.monitor.track_avg_flops(flops_one_batch, ctx.batch_size)
            except Exception as e:
                logger.warning("When using count flops functions, torch's "
                               "garbage collection mechanism may not be "
                               "timely resulting in OOM, please set "
                               "`cfg.eval.count_flops` to `False` "
                               "to avoid error or warning like this.")
                logger.error(e)
                # Raise warning at the first failure
                logger.warning(
                    "current flop count implementation is for general LLM "
                    "trainer case: "
                    "1) ctx.data_batch contains [input_ids, labels, "
                    "attn_mask]; and 2) the ctx.model takes first two "
                    "arguments should be and attention_mask. "
                    "If ctx.model is an adapter model, the model in 2) has "
                    "been replaced by ctx.model.model. "
                    "Please check the forward format or implement your own "
                    "flop_count function")
                ctx.monitor.flops_per_sample = -1

        # by default, we assume the data has the same input shape,
        # thus simply multiply the flops to avoid redundant forward
        ctx.monitor.total_flops += ctx.monitor.flops_per_sample * \
            ctx.batch_size
        
    def _save_best_local_only(self):
        """LoRA ì „ìš©(í•™ìŠµê°€ëŠ¥ íŒŒë¼ë¯¸í„°) ì²´í¬í¬ì¸íŠ¸: í´ë¼ì´ì–¸íŠ¸ë‹¹ 1ê°œë§Œ ìœ ì§€(ë®ì–´ì“°ê¸°)."""
        if not bool(getattr(self.cfg.eval, "local_only", False)):
            return

        acc = getattr(self, "accelerator", None)
        is_main = (acc.is_main_process if acc is not None else True)
        if not is_main:
            return

        cid  = int(getattr(getattr(self, "ctx", object()), "client_ID", 0))

        save_dir = "./checkpoints_1.0_local_only"
        os.makedirs(save_dir, exist_ok=True)

        # â˜… íŒŒì¼ëª… ê³ ì •(= step ì œê±°). ë®ì–´ì“°ê¸° 1ê°œë§Œ ìœ ì§€ë¨.
        save_path = os.path.join(save_dir, f"local_only_tldr_choice_qwen_client_{cid:03d}.ckpt")

        base = self._unwrap(self.ctx.model)
        sd = base.state_dict()        # AdapterModel.state_dict(return_trainable=True) â†’ LoRA í•™ìŠµê°€ëŠ¥ íŒŒë¼ë¯¸í„°ë§Œ
        torch.save(sd, save_path)
        logger.info(f"[local-only] saved(best) -> {save_path}")




    def _mid_eval_once(self):
        """CT-FTì—ì„œ every_n_train_stepsë§ˆë‹¤:
        (1) train ìŠ¤ëƒ…ìƒ· -> ì „ í´ë¼ì´ì–¸íŠ¸ ê³µìš© íŒŒì¼ì— append
        (2) train ì¹´ìš´í„° ì´ˆê¸°í™”
        (3) val/test í‰ê°€ -> í´ë¼ì´ì–¸íŠ¸ë³„ íŒŒì¼ì— append
        (4) val/test ì¹´ìš´í„° ì´ˆê¸°í™”
        """
 
        # --- ì¬ì§„ì… ê°€ë“œ ---
        if getattr(self, "_mid_eval_running", False):
            return
        self._mid_eval_running = True

        # --- í˜¸ì¶œ ì „ ìƒíƒœ ì €ì¥ ---
        prev_mode = getattr(self.ctx, "cur_mode", None)
        prev_split = getattr(self.ctx, "cur_split", None)   # â˜… ì¶”ê°€
        prev_training = None
        m = getattr(self.ctx, "model", None)
        if m is not None:
            try:
                prev_training = self._unwrap(m).training  # HF/Accelerate ë˜í•‘ ê³ ë ¤
            except Exception:
                prev_training = m.training

        try:
            # ====== â¬‡ ê¸°ì¡´ ë³¸ë¬¸ ê·¸ëŒ€ë¡œ ë‘  (ì•„ë˜ ì¤„ë¶€í„° ë„¤ ì½”ë“œ) ======
            using_accel = hasattr(self, 'accelerator') and self.accelerator is not None
            if using_accel:
                self.accelerator.wait_for_everyone()
            is_main = (not using_accel) or self.accelerator.is_main_process

            cid  = int(getattr(getattr(self, "ctx", object()), "client_ID", 0))
            rnd  = int(getattr(getattr(self, "ctx", object()), "round", 0))
            step = int(getattr(self, "_global_updates", 0))
            adapter_idx = getattr(getattr(self, "ctx", object()), "current_adapter_idx", None)

            outdir = getattr(self.cfg.eval, "outdir", "runs")
            os.makedirs(outdir, exist_ok=True)
            train_path = os.path.join(outdir, "train_results.raw")
            per_client_path = os.path.join(outdir, f"mid_eval/client_{cid:03d}.raw")
            os.makedirs(os.path.dirname(per_client_path), exist_ok=True)

            logger.info(f"[mid-eval] start: is_main={is_main}, step={step}, splits={['val','test']}")

            train_snap = self._train_snapshot_metrics()
            if not train_snap:
                train_snap = {
                    "train_total": 0, "train_loss": 0.0, "train_avg_loss": 0.0,
                    "train_seen": 0, "train_correct": 0, "train_acc": 0.0,
                }

            if is_main:
                rec = {
                    "client": cid, "round": rnd, "step": step,
                    "phase": "mid", "split": "train",
                    "adapter_idx": (int(adapter_idx) if adapter_idx is not None else None),
                    "total": train_snap.get("train_total", 0),
                    "loss": train_snap.get("train_loss", 0.0),
                    "avg_loss": train_snap.get("train_avg_loss", 0.0),
                    "seen": train_snap.get("train_seen", 0),
                    "correct": train_snap.get("train_correct", 0),
                    "acc": train_snap.get("train_acc", 0.0),
                }
                with open(train_path, "a", encoding="utf-8") as f:
                    f.write(json.dumps(rec, ensure_ascii=False) + "\n")

            self._reset_split_counters('train')
            if using_accel:
                self.accelerator.wait_for_everyone()

            for sp in ['val','test']:
                self._reset_split_counters(sp)
                if using_accel:
                    self.accelerator.wait_for_everyone()

                out = self.evaluate(target_data_split_name=sp)

                if is_main and isinstance(out, dict):
                    rec = {
                        "client": cid, "round": rnd, "step": step,
                        "phase": "mid", "split": sp,
                        "adapter_idx": (int(adapter_idx) if adapter_idx is not None else None),
                        "total":    out.get(f"{sp}_total", 0),
                        "loss":     out.get(f"{sp}_loss", 0.0),
                        "avg_loss": out.get(f"{sp}_avg_loss", 0.0),
                        "seen":     out.get(f"{sp}_seen", 0),
                        "correct":  out.get(f"{sp}_correct", 0),
                        "acc":      out.get(f"{sp}_acc", 0.0),
                    }
                    with open(per_client_path, "a", encoding="utf-8") as f:
                        f.write(json.dumps(rec, ensure_ascii=False) + "\n")

                # â¬‡ï¸ ESëŠ” testì¼ ë•Œ(ê·¸ë¦¬ê³  CT-FTì¼ ë•Œ)ë§Œ ìˆ˜í–‰
                if self._ct_ft and sp == 'test' and self._es_enabled and isinstance(out, dict):
                    # acc í‚¤ë¥¼ ê²¬ê³ í•˜ê²Œ ê°€ì ¸ì˜¤ê¸° (ìš°ì„ ìˆœìœ„: test_acc -> f'{sp}_acc' -> acc)
                    cur_acc = float(out['test_acc'])  # ì—¬ê¸° ê³ ì •!

                    if cur_acc > (self._es_best + self._es_min_delta):
                        self._es_best = cur_acc
                        self._es_wait = 0
                        if is_main:
                            logger.info(f"[EarlyStop] new best test_acc={cur_acc:.6f}")
                        self._save_best_local_only()
                    else:
                        self._es_wait += 1
                        if is_main:
                            logger.info(f"[EarlyStop] no improvement (wait={self._es_wait}/{self._es_patience}), "
                                        f"best={self._es_best:.6f}, curr={cur_acc:.6f}")
                        if self._es_wait >= self._es_patience:
                            self._es_triggered = True
                            if is_main:
                                logger.info("[EarlyStop] patience reached -> request stop")

                # ë§ˆì§€ë§‰ì— ì¹´ìš´í„° ë¦¬ì…‹
                self._reset_split_counters(sp)



            if using_accel:
                self.accelerator.wait_for_everyone()
            # ====== â¬† ê¸°ì¡´ ë³¸ë¬¸ ê·¸ëŒ€ë¡œ ë‘  ======

        finally:
            # --- ìƒíƒœ ë³µêµ¬ ---
            if prev_mode is not None:
                self.ctx.cur_mode = prev_mode
            if prev_split is not None:
                self.ctx.cur_split = prev_split            # â˜… ì¶”ê°€

            m = getattr(self.ctx, "model", None)
            if m is not None:
                if prev_training:
                    m.train()
                else:
                    m.eval()
            self._mid_eval_running = False

    def _reset_split_counters(self, split: str):
        """í•´ë‹¹ splitì˜ ëˆ„ì  ì¹´ìš´í„°ë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™”."""
        setattr(self.ctx, f'num_samples_{split}', 0)
        setattr(self.ctx, f'loss_total_{split}', 0.0)
        # ê³µìš© ì§‘ê³„(ì •í™•ë„ìš©)
        self.ctx.sample_seen = 0
        self.ctx.sample_correct_accum = 0
        # (ì˜µì…˜) ë¡œì»¬ ìŠ¤ëƒ…ìƒ· ìºì‹œ
        if hasattr(self.ctx, 'local_results_for_log'):
            self.ctx.local_results_for_log = {}

    def _train_snapshot_metrics(self):
        """í˜„ì¬ê¹Œì§€ì˜ train ëˆ„ì ê°’ì„ reduceí•´ì„œ ê·¸ëŒ€ë¡œ ë°˜í™˜(ìœˆë„ìš°/prev ì—†ìŒ)."""
        split = "train"
        num_samples = int(self.ctx.get(f'num_samples_{split}', 0))
        loss_total  = float(self.ctx.get(f'loss_total_{split}', 0.0))
        seen        = int(self.ctx.get('sample_seen', 0))
        correct     = int(self.ctx.get('sample_correct_accum', 0))

        using_accel = hasattr(self, 'accelerator') and self.accelerator is not None \
                    and getattr(self.accelerator, 'num_processes', 1) > 1

        if using_accel:
            import torch
            dev = self.accelerator.device
            total_all   = self.accelerator.reduce(torch.tensor([num_samples], device=dev, dtype=torch.long),    reduction='sum')[0].item()
            loss_all    = self.accelerator.reduce(torch.tensor([loss_total],  device=dev, dtype=torch.float32), reduction='sum')[0].item()
            seen_all    = self.accelerator.reduce(torch.tensor([seen],        device=dev, dtype=torch.long),    reduction='sum')[0].item()
            correct_all = self.accelerator.reduce(torch.tensor([correct],     device=dev, dtype=torch.long),    reduction='sum')[0].item()
        else:
            total_all, loss_all, seen_all, correct_all = num_samples, loss_total, seen, correct

        if total_all <= 0:
            return {}

        return {
            "train_total":   int(total_all),
            "train_loss":    float(loss_all),
            "train_avg_loss": float(loss_all / max(1, total_all)),
            "train_seen":    int(seen_all),
            "train_correct": int(correct_all),
            "train_acc":     float(correct_all / max(1, seen_all)),
        }



def call_llm_trainer(trainer_type):
    if trainer_type == 'llmtrainer':
        trainer_builder = LLMTrainer
        return trainer_builder


register_trainer('llmtrainer', call_llm_trainer)
