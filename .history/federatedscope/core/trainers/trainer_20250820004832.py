import collections
import copy
import logging
import os
import gc         # <--- í™•ì¸ ë˜ëŠ” ì¶”ê°€
import torch      # <--- í™•ì¸ ë˜ëŠ” ì¶”ê°€ (ì´ë¯¸ ìˆì„ ê°€ëŠ¥ì„± ë†’ìŒ)
import objgraph   # <--- í™•ì¸ ë˜ëŠ” ì¶”ê°€
import random # íŒŒì¼ ì´ë¦„ ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´ ì¶”ê°€

from federatedscope.core.trainers.base_trainer import BaseTrainer
from federatedscope.core.trainers.enums import MODE, LIFECYCLE
from federatedscope.core.auxiliaries.decorators import use_diff
from federatedscope.core.trainers.utils import format_log_hooks, filter_by_specified_keywords
from federatedscope.core.trainers.context import Context, CtxVar, lifecycle


from federatedscope.core.auxiliaries.dataloader_builder import get_dataloader
from federatedscope.core.data.wrap_dataset import WrapDataset
from federatedscope.core.auxiliaries.ReIterator import ReIterator

from accelerate.utils import gather_object

logger = logging.getLogger(__name__)


class Trainer(BaseTrainer):
    """
        Register, organize and run the train/test/val procedures
    """

    HOOK_TRIGGER = [
        "on_fit_start", "on_epoch_start", "on_batch_start", "on_batch_forward",
        "on_batch_backward", "on_batch_end", "on_epoch_end", "on_fit_end"
    ]

    def __init__(self,
                 model,
                 data,
                 device,
                 config,
                 only_for_eval=False,
                 monitor=None):
        self._cfg = config

        self.ctx = Context(model, self.cfg, data, device) #dataëŠ” ClientData í´ë˜ìŠ¤ í˜•íƒœ.

        # Parse data and setup init vars in ctx
        self._setup_data_related_var_in_ctx(self.ctx)

        assert monitor is not None, \
            f"Monitor not found in trainer with class {type(self)}"
        self.ctx.monitor = monitor
        # the "model_nums", and "models" are used for multi-model case and
        # model size calculation
        self.model_nums = 1
        self.ctx.models = [model]
        # "mirrored_models": whether the internal multi-models adopt the
        # same architects and almost the same behaviors,
        # which is used to simply the flops, model size calculation
        self.ctx.mirrored_models = False

        # Atomic operation during training/evaluation
        self.hooks_in_train = collections.defaultdict(list) #í‚¤(key)â†’ë¦¬ìŠ¤íŠ¸(value) ë§¤í•‘ì„ ê´€ë¦¬í•˜ëŠ” dict. ì¡´ì¬í•˜ì§€ ì•ŠëŠ” í‚¤ ì ‘ê·¼ ì‹œ ìë™ìœ¼ë¡œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“¤ì–´ ì£¼ëŠ” ì¥ì  ìˆìŒ.

        # By default, use the same trigger keys
        self.hooks_in_eval = copy.deepcopy(self.hooks_in_train)
        self.hooks_in_ft = copy.deepcopy(self.hooks_in_train)

        # register necessary hooks into self.hooks_in_train and
        # self.hooks_in_eval


        if not only_for_eval: #eval ë§Œ í• ê±°ë©´ train pass
            self.register_default_hooks_train()

        if self.cfg.finetune.before_eval: #eval ì´ì „ì— fine tunning í•  ê²ƒì¸ì§€ ì—¬ë¶€. False
            self.register_default_hooks_ft()

         # í‰ê°€ìš© í›…ë“¤(on_fit_start, on_batch_start, on_batch_end, on_fit_end ë“±)
        self.register_default_hooks_eval()



    @property
    def cfg(self):
        return self._cfg

    @cfg.setter
    def cfg(self, new_cfg):
        self._cfg = new_cfg
        self.ctx.cfg = new_cfg
        self._setup_data_related_var_in_ctx(self.ctx)

    def parse_data(self, data):
        """
        Populate ``${split}_data``, ``${split}_loader`` and \
        ``num_${split}_data`` for different data splits
        """
        raise NotImplementedError

    def setup_data(self, ctx):
        """
        Initialization data by ``cfg``.
        """
        pass

    def _setup_data_related_var_in_ctx(self, ctx):
        """
        Populate ``${split}_data``, ``${split}_loader`` and \
        ``num_${split}_data`` for different data splits, and setup init var \
        in ctx.
        """
        self.setup_data(ctx)
        init_dict = self.parse_data(ctx.data)
        ctx.merge_from_dict(init_dict) #ì´ ë–„ ctxëŠ” train_loader, num_train_data ê°™ì€ attributeê°€ ìƒê¹€.

    def register_default_hooks_train(self):
        pass

    def register_default_hooks_eval(self):
        pass

    def register_default_hooks_ft(self):
        pass

    def reset_hook_in_train(self, target_trigger, target_hook_name=None):
        hooks_dict = self.hooks_in_train
        del_one_hook_idx = self._reset_hook_in_trigger(hooks_dict,
                                                       target_hook_name,
                                                       target_trigger)
        return del_one_hook_idx

    def reset_hook_in_eval(self, target_trigger, target_hook_name=None):
        hooks_dict = self.hooks_in_eval
        del_one_hook_idx = self._reset_hook_in_trigger(hooks_dict,
                                                       target_hook_name,
                                                       target_trigger)
        return del_one_hook_idx

    def replace_hook_in_train(self, new_hook, target_trigger,
                              target_hook_name):
        del_one_hook_idx = self.reset_hook_in_train(
            target_trigger=target_trigger, target_hook_name=target_hook_name)
        self.register_hook_in_train(new_hook=new_hook,
                                    trigger=target_trigger,
                                    insert_pos=del_one_hook_idx)

    def replace_hook_in_eval(self, new_hook, target_trigger, target_hook_name):
        del_one_hook_idx = self.reset_hook_in_eval(
            target_trigger=target_trigger, target_hook_name=target_hook_name)
        self.register_hook_in_eval(new_hook=new_hook,
                                   trigger=target_trigger,
                                   insert_pos=del_one_hook_idx)
        
        

    def _reset_hook_in_trigger(self, hooks_dict, target_hook_name,
                               target_trigger):
        # clean/delete existing hooks for a specific trigger,
        # if target_hook_name given, will clean only the specific one;
        # otherwise, will clean all hooks for the trigger.

        #hooks_dict: self.hooks_in_train, self.hooks_in_eval, self.hooks_in_ft ì¤‘ í•˜ë‚˜.

        #target_trigger: "on_batch_end" ê°™ì€, í›… ë¦¬ìŠ¤íŠ¸ë¥¼ ì§€ìš¸ í‚¤

        #target_hook_name: ì‚­ì œí•  í›…ì˜ í•¨ìˆ˜ ì´ë¦„(funcB.__name__ == 'funcB')
        ####None ì´ë©´ â€œëª¨ë‘ ì‚­ì œâ€

        #3. ë°˜í™˜ê°’
        # -1: ì „ì²´ í›… ë¦¬ìŠ¤íŠ¸ë¥¼ ë¹„ì› ì„ ë•Œ

        # >=0: ì‚­ì œëœ í›…ì´ ì›ë˜ ì°¨ì§€í•˜ë˜ ì¸ë±ìŠ¤

        # None: (ì‚¬ì‹¤ ë…¼ë¦¬ìƒ ì˜ ë‚˜ì˜¤ì§„ ì•Šì§€ë§Œ) í›… ì´ë¦„ì„ ì§€ì •í–ˆëŠ”ë° ì°¾ì§€ ëª»í–ˆì„ ë•Œ



        """
        HOOK_TRIGGER = [
            "on_fit_start", "on_epoch_start", "on_batch_start", "on_batch_forward",
            "on_batch_backward", "on_batch_end", "on_epoch_end", "on_fit_end"
        ]
        """

        assert target_trigger in self.HOOK_TRIGGER, \
            f"Got {target_trigger} as hook trigger, you should specify a " \
            f"string within {self.HOOK_TRIGGER}."
        del_one_hook_idx = None
        if target_hook_name is None:
            hooks_dict[target_trigger] = []
            del_one_hook_idx = -1  # -1 indicates del the whole list
        else:
            for hook_idx in range(len(hooks_dict[target_trigger])):
                if target_hook_name == hooks_dict[target_trigger][
                        hook_idx].__name__:
                    del_one = hooks_dict[target_trigger].pop(hook_idx)
                    logger.info(f"Remove the hook `{del_one.__name__}` from "
                                f"hooks_set at trigger `{target_trigger}`")
                    del_one_hook_idx = hook_idx
                    break
            if del_one_hook_idx is None:
                logger.warning(
                    f"In hook del procedure, can't find the target hook "
                    f"named {target_hook_name}")
        return del_one_hook_idx

    def register_hook_in_train(self,
                               new_hook, #new_hook: ì¶”ê°€í•  í•¨ìˆ˜
                               trigger,#trigger: ì´ë²¤íŠ¸ ì´ë¦„ (ì˜ˆ: "on_fit_start")
                               insert_pos=None,
                               base_hook=None,
                               insert_mode="before"):
        hooks_dict = self.hooks_in_train
        self._register_hook(base_hook, hooks_dict, insert_mode, insert_pos,
                            new_hook, trigger)

    def register_hook_in_ft(self,
                            new_hook,
                            trigger,
                            insert_pos=None,
                            base_hook=None,
                            insert_mode="before"):
        hooks_dict = self.hooks_in_ft
        self._register_hook(base_hook, hooks_dict, insert_mode, insert_pos,
                            new_hook, trigger)

    def register_hook_in_eval(self,
                              new_hook, #new_hook: ì¶”ê°€í•  í•¨ìˆ˜
                              trigger, #trigger: ì´ë²¤íŠ¸ ì´ë¦„ (ì˜ˆ: "on_fit_start")
                              insert_pos=None,
                              base_hook=None,
                              insert_mode="before"):


        hooks_dict = self.hooks_in_eval #hooks_dict: ì‹¤ì œ í›… ì €ì¥ì†Œ (defaultdict(list))
        self._register_hook(base_hook, hooks_dict, insert_mode, insert_pos,
                            new_hook, trigger)#hook_dict, new_hook, triggerë§Œ ì‹¤ì œë¡œ ë³´ë©´ ë¨.

    def _register_hook(self, base_hook, hooks_dict, insert_mode, insert_pos,
                       new_hook, trigger): #hooks_dict, new_hook, trigger ë§Œ ì“°ì„.
        
        # 1) trigger ìœ íš¨ì„± ê²€ì‚¬
        assert trigger in self.HOOK_TRIGGER, \
            f"Got {trigger} as hook trigger, you should specify a string " \
            f"within {self.HOOK_TRIGGER}."
        # parse the insertion position
        # 2) ëŒ€ìƒ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ìë™ ìƒì„±)
        target_hook_set = hooks_dict[trigger]


        # 3) insert_pos ê³„ì‚°
        if insert_pos is not None: #FALSE
            assert (insert_pos == -1) or (insert_pos == len(target_hook_set)
                                          == 0) or \
                   (0 <= insert_pos <= (len(target_hook_set))), \
                   f"Got {insert_pos} as insert pos, you should specify a " \
                   f"integer (1) =-1 " \
                   f"or (2) =0 for null target_hook_set;" \
                   f"or (3) within [0, {(len(target_hook_set))}]."
        elif base_hook is not None: #FALSE
            base_hook_pos = target_hook_set.index(base_hook)
            insert_pos = base_hook_pos - 1 if insert_mode == "before" else \
                base_hook_pos + 1
            # bounding the insert_pos in rational range
            insert_pos = 0 if insert_pos < 0 else insert_pos
            insert_pos = -1 if insert_pos > len(
                target_hook_set) else insert_pos
        else:# ì•„ë¬´ ì˜µì…˜ ì—†ìœ¼ë©´ ë’¤ì— ë¶™ì´ê¸°
            insert_pos = -1  # By default, the new hook is called finally
        
        
        # register the new hook
        # 4) ì‹¤ì œ ë“±ë¡: append vs insert
        if insert_pos == -1: #ì¼ë°˜ì ìœ¼ë¡œ ì´ê±° ì ìš©.
            hooks_dict[trigger].append(new_hook)
        else: #insert_posì— register!!
            hooks_dict[trigger].insert(insert_pos, new_hook)



    @use_diff
    def train(self, target_data_split_name="train", hooks_set=None):
        hooks_set = hooks_set or self.hooks_in_train
        self.ctx.check_split(target_data_split_name)
        num_samples = self._run_routine(MODE.TRAIN, hooks_set,
                                        target_data_split_name)
        return num_samples, self.get_model_para(), self.ctx.eval_metrics
    

    """
    
    ğŸ”¹ Trigger â†’ Hook í•¨ìˆ˜ë“¤
    on_fit_start

    hook_on_fit_start_init

    hook_on_fit_start_flop_count

    hook_on_fit_start_add_regularizer

    hook_on_fit_start_load_model

    hook_on_fit_start_join_in_info

    on_epoch_start

    hook_on_epoch_start_init

    hook_on_epoch_start_flop_count

    on_batch_start

    hook_on_batch_start_init

    hook_on_batch_start_flop_count

    hook_on_batch_start_move_data_to_device

    on_batch_forward

    hook_on_batch_forward_init

    hook_on_batch_forward_flop_count

    hook_on_batch_forward_regularizer

    hook_on_batch_forward_loss

    hook_on_batch_forward_split_data

    hook_on_batch_forward_flop_count_eval

    on_batch_backward

    hook_on_batch_backward_init

    hook_on_batch_backward_regularizer

    hook_on_batch_backward_flop_count

    hook_on_batch_backward_loss

    on_batch_end

    hook_on_batch_end_init

    hook_on_batch_end_flop_count

    hook_on_batch_end_regularizer

    hook_on_batch_end_optimizer_step

    hook_on_batch_end_update_lr

    on_epoch_end

    hook_on_epoch_end_eval

    hook_on_epoch_end_early_stop

    hook_on_epoch_end_flop_count

    hook_on_epoch_end_save_model

    hook_on_epoch_end_regularizer

    on_fit_end

    hook_on_fit_end_save_model

    hook_on_fit_end_eval

    hook_on_fit_end_regularizer

    hook_on_fit_end_summary

    hook_on_fit_end_export_model





    """










    def evaluate(self, target_data_split_name="test", hooks_set=None):
        hooks_set = hooks_set or self.hooks_in_eval
        if self.ctx.check_split(target_data_split_name, skip=True):
            self._run_routine(MODE.TEST, hooks_set, target_data_split_name)
        else:
            self.ctx.eval_metrics = dict()
        return self.ctx.eval_metrics





    def finetune(self, target_data_split_name="train", hooks_set=None):
        hooks_set = hooks_set or self.hooks_in_ft

        self.ctx.check_split(target_data_split_name)

        self._run_routine(MODE.FINETUNE, hooks_set, target_data_split_name)

  

    """
    _run_routine=lifecycle(LIFECYCLE.ROUTINE)(_run_routine)ìœ¼ë¡œ ë°”ê¿ˆ. decoratorì˜ ì—­í• .
    ì—¬ê¸°ì„œ lifecycle(LIFECYCLE.ROUTINE)ëŠ” decorateë¥¼ ë°˜í™˜. 
    decorate(_run_routine)ì„ ì‹¤í–‰í•˜ê²Œ ë˜ê³  wrapperë¥¼ ìµœì¢… ë°˜í™˜.
    ê·¸ë¦¬ê³  wrapper(mode, hooks_set, dataset_name)ë¥¼ ì‹¤í–‰í•´ì„œ ìµœì¢… ë°˜í™˜í•˜ê²Œë˜ëŠ” í•©ì„±í•¨ìˆ˜ êµ¬ì¡°.
    """

    @lifecycle(LIFECYCLE.ROUTINE)
    def _run_routine(self, mode, hooks_set, dataset_name=None):
        # 0) ëª¨ë“œ/ìŠ¤í”Œë¦¿ íŠ¸ë˜í‚¹ ë° ì´ˆê¸°í™”
        self.ctx.track_mode(mode)
        self.ctx.track_split(dataset_name or mode)
        self.ctx.eval_metrics = {}

        # 1) on_fit_start í›…
        for hook in hooks_set["on_fit_start"]:
            hook(self.ctx)

        # 2) epoch/batch ë£¨í”„
        self._run_epoch(hooks_set)

        # 3) ë¡œì»¬ ê²°ê³¼ ê³„ì‚° (ì§‘ê³„ ì „ì— ë¨¼ì € ê³„ì‚°)
        split = self.ctx.cur_split
        local_results = {}
        num_samples = self.ctx.get(f'num_samples_{split}', 0)
        loss_total  = self.ctx.get(f'loss_total_{split}', 0.0)
        correct     = self.ctx.get(f'correct_{split}', 0)

        if num_samples > 0:
            local_results = {
                f'{split}_total':    int(num_samples),
                f'{split}_loss':     float(loss_total),
                f'{split}_avg_loss': float(loss_total / max(1, num_samples)),
                f'{split}_acc':      float(correct / max(1, num_samples)),
            }

        # âœ… ì¶”ê°€: ë¡œì»¬(ì§‘ê³„ ì „) ë¡œê·¸ìš© ë³´ê´€
        self.ctx.local_results_for_log = dict(local_results)  # â† ì§‘ê³„ ì „ ìŠ¤ëƒ…ìƒ·

        # âœ… ì¶”ê°€: ê° rankì˜ ë¡œì»¬ dictì— rank ë²ˆí˜¸ í¬í•¨(ëª¨ìœ¼ê¸°ìš©)
        per_rank_entry = dict(local_results)
        if hasattr(self, 'accelerator') and self.accelerator is not None:
            per_rank_entry['rank'] = self.accelerator.process_index
        else:
            per_rank_entry['rank'] = 0



        # 4) ë¶„ì‚° ì§‘ê³„(reduce)ëŠ” í•­ìƒ ìˆ˜í–‰
        using_accelerate = (
            hasattr(self, 'accelerator')
            and self.accelerator is not None
            and getattr(self.accelerator, 'num_processes', 1) > 1
        )

        # ì„œë²„ë¡œ ë°˜í™˜í•  "ìƒ˜í”Œ ìˆ˜"ë„ ì „ì—­ í•©ìœ¼ë¡œ ë§ì¶¤
        ret_num_samples = num_samples

        if using_accelerate:
            # --- reduce ì§ì „ ë°°ë¦¬ì–´ ---
            self.accelerator.wait_for_everyone()

            # (ë””ë²„ê·¸) ë¡œì»¬ í•©ê³„ ì¶œë ¥
            logger.info(
                f"[agg debug] using_accel={using_accelerate}, "
                f"world={getattr(self.accelerator,'num_processes',1)}, "
                f"rank={getattr(self.accelerator,'process_index',-1)}, "
                f"local_total={num_samples}"
            )

            import torch
            from accelerate.utils import gather_object  

            total_t   = torch.tensor([num_samples], device=self.accelerator.device, dtype=torch.long)
            loss_t    = torch.tensor([loss_total],  device=self.accelerator.device, dtype=torch.float32)
            correct_t = torch.tensor([correct],     device=self.accelerator.device, dtype=torch.long)

            # ëª¨ë“  ë­í¬ì—ì„œ í•©ê³„
            total_all   = self.accelerator.reduce(total_t,   reduction='sum')[0].item()
            loss_all    = self.accelerator.reduce(loss_t,    reduction='sum')[0].item()
            correct_all = self.accelerator.reduce(correct_t, reduction='sum')[0].item()

            if total_all > 0:
                final_results = {
                    f'{split}_total':    int(total_all),
                    f'{split}_loss':     float(loss_all),
                    f'{split}_avg_loss': float(loss_all / total_all),
                    f'{split}_acc':      float(correct_all / total_all),
                }

                self.ctx.eval_metrics = final_results
                ret_num_samples = total_all
            else:
                # ì „ ë­í¬ê°€ 0 ìƒ˜í”Œì¸ ì§„ì§œ ë¹ˆ ì¼€ì´ìŠ¤
                self.ctx.eval_metrics = {}
                ret_num_samples = 0


            # âœ… (rank0ì—ì„œë§Œ) ëª¨ë“  í”„ë¡œì„¸ìŠ¤ì˜ ë¡œì»¬ ê²°ê³¼ ìˆ˜ì§‘
            try:
                gathered = gather_object(per_rank_entry)  # ëª¨ë“  í”„ë¡œì„¸ìŠ¤ì—ì„œ í˜¸ì¶œ
                if self.accelerator.is_main_process:
                    # ê° í•­ëª©ì€ {'rank': i, '<split>_total': ..., ...}
                    self.ctx.per_proc_local_results = gathered
            except Exception as e:
                logger.warning(f"[agg debug] gather_object failed: {e}")                

            # --- reduce ì§í›„ ë°°ë¦¬ì–´: on_fit_end ì „ì— ëª¨ë“  ë­í¬ ë™ê¸°í™” ---
            self.accelerator.wait_for_everyone()

        else:
            # ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ ë˜ëŠ” accelerator ë¯¸ì‚¬ìš©
            self.ctx.eval_metrics = local_results
            ret_num_samples = num_samples

        # 5) on_fit_end í›… (ì—¬ê¸°ì„œ Accelerator ì‚­ì œ/ë©”ëª¨ë¦¬ ì •ë¦¬ í›… í˜¸ì¶œ)
        for hook in hooks_set["on_fit_end"]:
            hook(self.ctx)

        # 6) ë°˜í™˜ê°’: ì „ì—­ í•©ê³„(ë©€í‹°í”„ë¡œì„¸ìŠ¤ë©´ 480/40/146 ë“±, ë‹¨ì¼ì´ë©´ ë¡œì»¬ ê°’)
        return ret_num_samples    

    @lifecycle(LIFECYCLE.EPOCH) #í•œ ì—í­(epoch) ë‹¨ìœ„ë¡œ ë°˜ë³µ ì‹¤í–‰. ëë‚˜ë©´ ì—í­ìš© ì„ì‹œë³€ìˆ˜(CtxVar(..., "epoch")) ì¼ê´„ ì‚­ì œ
    def _run_epoch(self, hooks_set, run_step=-1):


        if run_step == -1:
            run_step = getattr(self.ctx, f"num_{self.ctx.cur_split}_epoch")#ì´ epoch ìˆ˜. batchë“  epoch ëª¨ë“œì´ë“  total data ë£¨í”„ ëª‡ ë²ˆ ë„ëŠ”ì§€ ê³„ì‚°ë¨. test/valì¼ë•ŒëŠ” 1.
        for epoch_i in range(run_step):
            self.ctx.cur_epoch_i = CtxVar(epoch_i, "epoch")

            for hook in hooks_set["on_epoch_start"]:
                hook(self.ctx)

            self._run_batch(hooks_set)#llm trainerë¡œ override. run_step=-1ë¡œ ëœë‹¤ëŠ” ê²ƒ ìœ ì˜. ë§ˆì§€ë§‰ epochì˜ ë¶€ì¡±í•œ batch updateë„ ê³ ë ¤.

            for hook in hooks_set["on_epoch_end"]:
                hook(self.ctx)

    @lifecycle(LIFECYCLE.BATCH) #í•œ ë°°ì¹˜(batch) ë‹¨ìœ„ë¡œ ë°˜ë³µ ì‹¤í–‰. ëë‚˜ë©´ ë°°ì¹˜ìš© ì„ì‹œë³€ìˆ˜(CtxVar(..., "batch")) ì¼ê´„ ì‚­ì œ.
    def _run_batch(self, hooks_set, run_step=-1):
        if run_step == -1:
            run_step = getattr(self.ctx, f"num_{self.ctx.cur_split}_batch") #ì¼ë°˜ì ì¸ epochë‹¹ batch ê°œìˆ˜
        for batch_i in range(run_step):
            self.ctx.cur_batch_i = CtxVar(batch_i, LIFECYCLE.BATCH)

            for hook in hooks_set["on_batch_start"]:
                hook(self.ctx)

            for hook in hooks_set["on_batch_forward"]:
                hook(self.ctx)

            for hook in hooks_set["on_batch_backward"]:
                hook(self.ctx)

            for hook in hooks_set["on_batch_end"]:
                hook(self.ctx)

            # Break in the final epoch. ë§ˆì§€ë§‰ epochë–„ëŠ” self.ctx.num_train_batch_last_epoch ë²ˆë§Œ ëŒë„ë¡!!
            if self.ctx.cur_mode in [
                    MODE.TRAIN, MODE.FINETUNE
            ] and self.ctx.cur_epoch_i == self.ctx.num_train_epoch - 1:
                if batch_i >= self.ctx.num_train_batch_last_epoch - 1:
                    break

    def update(self, model_parameters, strict=False):
        """
            Called by the FL client to update the model parameters
        Arguments:
            model_parameters (dict): {model_name: model_val}
            strict (bool): ensure the k-v paris are strictly same
        """
        pass

    def get_model_para(self):
        """

        :return: model_parameters (dict): {model_name: model_val}
        """
        pass

    def print_trainer_meta_info(self):
        """
            print some meta info for code-users, e.g., model type; the para
            names will be filtered out, etc.,
        """
        logger.info(f"Model meta-info: {type(self.ctx.model)}.")
        logger.debug(f"Model meta-info: {self.ctx.model}.")
        # logger.info(f"Data meta-info: {self.ctx['data']}.")

        ori_para_names = set(self.ctx.model.state_dict().keys()) #loraë§Œ í•´ë‹¹. 336

        #ê°œì¸í™” í•„í„°ë§ í›„ ë³´ì¡´ë  íŒŒë¼ë¯¸í„°
        preserved_paras = self._param_filter(self.ctx.model.state_dict()) #í•„í„°ë§ ë˜ì§€ ì•Šì•„ ê·¸ëŒ€ë¡œ self.ctx.model.state_dict(). ì¦‰ í™œì„±í™”ëœ adapterì˜ paramë§Œ ë‚˜ì˜´.
        preserved_para_names = set(preserved_paras.keys())

        #í•„í„°ë§ëœ(ì œì™¸ëœ) íŒŒë¼ë¯¸í„° ê³„ì‚°
        filtered_para_names = ori_para_names - preserved_para_names


        #íŒŒë¼ë¯¸í„° í†µê³„ ë¡œê·¸
        logger.info(f"Num of original para names: {len(ori_para_names)}.") #336
        logger.info(f"Num of original trainable para names:"
                    f" {len(self.ctx['trainable_para_names'])}.")#626. ê¸°ë³¸ base+ëª¨ë“  adapter parameter ë°˜í™˜.
        logger.info(
            f"Num of preserved para names in local update:"
            f" {len(preserved_para_names)}. \n"
            f"Preserved para names in local update: {preserved_para_names}.") #336
        logger.info(
            f"Num of filtered para names in local update:"
            f" {len(filtered_para_names)}. \n"
            f"Filtered para names in local update: {filtered_para_names}.")#0
        

        #í›…(hook) ì„¤ì • ë¡œê·¸

        logger.info(f"After register default hooks,\n"
                    f"\tthe hooks_in_train is:\n\t"
                    f"{format_log_hooks(self.hooks_in_train)};\n"
                    f"\tthe hooks_in_eval is:\n\
            t{format_log_hooks(self.hooks_in_eval)}")

    def _param_filter(self, state_dict, filter_keywords=None): #ì‹¤ì§ì ìœ¼ë¡œ ì•„ë¬´ê²ƒë„ í•„í„°ë§ ë˜ì§€ ì•ŠëŠ” state_dict ë¦¬í„´. ë”°ë¼ì„œ ë³¼ í•„ìš” ì—†ìŒ. ì„œë²„ë¡œ ë³´ë‚¼ íŒŒë¼ë¯¸í„°ë¥¼ í•„í„°ë§í•˜ê¸° ìœ„í•¨.ì¦‰, ê°œì¸í™” FL(Personalized FL)ì—ì„œ ë¡œì»¬ì—ë§Œ ë‚¨ê¸¸ íŒŒë¼ë¯¸í„°ë¥¼ ë¹¼ê³ , ì „ì†¡ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ë§Œ ê³¨ë¼ë‚´ëŠ” ì—­í• . 

        #state_dict: (íŒŒë¼ë¯¸í„°ì´ë¦„, Tensor) ìŒì˜ dict

        #filter_keywords: íŒŒë¼ë¯¸í„° ì´ë¦„ì— í¬í•¨ë˜ë©´ ì œì™¸ì‹œí‚¬ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (ex. "bn", "norm")

        #trainable_filter: í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ë§Œ ë³´ë‚¼ì§€ ì—¬ë¶€


        """
        model parameter filter when transmit between local and gloabl,
        which is useful in personalization.
        e.g., setting cfg.personalization.local_param= ['bn', 'norms']
        indicates the implementation of
        "FedBN: Federated Learning on Non-IID Features via Local Batch
        Normalization, ICML2021", which can be found in
        https://openreview.net/forum?id=6YEQUn0QICG

        Arguments:
            state_dict (dict): PyTorch Module object's state_dict.
        Returns:
            state_dict (dict): remove the keys that match any of the given
            keywords.
        """
        # 1) ê¸°ë³¸ ëª¨ë“œ(local/global)ì¼ ë• ì•„ë¬´ ê²ƒë„ ê³µìœ í•˜ì§€ ì•ŠìŒ.
        #### ê°œì¸í™”ê°€ ì•„ë‹Œ í‘œì¤€ ì—°í•©í•™ìŠµ ëª¨ë“œì´ë¯€ë¡œ ì•„ì˜ˆ ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•´ â€œë¡œì»¬ íŒŒë¼ë¯¸í„°ë¥¼ ì „í˜€ ë³´ë‚´ì§€ ì•Šê² ë‹¤â€ê³  ì„¤ì •
        if self.cfg.federate.method in ["local", "global"]: #ì¼ë°˜ì ìœ¼ë¡  FedAvg
            return {}
        

        # 2) í•„í„° í‚¤ì›Œë“œ ëª©ë¡ì´ ì£¼ì–´ì§€ì§€ ì•Šì•˜ë‹¤ë©´ cfg.personalization.local_param ì‚¬ìš©
        #### ì˜ˆë¥¼ ë“¤ì–´ cfg.personalization.local_param = ["bn","norm"] ì²˜ëŸ¼ ì„¤ì •í•´ ë‘ë©´ "bn"ì´ë‚˜ "norm"ì´ ì´ë¦„ì— ë“¤ì–´ê°„ íŒŒë¼ë¯¸í„°ëŠ” ë¡œì»¬ì—ë§Œ ë‚¨ê¸°ê² ë‹¤ëŠ” ì˜ë¯¸.
        if filter_keywords is None: #True
            filter_keywords = self.cfg.personalization.local_param #[]

        # 3) â€œê³µìœ  ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°â€ì¸ì§€ ê²€ì‚¬í•˜ëŠ” í•„í„° í•¨ìˆ˜. trainable_para_namesë§Œ í•„í„°ë§ í•œë‹¤.

        #trainable_filter ëŠ” ë‹¹ì—°íˆ í•¨ìˆ˜. ì„ì˜ì˜ input pì— ëŒ€í•´ ì–´ë–¤ output(Boolean)ì„ ë‚´ë†“ì„ì§€ë¥¼ ê²°ì •í•˜ëŠ” ê²ƒ.
        #share_non_trainable_para = True ì¸ ê²½ìš° ì–´ë–¤ input pê°€ ë“¤ì–´ê°€ë„ ë¬´ì¡°ê±´ True ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        #share_non_trainable_para = False ì¸ ê²½ìš° p in self.ctx.trainable_para_namesë¥¼ ë°˜ì˜í•œ Boolean ë°˜í™˜!!




        #hare_non_trainable_para = Falseë¡œ êµ¬í˜„ ì¤‘->ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ í†µê³¼ì‹œí‚¤ëŠ” lambda p: True ì„¤ì •

        trainable_filter = lambda p: True if \
            self.cfg.personalization.share_non_trainable_para else \
            lambda p: p in self.ctx.trainable_para_names 
        


        # 4) â€œí‚¤ì›Œë“œ í•„í„°â€ í•¨ìˆ˜: ì´ë¦„ì— íŠ¹ì • í‚¤ì›Œë“œê°€ í¬í•¨ë˜ëŠ”ì§€ ê²€ì‚¬
        keyword_filter = filter_by_specified_keywords


        # 5) ìµœì¢… ì „ì†¡ íŒŒë¼ë¯¸í„°ë§Œ ê³¨ë¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜. state_dictì˜ (elem[0]: name, elem[1]: tensor) ìŒë“¤ ì¤‘ì—ì„œ trainable_filter(tensor)ì™€ keyword_filter(name, filter_keywords)ê°€ ë‘˜ ë‹¤ Trueì¸ í•­ëª©ë§Œ ë‚¨ê²¨
        return dict(
            filter(
                lambda elem: trainable_filter(elem[1]) and keyword_filter(
                    elem[0], filter_keywords), state_dict.items()) #filter_keywords=[]-> keyword_filter(elem[0], filter_keywords) ì–¸ì œë‚˜ True. trainable_filter: ì–¸ì œë‚˜ true ë°˜í™˜í•˜ì—¬ trainable_filter(elem[1])ë„ ì–¸ì œë‚˜ True.
                    )  #ì‹¤ì§ˆì ìœ¼ë¡œ dict(filter(lambda elem: True, state_dict.items())). filter(lambda elem: True, state_dict.items())ëŠ” state_dict.items() ì¤‘ Trueì¸ ê²ƒë§Œ filtering í•˜ëŠ” ê²ƒ. ->ëª¨ë“  (name, tensor)ê°€ í†µê³¼
        #dict(...)ë¡œ ê°ì‹¸ë‹ˆ state_dict.items()ê³¼ ë™ì¼í•œ í•­ëª©ë“¤ì„ ë‹´ì€ ìƒˆ ë”•ì…”ë„ˆë¦¬ê°€ ìƒê¹€. ì¦‰ ì•„ë¬´ê²ƒë„ í•„í„°ë§ ë˜ì§€ ì•Šê³  ë‚´ìš©ë§Œ ê·¸ëŒ€ë¡œ ë³µì œë¨.

    def save_model(self, path, cur_round=-1):
        raise NotImplementedError(
            "The function `save_model` should be implemented according to "
            "the ML backend (Pytorch, Tensorflow ...).")

    def load_model(self, path):
        raise NotImplementedError(
            "The function `load_model` should be implemented according to "
            "the ML backend (Pytorch, Tensorflow ...).")
