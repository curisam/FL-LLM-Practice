# federatedscope/core/monitors/monitor.py
import copy
import json
import logging
import os
import gzip
import shutil
import datetime
from collections import defaultdict
from importlib import import_module
import time

import numpy as np

from federatedscope.core.auxiliaries.logging import logline_2_wandb_dict
from federatedscope.core.monitors.metric_calculator import MetricCalculator

try:
    import torch
except ImportError:
    torch = None

import torch.distributed as dist

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

global_all_monitors = []  # used in standalone mode, to merge sys metric results for all workers


# ============================================================================
# [ADD] main/rank0 íŒë‹¨ + outdir ì •ê·œí™” + íŒŒì¼ë¡œê±° ì„¤ì¹˜ ìœ í‹¸
# ============================================================================

_FILE_LOGGER_INSTALLED = False  # ì¤‘ë³µ FileHandler ë°©ì§€


def _is_main_process():
    # torch.distributed â†’ ENV
    try:
        import torch.distributed as dist
        if dist.is_available() and dist.is_initialized():
            return int(dist.get_rank()) == 0
    except Exception:
        pass
    try:
        return int(os.environ.get("RANK", os.environ.get("LOCAL_RANK", "0") or 0)) == 0
    except Exception:
        return True


def _normalize_outdir(path: str) -> str:
    """sub_exp/.. í•˜ìœ„ë¡œ ë‚´ë ¤ê°€ë©´ sub_exp ì•ê¹Œì§€ë§Œ ë‚¨ê²¨ ë‹¨ì¼ í´ë”ë§Œ ì‚¬ìš©"""
    if not path:
        return path
    norm = os.path.normpath(path)
    parts = norm.split(os.sep)
    if "sub_exp" in parts:
        idx = parts.index("sub_exp")
        return os.sep.join(parts[:idx])
    return norm


def _install_rank0_file_logger_once(outdir: str):
    """rank0ì—ì„œë§Œ exp_print.log íŒŒì¼í•¸ë“¤ëŸ¬ë¥¼ 1íšŒ ì„¤ì¹˜"""
    global _FILE_LOGGER_INSTALLED
    if _FILE_LOGGER_INSTALLED:
        return
    if not _is_main_process():
        return
    if not outdir:
        return
    
    #ë£¨íŠ¸ ë¡œê±°ë¥¼ ê°€ì ¸ì™€ì„œ INFO ë ˆë²¨ ì´ìƒë§Œ ì¶œë ¥í•˜ë„ë¡ ì„¤ì •

    os.makedirs(outdir, exist_ok=True) #ë¡œê·¸ íŒŒì¼ì„ ì €ì¥í•  ë””ë ‰í† ë¦¬ë¥¼ ìƒì„± (ì´ë¯¸ ìˆìœ¼ë©´ ë¬´ì‹œ)
    root = logging.getLogger()      # ë£¨íŠ¸ ë¡œê±°ì— ë‹¬ì•„ì„œ ì „ì²´ ë¡œê·¸ ìˆ˜ì§‘. ë°©ì†¡êµ­ ì—­í• 
    root.setLevel(logging.INFO)

    # ì´ë¯¸ ì„¤ì¹˜ëœ íŒŒì¼ í•¸ë“¤ëŸ¬ê°€ ìˆëŠ”ì§€ ê²€ì‚¬->í•¸ë“¤ëŸ¬ê°€ ì—¬ëŸ¬ ê°œ ë¶™ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ì¤‘ë³µ ì„¤ì¹˜ ë°©ì§€ìš© ê²€ì‚¬
    for h in list(root.handlers):#root.handlers: ë£¨íŠ¸ ë¡œê±°ì— ë¶™ì€ í•¸ë“¤ëŸ¬ ëª©ë¡
        if isinstance(h, logging.FileHandler):
            try:
                if os.path.basename(getattr(h, 'baseFilename', '')) == "exp_print.log": #ë¡œê·¸ íŒŒì¼ ì´ë¦„ì´ ì •í™•íˆ "exp_print.log"ì¸ì§€ í™•ì¸
                    _FILE_LOGGER_INSTALLED = True
                    return #ì´ë¯¸ ì„¤ì¹˜ë˜ì–´ ìˆìœ¼ë‹ˆ í•¨ìˆ˜ ì¢…ë£Œ
            except Exception:
                pass
    #ìƒˆ íŒŒì¼ í•¸ë“¤ëŸ¬(FileHandler) ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ë…¹ìŒê¸° ì—­í• . í„°ë¯¸ë„ë„ íŒŒì¼í•¸ë“¤ëŸ¬ì™€ ìœ ì‚¬í•œ stream handler. ë‹¤ë§Œ ì´ê²ƒì€ defaultë¡œ ì„¤ì •ë¨.
    fh = logging.FileHandler(os.path.join(outdir, "exp_print.log"),
                             mode="a", encoding="utf-8", delay=True)#ë¡œê·¸ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•˜ê² ë‹¤ëŠ” ëœ». ì €ì¥í•  íŒŒì¼ ì´ë¦„:"exp_print.log". mode="a":ê¸°ì¡´ íŒŒì¼ì— ë§ë¶™ì´ê¸°. encoding="utf-8": í•œê¸€ í¬í•¨. delay=True: ì‹¤ì œ ë¡œê·¸ê°€ ì²˜ìŒ ì°í ë•Œ íŒŒì¼ì„ ì—´ê² ë‹¤ëŠ” ìµœì í™”.
    
    #ë¡œê·¸ ë©”ì‹œì§€ì˜ ì¶œë ¥ í˜•ì‹ì„ ì§€ì •í•©ë‹ˆë‹¤:
    """
    í‚¤ì›Œë“œ	ì„¤ëª…
    %(asctime)s	ë¡œê·¸ ì‹œê°„
    %(name)s	ë¡œê±° ì´ë¦„
    %(lineno)d	ë¡œê·¸ê°€ ë°œìƒí•œ ì†ŒìŠ¤ ì½”ë“œ ë¼ì¸ ë²ˆí˜¸
    %(levelname)s	ë¡œê·¸ ë ˆë²¨(INFO, WARNING, ...)
    %(message)s	ì‹¤ì œ ë¡œê·¸ ë‚´ìš©
    
    ì˜ˆì‹œ:
    2025-08-07 20:51:11 (monitor.py:300) INFO: í•™ìŠµì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.       
    """
    fmt = logging.Formatter("%(asctime)s (%(name)s:%(lineno)d) %(levelname)s: %(message)s")

    """
    ë°©ê¸ˆ ë§Œë“  ë¡œê·¸ í¬ë§·ì„ í•¸ë“¤ëŸ¬ì— ì§€ì •í•©ë‹ˆë‹¤.

    í•¸ë“¤ëŸ¬ë„ INFO ë ˆë²¨ ì´ìƒë§Œ ê¸°ë¡í•˜ê²Œ ì„¤ì •í•©ë‹ˆë‹¤.  
    """

    fh.setFormatter(fmt)
    fh.setLevel(logging.INFO)


    """
    ì´ íŒŒì¼ í•¸ë“¤ëŸ¬ë¥¼ ë£¨íŠ¸ ë¡œê±°ì— ë“±ë¡í•©ë‹ˆë‹¤.

    ì´í›„ ëª¨ë“  logging.info(...) ë“±ì˜ ë¡œê·¸ëŠ” exp_print.logë¡œ ì €ì¥ë©ë‹ˆë‹¤.

    """
    root.addHandler(fh)#ë°©ì†¡êµ­ì— ë…¹ìŒê¸° ì—°ê²°
    _FILE_LOGGER_INSTALLED = True


class Monitor(object):
    """
    Provide the monitoring functionalities such as formatting the \
    evaluation results into diverse metrics. \
    Besides the prediction related performance, the monitor also can \
    track efficiency related metrics for a worker

    Args:
        cfg: a cfg node object
        monitored_object: object to be monitored

    Attributes:
        log_res_best: best ever seen results
        outdir: output directory
        use_wandb: whether use ``wandb``
        wandb_online_track: whether use ``wandb`` to track online
        monitored_object: object to be monitored
        metric_calculator: metric calculator, /
            see ``core.monitors.metric_calculator``
        round_wise_update_key: key to decide which result of evaluation \
            round is better
    """
    SUPPORTED_FORMS = ['weighted_avg', 'avg', 'fairness', 'raw']

    def __init__(self, cfg, monitored_object=None):
        self.cfg = cfg
        self.log_res_best = {}

        self.outdir = cfg.outdir

        self.use_wandb = cfg.wandb.use
        self.wandb_online_track = cfg.wandb.online_track if cfg.wandb.use \
            else False
        # self.use_tensorboard = cfg.use_tensorboard

        self.monitored_object = monitored_object
        self.metric_calculator = MetricCalculator(cfg.eval.metrics) 
        
        #self.metric_calculator.eval_metric-> loss, acc, avg_loss, totalë¥¼ keyë¡œ ê°–ëŠ” dict -> #{'acc': (<function eval_acc at 0x7f2edf2eb310>, True), 'avg_loss': (<function eval_avg_loss at 0x7f2edf2eb790>, False), 'loss': (<function eval_loss at 0x7f2edf2eb700>, False), 'total': (<function eval_total at 0x7f2edf2eb820>, False)}

        # Obtain the whether the larger the better
        self.round_wise_update_key = cfg.eval.best_res_update_round_wise_key  #test_loss
        update_key = None
        for mode in ['train', 'val', 'test']:
            if mode in self.round_wise_update_key: #test, test_lossë§Œ ëŒ€ì‘ ë¨.
                update_key = self.round_wise_update_key.split(f'{mode}_')[1] #'test_loss'ë¥¼ 'test_'ë¡œ ë‚˜ëˆ´ë”ë‹ˆ: ì•ì—ëŠ” ì•„ë¬´ ê²ƒë„ ì—†ì–´ì„œ '', ë’¤ì—ëŠ” 'loss' -> loss
        if update_key is None:
            # ì•ˆì „ì¥ì¹˜: metricsì— ìˆëŠ” ì„ì˜ì˜ ì²« í‚¤ë¥¼ ì‚¬ìš©
            # (ì‹¤ì „ì—ì„œëŠ” cfg ì„¤ì •ì„ ê¶Œì¥)
            update_key = list(self.metric_calculator.eval_metric.keys())[0]

        assert update_key in self.metric_calculator.eval_metric, \
            f'{update_key} not found in metrics.' #update_keyëŠ” lossì¸ ìƒí™©
        self.the_larger_the_better = self.metric_calculator.eval_metric[update_key][1] #Falseë¡œ ë‚˜ì˜´. lossëŠ” í¬ë©´ ì•ˆì¢‹ì€ê±°ê¸°ë•Œë¬¸ì—

        # =======  efficiency indicators of the worker to be monitored =======
        self.total_model_size = 0 # model size used in the worker, in terms, ëª¨ë¸ì˜ ì´ íŒŒë¼ë¯¸í„° ìˆ˜ (ì •ìˆ˜ê°’)
       
        self.flops_per_sample = 0 # average flops for forwarding each data, ìƒ˜í”Œ 1ê°œë¥¼ ì²˜ë¦¬í•  ë•Œì˜ í‰ê·  FLOPs
        self.flop_count = 0 # used to calculated the running mean for, ëˆ„ì  FLOPs ì¸¡ì •ì„ ìœ„í•œ ì¹´ìš´íŠ¸
        self.total_flops = 0 # total computation flops to convergence until, ì§€ê¸ˆê¹Œì§€ ì´ ì—°ì‚° FLOPs
        
        self.total_upload_bytes = 0 # total upload space cost in bytes, ì§€ê¸ˆê¹Œì§€ ì—…ë¡œë“œí•œ ë°ì´í„° ì´ëŸ‰ (bytes ë‹¨ìœ„), í´ë¼ì´ì–¸íŠ¸ â†’ ì„œë²„ë¡œ ì „ì†¡í•œ ëª¨ë¸ ì—…ë°ì´íŠ¸ì˜ í¬ê¸°
        self.total_download_bytes = 0 # total download space cost in bytes, ì§€ê¸ˆê¹Œì§€ ë‹¤ìš´ë¡œë“œí•œ ë°ì´í„° ì´ëŸ‰ (bytes ë‹¨ìœ„), ì„œë²„ â†’ í´ë¼ì´ì–¸íŠ¸ë¡œ ì „ì†¡í•œ ëª¨ë¸ì˜ í¬ê¸°


        self.fl_begin_wall_time = datetime.datetime.now() #í•™ìŠµ ì‹œì‘ ì‹œê°„
        
        self.fl_end_wall_time = 0 #í•™ìŠµ ì¢…ë£Œ ì‹œê°„
        
        self.global_convergence_round = 0 # total fl rounds to convergence, ì „ì²´ í•™ìŠµ ë¼ìš´ë“œ ì¤‘ ê¸€ë¡œë²Œ ìˆ˜ë ´ ì‹œì 
        self.global_convergence_wall_time = 0 #ìœ„ ì‹œì ê¹Œì§€ ê²½ê³¼ ì‹œê°„ (wall time)
        
        self.local_convergence_round = 0 # total fl rounds to convergence, í´ë¼ì´ì–¸íŠ¸ ë¡œì»¬ ìˆ˜ë ´ ì‹œì 
        self.local_convergence_wall_time = 0 #ìœ„ ì‹œì ê¹Œì§€ ê²½ê³¼ ì‹œê°„

        if self.wandb_online_track: #False
            global_all_monitors.append(self)
        if self.use_wandb: #False
            try:
                import wandb  # noqa: F401
            except ImportError:
                logger.error("cfg.wandb.use=True but not install the wandb package")
                exit()

        # [ì¶”ê°€] rank0ë§Œ exp_print.log íŒŒì¼í•¸ë“¤ëŸ¬ ì„¤ì¹˜ (1íšŒ)
        _install_rank0_file_logger_once(self.outdir)

    def eval(self, ctx): #eval_metricì˜ keyë“¤ì— ìˆëŠ” ê²ƒì— ëŒ€í•´ ì¸¡ì •í•œ ê°’ë“¤ ë°”íƒ•ìœ¼ë¡œ dictionary return. #ì£¼ìš” í˜¸ì¶œìœ„ì¹˜: eval ì‹œì 
        """
        Evaluates the given context with ``metric_calculator``.
        """
        results = self.metric_calculator.eval(ctx)
        return results

    def global_converged(self): #ğŸ”¹ ëª©ì : í•™ìŠµ ë„ì¤‘ ê¸€ë¡œë²Œ ìˆ˜ë ´(Global convergence)ì´ ì´ë£¨ì–´ì§„ ì‹œì ì„ ê¸°ë¡ #ì£¼ìš” í˜¸ì¶œìœ„ì¹˜: early stopì‹œ
        """Calculate wall time and round when global convergence has been reached."""
        self.global_convergence_wall_time = datetime.datetime.now() - self.fl_begin_wall_time #FL ê³¼ì • ì „ì²´ê°€ ì‹œì‘ëœ ì‹œì (self.fl_begin_wall_time)ìœ¼ë¡œë¶€í„° ê¸€ë¡œë²Œ ì¡°ê¸° ì¢…ë£Œê°€ ê²°ì •ë˜ê¸°ê¹Œì§€ ê²½ê³¼í•œ ì‹¤ì œ ë²½ì‹œê³„ ì‹œê°„(wall-clock time)ì„ ê³„ì‚°í•´ì„œ ì €ì¥
        #ë§ˆì§€ë§‰ GFL ë¼ìš´ë“œ ì €ì¥.
        self.global_convergence_round = self.monitored_object.state

    def local_converged(self): # ğŸ”¹ ëª©ì : í´ë¼ì´ì–¸íŠ¸ ë¡œì»¬ ìˆ˜ë ´ ì‹œì  ê¸°ë¡ (ì„œë²„ì™€ëŠ” ë³„ê°œ) #ì£¼ìš” í˜¸ì¶œìœ„ì¹˜: í´ë¼ì´ì–¸íŠ¸ ì¢…ë£Œì‹œ
        """Calculate wall time and round when local convergence has been reached."""
        self.local_convergence_wall_time = datetime.datetime.now() - self.fl_begin_wall_time #FL ê³¼ì • ì „ì²´ê°€ ì‹œì‘ëœ ì‹œì (self.fl_begin_wall_time)ìœ¼ë¡œë¶€í„° ë¡œì»¬ ì¡°ê¸° ì¢…ë£Œê°€ ê²°ì •ë˜ê¸°ê¹Œì§€ ê²½ê³¼í•œ ì‹¤ì œ ë²½ì‹œê³„ ì‹œê°„(wall-clock time)ì„ ê³„ì‚°í•´ì„œ ì €ì¥
        #í´ë¼ì´ì–¸íŠ¸ê°€ ë§ˆì§€ë§‰ìœ¼ë¡œ ìˆ˜í–‰í•œ FL ë¼ìš´ë“œ ë²ˆí˜¸(self.monitored_object.state)ë¥¼ ê¸°ë¡. monitored_object ëŠ” ì´ ê²½ìš° BaseClient í˜¹ì€ LLMMultiLoRAClient ì¸ìŠ¤í„´ìŠ¤ë¡œ, ê·¸ .state ê°€ â€œí˜„ì¬ í•™ìŠµì´ ì§„í–‰ëœ ë¼ìš´ë“œ ìˆ˜â€ë¥¼ ì˜ë¯¸. ëª‡ ë²ˆì§¸ ë¼ìš´ë“œê¹Œì§€ ë¡œì»¬ í•™ìŠµì´ ì§„í–‰ë˜ì—ˆì„ ë•Œ ë©ˆì·„ëŠ”ì§€
        self.local_convergence_round = self.monitored_object.state

    def finish_fl(self): #ğŸ”¹ ëª©ì : í•™ìŠµ ì¢…ë£Œ ì‹œ, ì‹œìŠ¤í…œ ì§€í‘œë“¤ì„ system_metrics.log íŒŒì¼ì— ê¸°ë¡ (ë‹¨, rank 0ì—ì„œë§Œ ì‹¤í–‰) #ì£¼ìš” í˜¸ì¶œìœ„ì¹˜: ì¢…ë£Œ ì§ì „
        """
        When FL finished, write system metrics to file.
        """

        #í•™ìŠµ ì¢…ë£Œ ì‹œê°„, ì´ ì—…ë¡œë“œ/ë‹¤ìš´ë¡œë“œ ë°”ì´íŠ¸, ëª¨ë¸ í¬ê¸° ë“± ê¸°ë¡
        
        self.fl_end_wall_time = datetime.datetime.now() - self.fl_begin_wall_time
        if not _is_main_process():  # âœ… rank0ë§Œ íŒŒì¼ ê¸°ë¡
            return

        system_metrics = self.get_sys_metrics()
        sys_metric_f_name = os.path.join(self.outdir, "system_metrics.log")
        os.makedirs(self.outdir, exist_ok=True)
        with open(sys_metric_f_name, "a") as f:
            f.write(json.dumps(system_metrics) + "\n")#í•œ ì¤„ JSONìœ¼ë¡œ ê¸°ë¡ë˜ë©°, ì´í›„ merge_system_metrics_simulation_mode()ì—ì„œ ì´ íŒŒì¼ì„ ì¤„ ë‹¨ìœ„ë¡œ ì½ì–´ì™€ í‰ê·  ê³„ì‚°ì— í™œìš©í•œë‹¤.

    def get_sys_metrics(self, verbose=True): #ğŸ”¹ ëª©ì : í˜„ì¬ê¹Œì§€ ëª¨ì•„ì§„ ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ë“¤ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜. ğŸ“¦ ì‚¬ìš©ì²˜: finish_fl(), merge_system_metrics_simulation_mode()
        system_metrics = {
            "id": self.monitored_object.ID,                                           # Monitor.__init__() í˜¸ì¶œ ì‹œ ê³„ì‚°ë¨
            "fl_end_time_minutes": self.fl_end_wall_time.total_seconds() / 60         # Monitor.finish_fl() í˜¸ì¶œ ì‹œ ê³„ì‚°ë¨
            if isinstance(self.fl_end_wall_time, datetime.timedelta) else 0,
            "total_model_size": self.total_model_size, #track_model_size() í˜¸ì¶œ ì‹œ.
            "total_flops": self.total_flops, #í•™ìŠµ ì¤‘ track_avg_flops() ë“± í†µí•´ ëˆ„ì  ê³„ì‚°
            "total_upload_bytes": self.total_upload_bytes, # CommManager.send() â†’ monitor.track_upload_bytes() í˜¸ì¶œ ì‹œ
            "total_download_bytes": self.total_download_bytes, # ì„œë²„ê°€ í´ë¼ì´ì–¸íŠ¸ì—ê²Œ ëª¨ë¸ì„ ë‚´ë ¤ì¤„ ë•Œ ì¶”ì 
            "global_convergence_round": self.global_convergence_round, #global_converged() í˜¸ì¶œ ì‹œ ê¸°ë¡
            "local_convergence_round": self.local_convergence_round, # local_converged() í˜¸ì¶œ ì‹œ ê¸°ë¡
            "global_convergence_time_minutes": self.global_convergence_wall_time.total_seconds() / 60 # global_converged() í˜¸ì¶œ ì‹œ ê³„ì‚°
            if isinstance(self.global_convergence_wall_time, datetime.timedelta) else 0,
            "local_convergence_time_minutes": self.local_convergence_wall_time.total_seconds() / 60 # local_converged() í˜¸ì¶œ ì‹œ ê³„ì‚°
            if isinstance(self.local_convergence_wall_time, datetime.timedelta) else 0,
        }
        if verbose:
            logger.info(
                f"In worker #{self.monitored_object.ID}, the system-related "
                f"metrics are: {str(system_metrics)}")
        return system_metrics

    def merge_system_metrics_simulation_mode(self,
                                             file_io=True,
                                             from_global_monitors=False): # Standalone ëª¨ë“œì—ì„œ ì—¬ëŸ¬ í´ë¼ì´ì–¸íŠ¸ì˜ ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ì„ ëª¨ì•„ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ë¥¼ ê³„ì‚°í•¨. ê²°ê³¼ë¥¼ system_metrics.logì— í‰ê· /í‘œì¤€í¸ì°¨ í–‰ ì¶”ê°€
        """
        Average the system metrics recorded in ``system_metrics.json`` by all workers
        """

        """
        Standalone ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œì—ì„œ ê° í´ë¼ì´ì–¸íŠ¸ê°€ í•™ìŠµ ì¢…ë£Œ ì‹œ ê¸°ë¡í•œ system_metrics.logë¥¼ ì½ì–´ì„œ

        ëª¨ë“  í´ë¼ì´ì–¸íŠ¸ ê°’ì˜ **í‰ê· (sys_avg)**ê³¼ **í‘œì¤€í¸ì°¨(sys_std)**ë¥¼ ê³„ì‚°

        ë‹¤ì‹œ ë¡œê·¸ íŒŒì¼ì— append        
        
        """


        all_sys_metrics = defaultdict(list)#valueë“¤ì€ list. ëª¨ë“  clientì˜ ê°’ì„ listë¡œ ì €ì¥.
        avg_sys_metrics = defaultdict() #ì¼ë°˜ dictì™€ ë‹¤ë¥´ê²Œ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” í‚¤ ì ‘ê·¼ ì‹œ ìë™ìœ¼ë¡œ ê¸°ë³¸ê°’ ìƒì„± í›„ ë°˜í™˜.
        std_sys_metrics = defaultdict()

        if file_io: #íŒŒì¼ì—ì„œ ì½ê¸° ëª¨ë“œ
            if not _is_main_process():   # âœ… rank0ë§Œ ë³‘í•©/ì“°ê¸°. ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ê°€ ë™ì‹œì— ê°™ì€ íŒŒì¼ ì“°ëŠ” ê±¸ ë°©ì§€.
                return
            sys_metric_f_name = os.path.join(self.outdir, "system_metrics.log")
            if not os.path.exists(sys_metric_f_name): #ë¡œê·¸ íŒŒì¼ ì—†ìœ¼ë©´ ê²½ê³  í›„ ì¢…ë£Œ
                logger.warning(
                    "You have not tracked the workers' system metrics in "
                    "$outdir$/system_metrics.log, we will skip the merging. "
                    "Plz check whether you do not want to call monitor.finish_fl()")
                return
            with open(sys_metric_f_name, "r") as f: #ê° ë¼ì¸ ì½ì–´ì„œ ëˆ„ì 
                for line in f: #"system_metrics.log"ë¥¼ ë¼ì¸ë³„ë¡œ ì½ìŒ.
                    """
                    fì˜ ì˜ˆì‹œ.
                    {"id": 1, "total_upload_bytes": 1234, "total_flops": 111}
                    {"id": 2, "total_upload_bytes": 2345, "total_flops": 222}
                    {"id": 3, "total_upload_bytes": 3456, "total_flops": 333}                  
                    """
                    res = json.loads(line)# {"id": 1, "total_upload_bytes": 1234, "total_flops": 111}

                    for k, v in res.items():
                        all_sys_metrics[k].append(v)

                """
                all_sys_metrics = {
                    "id": [1, 2, 3],
                    "total_upload_bytes": [1234, 2345, 3456],
                    "total_flops": [111, 222, 333]
                }
                """
            
            #ì¤‘ë³µ id ì²´í¬
            id_to_be_merged = all_sys_metrics["id"]
            if len(id_to_be_merged) != len(set(id_to_be_merged)):
                logger.warning(
                    f"The sys_metric_file ({sys_metric_f_name}) contains "
                    f"duplicated tracked sys-results with these ids: f{id_to_be_merged} "
                    f"We will skip the merging as the merge is invalid. "
                    f"Plz check whether you specify the 'outdir' "
                    f"as the same as the one of another older experiment.")
                return
        elif from_global_monitors: #ë©”ëª¨ë¦¬ì—ì„œ ì½ê¸°. íŒŒì¼ ëŒ€ì‹  ë©”ëª¨ë¦¬ì— ìˆëŠ”  global_all_monitors ë¦¬ìŠ¤íŠ¸ì—ì„œ ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ì„ ìˆ˜ì§‘.
            for monitor in global_all_monitors:
                res = monitor.get_sys_metrics(verbose=False)

                for k, v in res.items():
                    all_sys_metrics[k].append(v)
        else:
            raise ValueError("file_io or from_monitors should be True: "
                             f"but got file_io={file_io}, from_monitors={from_global_monitors}")

        for k, v in all_sys_metrics.items():
            if k == "id": #"id" í‚¤ëŠ” ìˆ«ìê°€ ì•„ë‹ˆë¼ ì›Œì»¤ ì‹ë³„ìì´ë¯€ë¡œ â†’ "sys_avg" / "sys_std" ë¼ë²¨ë¡œ ë³€ê²½
                avg_sys_metrics[k] = "sys_avg"
                std_sys_metrics[k] = "sys_std"
            else:
                v = np.array(v).astype("float")
                mean_res = np.mean(v)
                std_res = np.std(v)
                if "flops" in k or "bytes" in k or "size" in k: #"flops", "bytes", "size" í¬í•¨ ì‹œ ì‚¬ëŒì´ ì½ê¸° ì¢‹ì€ ë‹¨ìœ„ë¡œ ë³€í™˜ (ì˜ˆ: 1048576 â†’ "1.0M")
                    mean_res = self.convert_size(mean_res)
                    std_res = self.convert_size(std_res)
                avg_sys_metrics[f"sys_avg/{k}"] = mean_res
                std_sys_metrics[f"sys_std/{k}"] = std_res

        logger.info(f"After merging the system metrics from all works, we got avg: {avg_sys_metrics}")
        logger.info(f"After merging the system metrics from all works, we got std: {std_sys_metrics}")
        if file_io:
            with open(sys_metric_f_name, "a") as f: #í‰ê· /í‘œì¤€í¸ì°¨ ê²°ê³¼ë¥¼ ê°™ì€ ë¡œê·¸ íŒŒì¼ì— ë’¤ì— ë‘ ì¤„ë¡œ ì¶”ê°€
                f.write(json.dumps(avg_sys_metrics) + "\n")
                f.write(json.dumps(std_sys_metrics) + "\n")

        if self.use_wandb and self.wandb_online_track: #False
            try:
                import wandb
                for k, v in avg_sys_metrics.items():
                    wandb.summary[k] = v
                for k, v in std_sys_metrics.items():
                    wandb.summary[k] = v
            except ImportError:
                logger.error("cfg.wandb.use=True but not install the wandb package")
                exit()

    def save_formatted_results(self,
                               formatted_res,
                               save_file_name="eval_results.log"): #ğŸ”¹ ëª©ì : ê° ë¼ìš´ë“œì—ì„œ í¬ë§·íŒ…ëœ í‰ê°€ ê²°ê³¼ë¥¼ ë¡œê·¸ íŒŒì¼ì— ì €ì¥. ë‹¨, rank 0ì—ì„œë§Œ ìˆ˜í–‰ë¨. ğŸ“¦ ì €ì¥ ìœ„ì¹˜: eval_results.log
        """
        Save formatted results to a file.
        """
        line = str(formatted_res) + "\n" #formatted_res (ì˜ˆ: {'Role': 'Client #1', 'Round': 3, 'Results': {...}})ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•´ì„œ í•œ ì¤„ í˜•íƒœë¡œ ë§Œë“¦.
        if save_file_name != "":
            if _is_main_process():        # âœ… rank0ë§Œ
                os.makedirs(self.outdir, exist_ok=True)
                with open(os.path.join(self.outdir, save_file_name), "a") as outfile: #eval_results.log (ê¸°ë³¸) íŒŒì¼ì„ append ëª¨ë“œë¡œ ì—´ì–´ì„œ í•œ ì¤„ ì¶”ê°€
                    outfile.write(line)
        if self.use_wandb and self.wandb_online_track: #FALSE.
            try:
                import wandb
                exp_stop_normal = False
                exp_stop_normal, log_res = logline_2_wandb_dict(
                    exp_stop_normal, line, self.log_res_best, raw_out=False)
                wandb.log(log_res)
            except ImportError:
                logger.error("cfg.wandb.use=True but not install the wandb package")
                exit()

    def finish_fed_runner(self, fl_mode=None): #ğŸ”¹ ëª©ì : ì „ì²´ ì‹¤í—˜ ì¢…ë£Œ ì‹œ í›„ì²˜ë¦¬ ìˆ˜í–‰. ğŸ“¦ ë™ì‘: eval_results.raw ì••ì¶•.  standaloneì¸ ê²½ìš° system metric merge. 
        """
        Finish the Fed runner.
        """
        self.compress_raw_res_file() #eval_results.raw ì••ì¶•
        if fl_mode == "standalone":
            self.merge_system_metrics_simulation_mode() #í´ë¼ì´ì–¸íŠ¸ë“¤ì˜ í‰ê· /std ì§‘ê³„í•˜ì—¬ "system_metrics.log"ì— ì¶”ê°€.

        if self.use_wandb and not self.wandb_online_track: #False
            try:
                import wandb
            except ImportError:
                logger.error("cfg.wandb.use=True but not install the wandb package")
                exit()

            from federatedscope.core.auxiliaries.logging import logfile_2_wandb_dict
            with open(os.path.join(self.outdir, "eval_results.log"), "r") as exp_log_f:
                # track the prediction related performance
                all_log_res, exp_stop_normal, last_line, log_res_best = \
                    logfile_2_wandb_dict(exp_log_f, raw_out=False)
                for log_res in all_log_res:
                    wandb.log(log_res)
                wandb.log(log_res_best)

                # track the system related performance
                sys_metric_f_name = os.path.join(self.outdir, "system_metrics.log")
                with open(sys_metric_f_name, "r") as f:
                    for line in f:
                        res = json.loads(line)
                        if res["id"] in ["sys_avg", "sys_std"]:
                            for k, v in res.items():
                                wandb.summary[k] = v

    def compress_raw_res_file(self): #ğŸ”¹ ëª©ì : eval_results.raw íŒŒì¼ì„ .gz ì••ì¶•í•˜ì—¬ ë””ìŠ¤í¬ ê³µê°„ ì ˆì•½. ğŸ§© ë™ì‘: eval_results.raw â†’ eval_results.raw.gz ë³€í™˜,  ê¸°ì¡´ íŒŒì¼ ì‚­ì œ
        """
        Compress the raw res file to be written to disk.
        """
        if not _is_main_process():  # âœ… rank0ë§Œ
            return
        old_f_name = os.path.join(self.outdir, "eval_results.raw")
        if os.path.exists(old_f_name):
            logger.info(
                "We will compress the file eval_results.raw into a .gz file, "
                "and delete the old one")
            with open(old_f_name, 'rb') as f_in:
                with gzip.open(old_f_name + ".gz", 'wb') as f_out:
                    shutil.copyfileobj(f_in, f_out)
            os.remove(old_f_name)


    def format_eval_res(self,
                        results,
                        rnd,
                        role=-1,
                        forms=None,
                        return_raw=False): #ğŸ”¹ ëª©ì : ê° ë¼ìš´ë“œ ê²°ê³¼(results)ë¥¼ ì—¬ëŸ¬ í˜•íƒœ(avg, weighted_avg, fairness, raw)ë¡œ í¬ë§·í•˜ì—¬ ë°˜í™˜ #í‰ê°€ ë¡œê·¸ ìƒí™©ì—ì„œ ì„œë²„, í´ë¼ì´ì–¸íŠ¸ì—ì„œ í˜¸ì¶œ.
        #í´ë¼ì´ì–¸íŠ¸: federatedscope/core/workers/client.py (raw í¬ë§· ë§Œë“¤ ë–„)
        #####  callback_funcs_for_evaluate í˜¹ì€ ìœ ì‚¬í•œ evaluate í•¸ë“¤ëŸ¬ ë‚´ì—ì„œ monitor.format_eval_res(results_client, rnd, role=f"Client #{cid}", forms=['raw'], return_raw=True)â†’ Raw ë¼ì¸(JSONL)ìœ¼ë¡œ ê¸°ë¡í•˜ëŠ” ìš©ë„, round_formatted_results_raw ë°˜í™˜
        # ì„œë²„: federatedscope/core/workers/server.py (avg/weighted_avg/fairness ì§‘ê³„ ë§Œë“¤ ë–„). merge_eval_results_from_all_clients methodì—ì„œ ì“°ì„.
        #####  formatted = monitor.format_eval_res(results_aggregated, rnd, role="Server #", forms=['weighted_avg','avg','fairness','raw']), round_formatted_results ë°˜í™˜
        ##### monitor.save_formatted_results(formatted)->eval_results.logì— í•œ ì¤„ append
        """
        Format the evaluation results from trainer.ctx.eval_results

        Args:
            results (dict): a dict to store the evaluation results {metric: value}
            rnd (int|string): FL round
            role (int|string): the output role
            forms (list): format type
            return_raw (bool): return either raw results, or other results

        Returns:
            dict: round_formatted_results / round_formatted_results_raw
        """




        # ê³µí†µ í—¤ë” êµ¬ì„±.
        #formsê°€ ì§€ì • ì•ˆë˜ë©´ 4ê°€ì§€ ëª¨ë‘ ê³„ì‚° ëŒ€ìƒìœ¼ë¡œ ì¡ìŒ. ë°˜í™˜ ë”•ì…”ë„ˆë¦¬ì˜ ê³µí†µ í—¤ë”(ëˆ„ê°€, ëª‡ ë¼ìš´ë“œì¸ì§€) ì„¸íŒ….
        if forms is None:
            forms = ['weighted_avg', 'avg', 'fairness', 'raw']
        round_formatted_results = {'Role': role, 'Round': rnd}
        round_formatted_results_raw = {'Role': role, 'Round': rnd}

        if 'group_avg' in forms: #íŠ¹ìˆ˜ ë¶„ê¸°
            new_results = {}
            num_of_client_for_data = self.cfg.data.num_of_client_for_data #[]
            client_start_id = 1
            for group_id, num_clients in enumerate(num_of_client_for_data):
                if client_start_id > len(results):
                    break
                group_res = copy.deepcopy(results[client_start_id])
                num_div = num_clients - max(
                    0, client_start_id + num_clients - len(results) - 1)
                for client_id in range(client_start_id,
                                    client_start_id + num_clients):
                    if client_id > len(results):
                        break
                    for k, v in group_res.items():
                        if isinstance(v, dict):
                            for kk in v:
                                if client_id == client_start_id:
                                    group_res[k][kk] /= num_div
                                else:
                                    group_res[k][kk] += results[client_id][k][kk] / num_div
                        else:
                            if client_id == client_start_id:
                                group_res[k] /= num_div
                            else:
                                group_res[k] += results[client_id][k] / num_div
                new_results[group_id + 1] = group_res
                client_start_id += num_clients
                round_formatted_results['Results_group_avg'] = new_results
        else: #ì¼ë°˜ ì¼€ì´ìŠ¤: forms ë£¨í”„
            for form in forms:
                new_results = copy.deepcopy(results)
                if not str(role).lower().startswith('server') or form == 'raw': #í´ë¼ì´ì–¸íŠ¸ ì—­í• ì´ê±°ë‚˜ raw í¬ë§·ì´ë©´: ê·¸ëƒ¥ ì›ë³¸(results)ì„ Results_rawë¡œ ë¶™ì´ê³  ë.
                    round_formatted_results_raw['Results_raw'] = new_results
                elif form not in Monitor.SUPPORTED_FORMS:
                    continue
                else: #ì„œë²„ ì—­í•  + SUPPORTED_FORMS(= ['weighted_avg','avg','fairness','raw'])ì¼ ë•Œë§Œ ì•„ë˜ ì§‘ê³„ ê³„ì‚°ì„ ìˆ˜í–‰. ì—¬ê¸°ì„œ ì“°ëŠ” resultsëŠ” **ì„œë²„ê°€ í´ë¼ì´ì–¸íŠ¸ë¡œë¶€í„° ëª¨ì•„ì˜¨ ê° metricë“¤ì˜ "í´ë¼ì´ì–¸íŠ¸ë³„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸"**ì…ë‹ˆë‹¤.
                    for key in results.keys():
                        dataset_name = key.split("_")[0] # ì˜ˆ: "val_acc" â†’ "val"
                        if f'{dataset_name}_total' not in results:
                            raise ValueError(
                                "Results to be formatted should include the dataset_num in the dict, "
                                f"with key = {dataset_name}_total")


                        dataset_num = np.array(results[f'{dataset_name}_total'])

                        # === total / correctëŠ” í‰ê· ë§Œ ê³„ì‚°í•˜ê³  ê±´ë„ˆë›°ê¸° ===
                        if key in [f'{dataset_name}_total', f'{dataset_name}_correct']:
                            new_results[key] = np.mean(new_results[key])
                            continue  # ì•„ë˜ weighted_avg / avg / fairness ê³„ì‚° ê±´ë„ˆëœ€

                        # === ë‚˜ë¨¸ì§€ metricì€ ì§‘ê³„ ë°©ì‹ì— ë”°ë¼ ì²˜ë¦¬ ===
                        all_res = np.array(copy.copy(results[key])) #ì„œë²„ê°€ ëª¨ì€ í´ë¼ì´ì–¸íŠ¸ë³„ metric ê°’ì˜ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ëª¨ë“  í´ë¼ì˜ val_acc ëª©ë¡).
                        if form == 'weighted_avg': #ì„ì˜ì˜ keyì— ëŒ€í•´ì„œ weight avg. train/val/testì¸ì§€ëŠ” ì•ì„  dataset_nameì—ì„œ ê²°ì •ë¨. dataset_numë„ ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê²°ì •ëì—ˆìŒ. 
                            new_results[key] = np.sum(
                                np.array(new_results[key]) * dataset_num) / np.sum(dataset_num)
                        elif form == "avg":
                            new_results[key] = np.mean(new_results[key])
                        elif form == "fairness" and all_res.size > 1:
                            new_results.pop(key, None)
                            all_res.sort()
                            new_results[f"{key}_std"] = np.std(np.array(all_res)) #í‘œì¤€í¸ì°¨: í´ë¼ ê°„ ê°’ì˜ í©ì–´ì§ ì •ë„. ê°’ì´ í´ìˆ˜ë¡ ë¶ˆê· í˜•/ê²©ì°¨ê°€ í¼.
                            new_results[f"{key}_bottom_decile"] = all_res[all_res.size // 10] #í•˜ìœ„ 10% ì§€ì ì˜ ê°’(10ë²ˆì§¸ ë¶„ìœ„ê°’)
                            new_results[f"{key}_top_decile"] = all_res[all_res.size * 9 // 10] #ìƒìœ„ 10% ì§€ì ì˜ ê°’(90ë²ˆì§¸ ë¶„ìœ„ê°’)
                            new_results[f"{key}_min"] = all_res[0] #ë¶„í¬ì˜ ìµœì†Ÿê°’
                            new_results[f"{key}_max"] = all_res[-1] #ë¶„í¬ì˜ ìµœëŒ“ê°’
                            new_results[f"{key}_bottom10%"] = np.mean(all_res[:all_res.size // 10]) #í•˜ìœ„ 10% êµ¬ê°„ì˜ í‰ê· 
                            new_results[f"{key}_top10%"] = np.mean(all_res[all_res.size * 9 // 10:]) #ìƒìœ„ 10% êµ¬ê°„ì˜ í‰ê· 
                            new_results[f"{key}_cos1"] = np.mean(all_res) / (np.sqrt(np.mean(all_res**2))) #ë²¡í„° all_resì™€ ëª¨ë‘ ê°™ì€ ê°’ì¸ ë²¡í„°(ì˜ˆ: 1,1,1,...) ì˜ â€œìœ ì‚¬ë„â€.
                            all_res_preprocessed = all_res + 1e-9 #ì•ˆì •í™”ë¥¼ ìœ„í•œ ì „ì²˜ë¦¬.
                            new_results[f"{key}_entropy"] = np.sum(
                                -all_res_preprocessed / np.sum(all_res_preprocessed) *
                                (np.log(all_res_preprocessed / np.sum(all_res_preprocessed)))) #ë¶„í¬ë¥¼ ì •ê·œí™”í•´ì„œ í™•ë¥ ì²˜ëŸ¼ ë§Œë“¤ê³ (ê° ì—”íŠ¸ë¡œí”¼ ê°’ / í•©) ë¥¼ ê³„ì‚°. ê°’ì´ ë†’ì„ìˆ˜ë¡ ë¶„í¬ê°€ í¼ì ¸ ìˆìŒ(ê· ì¼ì— ê°€ê¹Œì›€), ë‚®ì„ìˆ˜ë¡ í•œìª½ì— ëª°ë¦¼.



                    round_formatted_results[f'Results_{form}'] = new_results

        # â›”ï¸ ë” ì´ìƒ ì—¬ê¸°ì„œ íŒŒì¼ì„ ì“°ì§€ ì•ŠìŒ (rank0ê°€ client.pyì—ì„œ ê¸°ë¡)
        return round_formatted_results_raw if return_raw else round_formatted_results




    def calc_model_metric(self, last_model, local_updated_models, rnd): #ğŸ”¹ ëª©ì : ëª¨ë¸ ìˆ˜ì¤€ì˜ ë©”íŠ¸ë¦­(cos_sim, l2_distance, ë“±)ì„ ê³„ì‚°í•˜ì—¬ ë¡œê¹… #ì„œë²„ ë¼ìš´ë“œ ëì—ì„œ í˜¸ì¶œ. self.cfg.eval.monitoring=[]ë¼ ì˜ë¯¸ ì—†ëŠ” ë“¯.
        """
        Arguments:
            last_model (dict): the state of last round.
            local_updated_models (list): each element is (data_size, model).

        Returns:
            dict: model_metric_dict
        """

        """
        ğŸ“¦ ì¸ì:

            last_model: ì „ ë¼ìš´ë“œ ì„œë²„ ëª¨ë¸

            local_updated_models: ê° í´ë¼ì´ì–¸íŠ¸ì˜ ëª¨ë¸


        ğŸ“¦ ê²°ê³¼:
            {'train_cos_sim': 0.97, 'train_l2_dist': 1.23}

        """

        model_metric_dict = {}
        for metric in self.cfg.eval.monitoring: #self.cfg.eval.monitoring=[]
            func_name = f'calc_{metric}'
            calc_metric = getattr(
                import_module('federatedscope.core.monitors.metric_calculator'),
                func_name)
            metric_value = calc_metric(last_model, local_updated_models)
            model_metric_dict[f'train_{metric}'] = metric_value
        formatted_log = {
            'Role': 'Server #',
            'Round': rnd,
            'Results_model_metric': model_metric_dict
        }
        if len(model_metric_dict.keys()):
            logger.info(formatted_log)

        return model_metric_dict

    def convert_size(self, size_bytes): #ğŸ”¹ ëª©ì : ë°”ì´íŠ¸ ë‹¨ìœ„ ê°’ì„ ì‚¬ëŒì´ ë³´ê¸° ì‰¬ìš´ ë‹¨ìœ„(ì˜ˆ: MB, GB)ë¡œ ë³€í™˜. ğŸ“¦ ì˜ˆì‹œ: convert_size(1048576) â†’ '1.0M'
        """
        Convert bytes to human-readable size.
        """
        import math
        if size_bytes <= 0:
            return str(size_bytes)
        size_name = ("", "K", "M", "G", "T", "P", "E", "Z", "Y")
        i = int(math.floor(math.log(size_bytes, 1024)))
        p = math.pow(1024, i)
        s = round(size_bytes / p, 2)
        return f"{s}{size_name[i]}"

    def track_model_size(self, models): #ğŸ”¹ ëª©ì : ëª¨ë¸ì˜ ì „ì²´ íŒŒë¼ë¯¸í„° ìˆ˜ë¥¼ ê³„ì‚° (self.total_model_sizeì— ì €ì¥).  ğŸ“¦ í˜¸ì¶œ ì‹œì : í•™ìŠµ ì‹œì‘ ì „ (ë³´í†µ client.py ì´ˆê¸°í™”ì—ì„œ)
        """
        calculate the total model size given the models hold by the worker/trainer
        """
        if self.total_model_size != 0:
            logger.warning(
                "the total_model_size is not zero. You may have been "
                "calculated the total_model_size before")

        if not hasattr(models, '__iter__'):
            models = [models]
        for model in models:
            assert isinstance(model, torch.nn.Module), \
                f"the `model` should be type torch.nn.Module when " \
                f"calculating its size, but got {type(model)}"
            for name, para in model.named_parameters():
                self.total_model_size += para.numel()

    def track_avg_flops(self, flops, sample_num=1): #ğŸ”¹ ëª©ì : ìƒ˜í”Œë‹¹ í‰ê·  FLOPsë¥¼ ì¶”ì . ëˆ„ì  ë°©ì‹: moving average #í•™ìŠµ ì¤‘ í˜¸ì¶œ
        """
        update the average flops for forwarding each data sample,
        for most models and tasks,
        the averaging is not needed as the input shape is fixed
        """
        self.flops_per_sample = (self.flops_per_sample * self.flop_count + flops) / (self.flop_count + sample_num)
        self.flop_count += 1

    def track_upload_bytes(self, bytes): #ëª©ì : í´ë¼ì´ì–¸íŠ¸ê°€ ì„œë²„ë¡œ ì—…ë¡œë“œí•œ ëª¨ë¸ í¬ê¸° ëˆ„ì . # CommManager.send() ë‚´ë¶€ì—ì„œ í˜¸ì¶œ
        """
        Track the number of bytes uploaded.
        """
        self.total_upload_bytes += bytes

    def track_download_bytes(self, bytes): #ëª©ì : ì„œë²„ê°€ í´ë¼ì´ì–¸íŠ¸ë¡œ ë‚´ë ¤ë³´ë‚¸ ëª¨ë¸ í¬ê¸° ëˆ„ì  # CommManagerì—ì„œ í˜¸ì¶œ
        """
        Track the number of bytes downloaded.
        """
        self.total_download_bytes += bytes

    def update_best_result(self, best_results, new_results, results_type): #ğŸ”¹ ëª©ì : í˜„ì¬ í‰ê°€ ê²°ê³¼ê°€ ê¸°ì¡´ bestë³´ë‹¤ ë” ë‚«ë‹¤ë©´ ê°±ì‹ . ğŸ“¦ ë¡œì§ ìš”ì•½: round_wise_update_keyì— ë”°ë¼ ë” ì¢‹ì€ ì„±ëŠ¥ì¸ì§€ íŒë‹¨ (lossëŠ” ì‘ì„ìˆ˜ë¡, accëŠ” í´ìˆ˜ë¡) #í‰ê°€ í›„ì— í˜¸ì¶œ.

        """
        best_results: ì§€ê¸ˆê¹Œì§€ ì €ì¥ëœ best ê²°ê³¼ë“¤ì˜ ë”•ì…”ë„ˆë¦¬
        ì˜ˆ: {"server_best": {"val_loss": 0.52, "val_acc": 0.88}, ...}

        new_results: í˜„ì¬ ë¼ìš´ë“œì—ì„œ ë‚˜ì˜¨ í‰ê°€ ê²°ê³¼
        ì˜ˆ: {"val_loss": 0.48, "val_acc": 0.86}

        results_type: í˜„ì¬ ì—…ë°ì´íŠ¸ ëŒ€ìƒì˜ í‚¤
        ì˜ˆ: "server_best", "client_best_individual"

        self.round_wise_update_key: ì–´ë–¤ ì§€í‘œë¥¼ ê¸°ì¤€ìœ¼ë¡œ bestì¸ì§€ ê²°ì • (ì˜ˆ: "val_loss")

        self.the_larger_the_better: í•´ë‹¹ ì§€í‘œê°€ í´ìˆ˜ë¡ ì¢‹ì€ì§€ ì—¬ë¶€ (ì˜ˆ: "acc"ëŠ” True, "loss"ëŠ” False
        
        
        """

        """
        Update best evaluation results.
        by default, the update is based on validation loss with
        ``round_wise_update_key="val_loss" ``
        """
        update_best_this_round = False
        if not isinstance(new_results, dict):
            raise ValueError(
                f"update best results require `results` a dict, but got"
                f" {type(new_results)}")
        else:
            #ì´ˆê¸°í™”.
            if results_type not in best_results:
                best_results[results_type] = dict()
            best_result = best_results[results_type]

            # update different keys separately: the best values can be in different rounds
            if self.round_wise_update_key is None: #test_lossë¼ ë³´í†µ ì—¬ê¸°ì— ì•ˆê±¸ë¦¼!!
                for key in new_results:
                    cur_result = new_results[key]
                    if 'loss' in key or 'std' in key:  # the smaller, the better
                        if results_type in [
                                "client_best_individual",
                                "unseen_client_best_individual"
                        ]:
                            cur_result = min(cur_result) # í´ë¼ì´ì–¸íŠ¸  ê°œë³„ ê²°ê³¼ë©´ ìµœì†Œ
                        if key not in best_result or cur_result < best_result[key]:
                            best_result[key] = cur_result
                            update_best_this_round = True

                    elif 'acc' in key:  # the larger, the better
                        if results_type in [
                                "client_best_individual",
                                "unseen_client_best_individual"
                        ]:
                            cur_result = max(cur_result) # í´ë¼ì´ì–¸íŠ¸  ê°œë³„ ê²°ê³¼ë©´ ìµœëŒ€
                        if key not in best_result or cur_result > best_result[key]:
                            best_result[key] = cur_result
                            update_best_this_round = True
                    else:
                        # unconcerned metric
                        pass
            # update different keys round-wise: if find better round_wise_update_key,
            # update others at the same time
            else:
                found_round_wise_update_key = False
                sorted_keys = [] #new_resultsì˜ keyë¥¼ ì§‘ì–´ë„£ì„ ì˜ˆì •. ë‹¤ë§Œ self.round_wise_update_keyë¥¼ ë§¨ ì•ìœ¼ë¡œ í•  ê²ƒ!!
                for key in new_results:
                    if self.round_wise_update_key in key: #self.round_wise_update_key: test_loss
                        sorted_keys.insert(0, key) #ë¦¬ìŠ¤íŠ¸ì˜ ë§¨ ì•ì— keyë¥¼ ì‚½ì….
                        found_round_wise_update_key = key
                    else:
                        sorted_keys.append(key)
                if not found_round_wise_update_key:
                    raise ValueError(
                        "Your specified eval.best_res_update_round_wise_key "
                        "is not in target results, use another key or check the name. \n"
                        f"Got eval.best_res_update_round_wise_key={self.round_wise_update_key}, "
                        f"the keys of results are {list(new_results.keys())}")

                # the first key must be the `round_wise_update_key`
                cur_result = new_results[found_round_wise_update_key] #self.round_wise_update_key ê´€ì ì˜ metric ê°€ì ¸ì˜´.

                if self.the_larger_the_better:
                    # The larger, the better
                    if results_type in [
                            "client_best_individual",
                            "unseen_client_best_individual"
                    ]:
                        cur_result = max(cur_result)#clientë“¤ ê²°ê³¼ì¤‘ maxì¸ê±¸ë¡œ ê°€ì ¸ì˜´.
                    if found_round_wise_update_key not in best_result or cur_result > best_result[found_round_wise_update_key]:
                        best_result[found_round_wise_update_key] = cur_result
                        update_best_this_round = True
                else:
                    # The smaller, the better
                    if results_type in [
                            "client_best_individual",
                            "unseen_client_best_individual"
                    ]:
                        cur_result = min(cur_result) #clientë“¤ ê²°ê³¼ì¤‘ minì¸ê±¸ë¡œ ê°€ì ¸ì˜´.
                    if found_round_wise_update_key not in best_result or cur_result < best_result[found_round_wise_update_key]:
                        best_result[found_round_wise_update_key] = cur_result
                        update_best_this_round = True

                # update other metrics only if update_best_this_round is True
                if update_best_this_round: #ì´ë²ˆ ë¼ìš´ë“œë•Œ bestê°€ ì—…ë°ì´íŠ¸ ë˜ëŠ” ê²½ìš°.
                    for key in sorted_keys[1:]: #self.round_wise_update_key ì œì™¸í•œ ê²ƒë“¤ì— ëŒ€í•´ ìµœì‹  ê²ƒìœ¼ë¡œ ì—…ë°ì´íŠ¸
                        cur_result = new_results[key]
                        if results_type in [
                                "client_best_individual",
                                "unseen_client_best_individual"
                        ]:
                            # Obtain whether the larger the better
                            for mode in ['train', 'val', 'test']:
                                if mode in key:
                                    _key = key.split(f'{mode}_')[1]
                                    if self.metric_calculator.eval_metric[_key][1]:#lagerì´ë©´ ì¢‹ì€ ê²ƒ.
                                        cur_result = max(cur_result)
                                    else:#smallerì´ë©´ ì¢‹ì€ ê²ƒ.
                                        cur_result = min(cur_result)
                        best_result[key] = cur_result #ë¹„êµ ì—†ì´ ë®ì–´ì”€.

        if update_best_this_round:
            line = f"Find new best result: {best_results}"
            logging.info(line) #ë¡œê·¸ë¡œ ë‚¨ê¹€.
            if self.use_wandb and self.wandb_online_track: #False
                try:
                    import wandb
                    exp_stop_normal = False
                    exp_stop_normal, log_res = logline_2_wandb_dict(
                        exp_stop_normal,
                        line,
                        self.log_res_best,
                        raw_out=False)
                    for k, v in self.log_res_best.items():
                        wandb.summary[k] = v
                except ImportError:
                    logger.error(
                        "cfg.wandb.use=True but not install the wandb package")
                    exit()
        return update_best_this_round #bestê°€ ê²½ì‹ ë˜ì—ˆëŠ”ì§€ì˜ ì—¬ë¶€ë¥¼ return.

    def add_items_to_best_result(self, best_results, new_results, results_type): #ğŸ”¹ ëª©ì : íŠ¹ì • í‰ê°€ê²°ê³¼ë¥¼ best result dictì— ë‹¨ìˆœ ì¶”ê°€. ğŸ“¦ ì°¨ì´ì : update_best_resultëŠ” ë¹„êµ í›„ ì—…ë°ì´íŠ¸. add_items_to_best_resultëŠ” ë¹„êµ ì—†ì´ ê·¸ëƒ¥ ì‚½ì…
        #PFL ë“± í›„ì²˜ë¦¬ì— í˜¸ì¶œ
        """
        Add a new key: value item (results-type: new_results) to best_result
        """
        best_results[results_type] = new_results
