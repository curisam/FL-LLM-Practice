2025-08-18 18:16:28 (root:426) INFO: [logger] file handler -> exp/tldr/choice_qwen/fedbis_test/exp_print.log
2025-08-18 18:16:28 (root:47) INFO: [main] outdir=exp/tldr/choice_qwen/fedbis_test
2025-08-18 18:16:52 (federatedscope.core.data.base_translator:290) INFO: Main process: Completion file found. Skipping generation.
2025-08-18 18:17:35 (federatedscope.core.data.base_translator:324) INFO: [Final Split Summary][loaded][server=0][rank=0/4] Train=92858, Val=33082, Test=50715, Total=176655
2025-08-18 18:17:35 (federatedscope.core.data.base_translator:333) INFO: [Final Split Summary][loaded][client=1][rank=0/4] Train=2793, Val=146, Test=40, Total=2979
2025-08-18 18:17:35 (federatedscope.core.data.base_translator:333) INFO: [Final Split Summary][loaded][client=2][rank=0/4] Train=214, Val=11, Test=40, Total=265
2025-08-18 18:17:35 (federatedscope.core.data.base_translator:333) INFO: [Final Split Summary][loaded][client=3][rank=0/4] Train=691, Val=36, Test=40, Total=767
2025-08-18 18:17:35 (federatedscope.core.data.base_translator:333) INFO: [Final Split Summary][loaded][client=4][rank=0/4] Train=213, Val=11, Test=40, Total=264
2025-08-18 18:17:35 (federatedscope.core.data.base_translator:333) INFO: [Final Split Summary][loaded][client=5][rank=0/4] Train=285, Val=14, Test=40, Total=339
2025-08-18 18:17:35 (federatedscope.core.data.base_translator:333) INFO: [Final Split Summary][loaded][client=6][rank=0/4] Train=2547, Val=134, Test=40, Total=2721
2025-08-18 18:17:35 (federatedscope.core.data.base_translator:333) INFO: [Final Split Summary][loaded][client=7][rank=0/4] Train=1088, Val=57, Test=40, Total=1185
2025-08-18 18:17:35 (federatedscope.core.data.base_translator:333) INFO: [Final Split Summary][loaded][client=8][rank=0/4] Train=1316, Val=69, Test=40, Total=1425
2025-08-18 18:17:35 (federatedscope.core.data.base_translator:333) INFO: [Final Split Summary][loaded][client=9][rank=0/4] Train=3572, Val=188, Test=40, Total=3800
2025-08-18 18:17:35 (federatedscope.core.data.base_translator:333) INFO: [Final Split Summary][loaded][client=10][rank=0/4] Train=1209, Val=63, Test=40, Total=1312
2025-08-18 18:17:35 (federatedscope.core.data.base_translator:333) INFO: [Final Split Summary][loaded][client=11][rank=0/4] Train=621, Val=32, Test=40, Total=693
2025-08-18 18:17:35 (federatedscope.core.data.base_translator:333) INFO: [Final Split Summary][loaded][client=12][rank=0/4] Train=2605, Val=137, Test=40, Total=2782
2025-08-18 18:17:35 (federatedscope.core.data.base_translator:333) INFO: [Final Split Summary][loaded][client=13][rank=0/4] Train=1372, Val=72, Test=40, Total=1484
2025-08-18 18:17:35 (federatedscope.core.data.base_translator:333) INFO: [Final Split Summary][loaded][client=14][rank=0/4] Train=3055, Val=160, Test=40, Total=3255
2025-08-18 18:17:35 (federatedscope.core.data.base_translator:333) INFO: [Final Split Summary][loaded][client=15][rank=0/4] Train=14550, Val=200, Test=40, Total=14790
2025-08-18 18:17:35 (federatedscope.core.data.base_translator:333) INFO: [Final Split Summary][loaded][client=16][rank=0/4] Train=2589, Val=136, Test=40, Total=2765
2025-08-18 18:17:35 (federatedscope.core.data.base_translator:333) INFO: [Final Split Summary][loaded][client=17][rank=0/4] Train=5883, Val=200, Test=40, Total=6123
2025-08-18 18:17:35 (federatedscope.core.data.base_translator:333) INFO: [Final Split Summary][loaded][client=18][rank=0/4] Train=2576, Val=135, Test=40, Total=2751
2025-08-18 18:17:35 (federatedscope.core.data.base_translator:333) INFO: [Final Split Summary][loaded][client=19][rank=0/4] Train=2102, Val=110, Test=40, Total=2252
2025-08-18 18:17:35 (federatedscope.core.data.base_translator:333) INFO: [Final Split Summary][loaded][client=20][rank=0/4] Train=2399, Val=126, Test=40, Total=2565
2025-08-18 18:17:35 (federatedscope.core.data.base_translator:333) INFO: [Final Split Summary][loaded][client=21][rank=0/4] Train=2915, Val=153, Test=40, Total=3108
2025-08-18 18:17:35 (federatedscope.core.data.base_translator:333) INFO: [Final Split Summary][loaded][client=22][rank=0/4] Train=224, Val=11, Test=40, Total=275
2025-08-18 18:17:35 (federatedscope.core.data.base_translator:333) INFO: [Final Split Summary][loaded][client=23][rank=0/4] Train=583, Val=30, Test=40, Total=653
2025-08-18 18:17:35 (federatedscope.core.data.base_translator:333) INFO: [Final Split Summary][loaded][client=24][rank=0/4] Train=4944, Val=200, Test=40, Total=5184
2025-08-18 18:17:35 (federatedscope.core.data.base_translator:333) INFO: [Final Split Summary][loaded][client=25][rank=0/4] Train=4647, Val=200, Test=40, Total=4887
2025-08-18 18:17:35 (federatedscope.core.data.base_translator:333) INFO: [Final Split Summary][loaded][client=26][rank=0/4] Train=3063, Val=161, Test=40, Total=3264
2025-08-18 18:17:35 (federatedscope.core.data.base_translator:333) INFO: [Final Split Summary][loaded][client=27][rank=0/4] Train=2342, Val=123, Test=40, Total=2505
2025-08-18 18:17:35 (federatedscope.core.data.base_translator:333) INFO: [Final Split Summary][loaded][client=28][rank=0/4] Train=1434, Val=75, Test=40, Total=1549
2025-08-18 18:17:35 (federatedscope.core.data.base_translator:333) INFO: [Final Split Summary][loaded][client=29][rank=0/4] Train=6191, Val=200, Test=40, Total=6431
2025-08-18 18:17:35 (federatedscope.core.data.base_translator:333) INFO: [Final Split Summary][loaded][client=30][rank=0/4] Train=3247, Val=170, Test=40, Total=3457
2025-08-18 18:17:35 (federatedscope.core.data.base_translator:333) INFO: [Final Split Summary][loaded][client=31][rank=0/4] Train=3679, Val=193, Test=40, Total=3912
2025-08-18 18:17:35 (federatedscope.core.data.base_translator:333) INFO: [Final Split Summary][loaded][client=32][rank=0/4] Train=2144, Val=112, Test=40, Total=2296
2025-08-18 18:17:35 (federatedscope.core.data.base_translator:333) INFO: [Final Split Summary][loaded][client=33][rank=0/4] Train=1409, Val=74, Test=40, Total=1523
2025-08-18 18:17:35 (federatedscope.core.data.base_translator:333) INFO: [Final Split Summary][loaded][client=34][rank=0/4] Train=4486, Val=200, Test=40, Total=4726
2025-08-18 18:17:35 (federatedscope.core.data.base_translator:333) INFO: [Final Split Summary][loaded][client=35][rank=0/4] Train=4736, Val=200, Test=40, Total=4976
2025-08-18 18:17:35 (federatedscope.core.data.base_translator:333) INFO: [Final Split Summary][loaded][client=36][rank=0/4] Train=1030, Val=54, Test=40, Total=1124
2025-08-18 18:17:35 (federatedscope.core.data.base_translator:333) INFO: [Final Split Summary][loaded][client=37][rank=0/4] Train=4273, Val=200, Test=40, Total=4513
2025-08-18 18:17:35 (federatedscope.core.data.base_translator:333) INFO: [Final Split Summary][loaded][client=38][rank=0/4] Train=6171, Val=200, Test=40, Total=6411
2025-08-18 18:17:35 (federatedscope.core.data.base_translator:333) INFO: [Final Split Summary][loaded][client=39][rank=0/4] Train=1594, Val=83, Test=40, Total=1717
2025-08-18 18:17:35 (federatedscope.core.data.base_translator:333) INFO: [Final Split Summary][loaded][client=40][rank=0/4] Train=4005, Val=200, Test=40, Total=4245
2025-08-18 18:17:35 (federatedscope.core.data.base_translator:333) INFO: [Final Split Summary][loaded][client=41][rank=0/4] Train=2275, Val=119, Test=40, Total=2434
2025-08-18 18:17:35 (federatedscope.core.data.base_translator:333) INFO: [Final Split Summary][loaded][client=42][rank=0/4] Train=5772, Val=200, Test=40, Total=6012
2025-08-18 18:17:35 (federatedscope.core.data.base_translator:333) INFO: [Final Split Summary][loaded][client=43][rank=0/4] Train=1694, Val=89, Test=40, Total=1823
2025-08-18 18:17:35 (federatedscope.core.data.base_translator:333) INFO: [Final Split Summary][loaded][client=44][rank=0/4] Train=7916, Val=200, Test=40, Total=8156
2025-08-18 18:17:35 (federatedscope.core.data.base_translator:333) INFO: [Final Split Summary][loaded][client=45][rank=0/4] Train=1901, Val=100, Test=40, Total=2041
2025-08-18 18:17:35 (federatedscope.core.data.base_translator:333) INFO: [Final Split Summary][loaded][client=46][rank=0/4] Train=2100, Val=110, Test=40, Total=2250
2025-08-18 18:17:35 (federatedscope.core.data.base_translator:333) INFO: [Final Split Summary][loaded][client=47][rank=0/4] Train=2812, Val=147, Test=40, Total=2999
2025-08-18 18:17:35 (federatedscope.core.data.base_translator:333) INFO: [Final Split Summary][loaded][client=48][rank=0/4] Train=880, Val=46, Test=40, Total=966
2025-08-18 18:17:35 (federatedscope.core.data.base_translator:333) INFO: [Final Split Summary][loaded][client=49][rank=0/4] Train=2521, Val=132, Test=40, Total=2693
2025-08-18 18:17:35 (federatedscope.core.data.base_translator:333) INFO: [Final Split Summary][loaded][client=50][rank=0/4] Train=2527, Val=133, Test=40, Total=2700
2025-08-18 18:17:35 (federatedscope.core.data.base_translator:333) INFO: [Final Split Summary][loaded][client=51][rank=0/4] Train=1580, Val=83, Test=40, Total=1703
2025-08-18 18:17:35 (federatedscope.core.data.base_translator:333) INFO: [Final Split Summary][loaded][client=52][rank=0/4] Train=3589, Val=188, Test=40, Total=3817
2025-08-18 18:17:35 (federatedscope.core.data.base_translator:333) INFO: [Final Split Summary][loaded][client=53][rank=0/4] Train=6791, Val=200, Test=40, Total=7031
2025-08-18 18:17:36 (federatedscope.core.configs.config:256) INFO: the used configs are: 
adapter:
  use: False
aggregator:
  BFT_args:
    
  byzantine_node_num: 0
  inside_weight: 1.0
  num_agg_groups: 1
  num_agg_topk: []
  outside_weight: 0.0
  robust_rule: fedavg
asyn:
  use: False
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  edge_num: 100
  edge_path: edge_data/
  freq: 10
  info_diff_type: l2
  inject_round: 0
  insert_round: 100000
  label_type: dirty
  max_ite: 400
  mean: [0.9637]
  mia_is_simulate_in: False
  mia_simulate_in_round: 20
  pgd_eps: 2
  pgd_lr: 0.1
  pgd_poisoning: False
  poison_ratio: 0.5
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  scale_para: 1.0
  scale_poisoning: False
  self_epoch: 6
  self_lr: 0.05
  self_opt: False
  setting: fix
  std: [0.1592]
  target_label_ind: -1
  trigger_path: trigger/
  trigger_type: edge
backend: torch
cfg_file: 
check_completeness: False
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 64
  cSBM_phi: [0.5, 0.5, 0.5]
  cache_dir: 
  consistent_label_distribution: True
  drop_last: False
  file_path: 
  hetero_data_name: []
  hetero_synth_batch_size: 32
  hetero_synth_feat_dim: 128
  hetero_synth_prim_weight: 0.5
  is_debug: False
  load_splits: False
  loader: 
  max_query_len: 128
  max_seq_len: 384
  max_tgt_len: 128
  num_contrast: 0
  num_of_client_for_data: []
  num_steps: 30
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  save_data: False
  save_splits: False
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.9, 0.09, 0.01]
  splits_path: ./final_data_splits
  splitter: meta
  splitter_args: []
  subsample: 1.0
  target_transform: []
  test_pre_transform: []
  test_target_transform: []
  test_transform: []
  transform: []
  trunc_stride: 128
  type: reddit-tldr-comparison-choice@llm
  val_pre_transform: []
  val_target_transform: []
  val_transform: []
  walk_length: 2
dataloader:
  batch_size: 2
  drop_last: False
  num_steps: 30
  num_workers: 0
  pin_memory: False
  shuffle: True
  sizes: [10, 5]
  theta: -1
  type: base
  walk_length: 2
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 0
eval:
  best_res_update_round_wise_key: test_loss
  count_flops: False
  freq: 25
  metrics: ['loss', 'acc']
  monitoring: []
  report: ['weighted_avg', 'avg', 'fairness', 'raw']
  split: ['val', 'test']
expname: 
expname_tag: 
feat_engr:
  num_bins: 5
  scenario: hfl
  secure:
    dp:
      
    encrypt:
      type: dummy
    key_size: 3072
    type: encrypt
  selec_threshold: 0.05
  selec_woe_binning: quantile
  type: 
federate:
  atc_load_from: 
  atc_vanilla: False
  client_idx_for_local_train: 0
  client_num: 53
  data_weighted_aggr: False
  ignore_weight: True
  join_in_info: []
  make_global_eval: False
  master_addr: 127.0.0.1
  master_port: 29500
  merge_test_data: False
  merge_val_data: False
  method: FedAvg
  mode: standalone
  online_aggr: False
  process_num: 1
  resource_info_file: 
  restore_from: 
  sample_client_num: 5
  sample_client_rate: -1.0
  sampler: uniform
  save_client_model: False
  save_freq: 25
  save_to: checkpoints_test/tldr_choice_qwen_fedbis.ckpt
  share_local_model: True
  total_round_num: 200
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  use: False
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
fedswa:
  use: False
finetune:
  batch_or_epoch: epoch
  before_eval: False
  epoch_linear: 10
  freeze_param: 
  local_param: []
  local_update_steps: 1
  lr_linear: 0.005
  optimizer:
    lr: 0.1
    type: SGD
  scheduler:
    type: 
    warmup_ratio: 0.0
  simple_tuning: False
  weight_decay: 0.0
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_accum_count: 1
  grad_clip: -1.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    pi_lr: 0.01
    psn: False
    sched: auto
    ss: 
    use: False
  fts:
    M: 100
    M_target: 200
    allow_load_existing_info: True
    diff: False
    fed_bo_max_iter: 50
    g_var: 1e-06
    gp_opt_schedule: 1
    local_bo_epochs: 50
    local_bo_max_iter: 50
    ls: 1.0
    obs_noise: 1e-06
    ss: 
    target_clients: []
    use: False
    v_kernel: 1.0
    var: 0.1
  init_cand_num: 16
  larger_better: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  pfedhpo:
    discrete: False
    ss: 
    target_fl_total_round: 1000
    train_anchor: False
    train_fl: False
    use: False
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    iter: 0
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
  trial_index: 0
  working_folder: hpo
llm:
  accelerator:
    config: 
    use: True
  adapter:
    args: [{'adapter_package': 'peft', 'adapter_method': 'lora', 'r': 8, 'lora_alpha': 16, 'lora_dropout': 0.05, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']}]
    count: 1
    grouping:
      round: 50
      use: False
    local_only: False
    mv_to_cpu: False
    use: True
    warmup:
      round: 10
      use: False
  cache:
    model: 
  chat:
    max_history_len: 10
    max_len: 1024
  deepspeed:
    ds_config: 
    use: False
  fedrlhf:
    config_file: 
    frequency: 100
    pretrained: False
    train:
      batch_or_epoch: batch
      local_update_steps: 10
    use: False
  grad_accum_step: 2
  max_new_token: 60
  num_completions: 2
  offsite_tuning:
    emu_align:
      data:
        root: data
        splits: [0.8, 0.1, 0.1]
        type: alpaca@llm
      exit_after_align: False
      init_enable_ground_truth: False
      initial_only: True
      kl_divergence: raw
      layerwise_distill: False
      restore_from: 
      save_to: 
      sim_loss: l2
      train:
        batch_or_epoch: batch
        enable_ground_truth: False
        initial_update_rounds: 50
        kd_loss_weight: 0.9
        lm_loss_weight: 0.1
        local_update_steps: 10
        optimizer:
          lr: 0.01
          type: SGD
      use: False
    emu_l: 1
    emu_r: 10
    eval_type: emu
    kwargs: [{}]
    llm_generated:
      ratio: 0.1
      use: False
    save_full_model: False
    strategy: drop_layer
    use: False
  retry_on_nan_loss: False
  reward_coeff: 0.1
  rlhf: False
  tok_len: 1024
model:
  contrast_temp: 1.0
  contrast_topk: 100
  downstream_tasks: []
  dropout: 0.5
  embed_size: 8
  gamma: 0
  graph_pooling: mean
  hidden: 256
  in_channels: 0
  input_shape: ()
  label_smoothing: 0.1
  lambda_: 0.1
  layer: 2
  length_penalty: 2.0
  llm_kwargs: [{}]
  llm_type: CausalLM
  load_from_local_pretrained_fs_config: 
  load_from_local_pretrained_model_path: 
  max_answer_len: 30
  max_length: 200
  max_tree_depth: 3
  min_length: 1
  model_num_per_trainer: 1
  model_type: google/bert_uncased_L-2_H-128_A-2
  n_best_size: 20
  no_repeat_ngram_size: 3
  null_score_diff_threshold: 0.0
  num_beams: 5
  num_item: 0
  num_labels: 1
  num_of_trees: 10
  num_user: 0
  out_channels: 1
  pretrain_tasks: []
  stage: 
  task: node
  type: Qwen/Qwen2-0.5B@huggingface_llm
  use_bias: True
  use_contrastive_loss: False
nbafl:
  use: False
outdir: exp/tldr/choice_qwen/fedbis_test
personalization:
  K: 5
  beta: 1.0
  epoch_feature: 1
  epoch_linear: 2
  local_param: []
  local_update_steps: 30
  lr: 1e-05
  lr_feature: 0.1
  lr_linear: 0.1
  regular_weight: 0.1
  share_non_trainable_para: False
  weight_decay: 0.0
print_decimal_digits: 6
quantization:
  method: none
  nbits: 8
regularizer:
  mu: 0.0
  type: 
seed: 0
sgdmf:
  use: False
train:
  batch_or_epoch: batch
  data_para_dids: []
  is_enable_half: True
  local_update_steps: 30
  optimizer:
    betas: (0.9, 0.95)
    lr: 1e-05
    type: AdamW
  scheduler:
    gamma: 0.1
    milestones: [1, 2, 3, 4, 5]
    type: 
    warmup_ratio: 0.0
trainer:
  choices: ['A', 'B']
  disp_freq: 50
  local_entropy:
    alpha: 0.75
    eps: 0.0001
    gamma: 0.03
    inc_factor: 1.0
  sam:
    adaptive: False
    eta: 0.0
    rho: 1.0
  type: llmrewardchoicetrainer
  val_freq: 100000000
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False
2025-08-18 18:17:37 (federatedscope.core.auxiliaries.utils:175) INFO: The device information file is not provided
2025-08-18 18:17:37 (federatedscope.core.auxiliaries.model_builder:139) WARNING: The input shape is None. Please specify the `data.input_shape`(a tuple) or give the representative data to `get_model` if necessary
2025-08-18 18:17:49 (federatedscope.core.fed_runner:211) INFO: Server has been set up ... 
2025-08-18 18:17:49 (federatedscope.llm.trainer.reward_choice_trainer:106) INFO: Choice token IDs: [362, 425]
2025-08-18 18:17:52 (federatedscope.core.fed_runner:275) INFO: Client 1 has been set up ... 
2025-08-18 18:17:53 (federatedscope.llm.trainer.reward_choice_trainer:106) INFO: Choice token IDs: [362, 425]
2025-08-18 18:17:56 (federatedscope.core.fed_runner:275) INFO: Client 2 has been set up ... 
2025-08-18 18:17:56 (federatedscope.llm.trainer.reward_choice_trainer:106) INFO: Choice token IDs: [362, 425]
2025-08-18 18:17:59 (federatedscope.core.fed_runner:275) INFO: Client 3 has been set up ... 
2025-08-18 18:18:00 (federatedscope.llm.trainer.reward_choice_trainer:106) INFO: Choice token IDs: [362, 425]
2025-08-18 18:18:02 (federatedscope.core.fed_runner:275) INFO: Client 4 has been set up ... 
2025-08-18 18:18:03 (federatedscope.llm.trainer.reward_choice_trainer:106) INFO: Choice token IDs: [362, 425]
2025-08-18 18:18:06 (federatedscope.core.fed_runner:275) INFO: Client 5 has been set up ... 
2025-08-18 18:18:06 (federatedscope.llm.trainer.reward_choice_trainer:106) INFO: Choice token IDs: [362, 425]
2025-08-18 18:18:10 (federatedscope.core.fed_runner:275) INFO: Client 6 has been set up ... 
2025-08-18 18:18:10 (federatedscope.llm.trainer.reward_choice_trainer:106) INFO: Choice token IDs: [362, 425]
2025-08-18 18:18:13 (federatedscope.core.fed_runner:275) INFO: Client 7 has been set up ... 
2025-08-18 18:18:14 (federatedscope.llm.trainer.reward_choice_trainer:106) INFO: Choice token IDs: [362, 425]
2025-08-18 18:18:17 (federatedscope.core.fed_runner:275) INFO: Client 8 has been set up ... 
2025-08-18 18:18:17 (federatedscope.llm.trainer.reward_choice_trainer:106) INFO: Choice token IDs: [362, 425]
2025-08-18 18:18:20 (federatedscope.core.fed_runner:275) INFO: Client 9 has been set up ... 
2025-08-18 18:18:21 (federatedscope.llm.trainer.reward_choice_trainer:106) INFO: Choice token IDs: [362, 425]
2025-08-18 18:18:24 (federatedscope.core.fed_runner:275) INFO: Client 10 has been set up ... 
2025-08-18 18:18:24 (federatedscope.llm.trainer.reward_choice_trainer:106) INFO: Choice token IDs: [362, 425]
2025-08-18 18:18:27 (federatedscope.core.fed_runner:275) INFO: Client 11 has been set up ... 
2025-08-18 18:18:27 (federatedscope.llm.trainer.reward_choice_trainer:106) INFO: Choice token IDs: [362, 425]
2025-08-18 18:18:30 (federatedscope.core.fed_runner:275) INFO: Client 12 has been set up ... 
2025-08-18 18:18:31 (federatedscope.llm.trainer.reward_choice_trainer:106) INFO: Choice token IDs: [362, 425]
2025-08-18 18:18:34 (federatedscope.core.fed_runner:275) INFO: Client 13 has been set up ... 
2025-08-18 18:18:34 (federatedscope.llm.trainer.reward_choice_trainer:106) INFO: Choice token IDs: [362, 425]
2025-08-18 18:18:37 (federatedscope.core.fed_runner:275) INFO: Client 14 has been set up ... 
2025-08-18 18:18:38 (federatedscope.llm.trainer.reward_choice_trainer:106) INFO: Choice token IDs: [362, 425]
2025-08-18 18:18:41 (federatedscope.core.fed_runner:275) INFO: Client 15 has been set up ... 
2025-08-18 18:18:41 (federatedscope.llm.trainer.reward_choice_trainer:106) INFO: Choice token IDs: [362, 425]
2025-08-18 18:18:44 (federatedscope.core.fed_runner:275) INFO: Client 16 has been set up ... 
2025-08-18 18:18:45 (federatedscope.llm.trainer.reward_choice_trainer:106) INFO: Choice token IDs: [362, 425]
2025-08-18 18:18:48 (federatedscope.core.fed_runner:275) INFO: Client 17 has been set up ... 
2025-08-18 18:18:48 (federatedscope.llm.trainer.reward_choice_trainer:106) INFO: Choice token IDs: [362, 425]
2025-08-18 18:18:51 (federatedscope.core.fed_runner:275) INFO: Client 18 has been set up ... 
2025-08-18 18:18:51 (federatedscope.llm.trainer.reward_choice_trainer:106) INFO: Choice token IDs: [362, 425]
2025-08-18 18:18:54 (federatedscope.core.fed_runner:275) INFO: Client 19 has been set up ... 
2025-08-18 18:18:54 (federatedscope.llm.trainer.reward_choice_trainer:106) INFO: Choice token IDs: [362, 425]
2025-08-18 18:18:57 (federatedscope.core.fed_runner:275) INFO: Client 20 has been set up ... 
2025-08-18 18:18:57 (federatedscope.llm.trainer.reward_choice_trainer:106) INFO: Choice token IDs: [362, 425]
2025-08-18 18:19:00 (federatedscope.core.fed_runner:275) INFO: Client 21 has been set up ... 
2025-08-18 18:19:00 (federatedscope.llm.trainer.reward_choice_trainer:106) INFO: Choice token IDs: [362, 425]
2025-08-18 18:19:02 (federatedscope.core.fed_runner:275) INFO: Client 22 has been set up ... 
2025-08-18 18:19:03 (federatedscope.llm.trainer.reward_choice_trainer:106) INFO: Choice token IDs: [362, 425]
2025-08-18 18:19:05 (federatedscope.core.fed_runner:275) INFO: Client 23 has been set up ... 
2025-08-18 18:19:06 (federatedscope.llm.trainer.reward_choice_trainer:106) INFO: Choice token IDs: [362, 425]
2025-08-18 18:19:08 (federatedscope.core.fed_runner:275) INFO: Client 24 has been set up ... 
2025-08-18 18:19:09 (federatedscope.llm.trainer.reward_choice_trainer:106) INFO: Choice token IDs: [362, 425]
2025-08-18 18:19:11 (federatedscope.core.fed_runner:275) INFO: Client 25 has been set up ... 
2025-08-18 18:19:12 (federatedscope.llm.trainer.reward_choice_trainer:106) INFO: Choice token IDs: [362, 425]
2025-08-18 18:19:14 (federatedscope.core.fed_runner:275) INFO: Client 26 has been set up ... 
2025-08-18 18:19:15 (federatedscope.llm.trainer.reward_choice_trainer:106) INFO: Choice token IDs: [362, 425]
2025-08-18 18:19:17 (federatedscope.core.fed_runner:275) INFO: Client 27 has been set up ... 
2025-08-18 18:19:18 (federatedscope.llm.trainer.reward_choice_trainer:106) INFO: Choice token IDs: [362, 425]
2025-08-18 18:19:20 (federatedscope.core.fed_runner:275) INFO: Client 28 has been set up ... 
2025-08-18 18:19:20 (federatedscope.llm.trainer.reward_choice_trainer:106) INFO: Choice token IDs: [362, 425]
2025-08-18 18:19:23 (federatedscope.core.fed_runner:275) INFO: Client 29 has been set up ... 
2025-08-18 18:19:23 (federatedscope.llm.trainer.reward_choice_trainer:106) INFO: Choice token IDs: [362, 425]
2025-08-18 18:19:26 (federatedscope.core.fed_runner:275) INFO: Client 30 has been set up ... 
2025-08-18 18:19:26 (federatedscope.llm.trainer.reward_choice_trainer:106) INFO: Choice token IDs: [362, 425]
2025-08-18 18:19:29 (federatedscope.core.fed_runner:275) INFO: Client 31 has been set up ... 
2025-08-18 18:19:29 (federatedscope.llm.trainer.reward_choice_trainer:106) INFO: Choice token IDs: [362, 425]
2025-08-18 18:19:32 (federatedscope.core.fed_runner:275) INFO: Client 32 has been set up ... 
2025-08-18 18:19:32 (federatedscope.llm.trainer.reward_choice_trainer:106) INFO: Choice token IDs: [362, 425]
2025-08-18 18:19:34 (federatedscope.core.fed_runner:275) INFO: Client 33 has been set up ... 
2025-08-18 18:19:35 (federatedscope.llm.trainer.reward_choice_trainer:106) INFO: Choice token IDs: [362, 425]
2025-08-18 18:19:37 (federatedscope.core.fed_runner:275) INFO: Client 34 has been set up ... 
2025-08-18 18:19:38 (federatedscope.llm.trainer.reward_choice_trainer:106) INFO: Choice token IDs: [362, 425]
2025-08-18 18:19:40 (federatedscope.core.fed_runner:275) INFO: Client 35 has been set up ... 
2025-08-18 18:19:41 (federatedscope.llm.trainer.reward_choice_trainer:106) INFO: Choice token IDs: [362, 425]
2025-08-18 18:19:43 (federatedscope.core.fed_runner:275) INFO: Client 36 has been set up ... 
2025-08-18 18:19:44 (federatedscope.llm.trainer.reward_choice_trainer:106) INFO: Choice token IDs: [362, 425]
2025-08-18 18:19:46 (federatedscope.core.fed_runner:275) INFO: Client 37 has been set up ... 
2025-08-18 18:19:47 (federatedscope.llm.trainer.reward_choice_trainer:106) INFO: Choice token IDs: [362, 425]
2025-08-18 18:19:49 (federatedscope.core.fed_runner:275) INFO: Client 38 has been set up ... 
2025-08-18 18:19:49 (federatedscope.llm.trainer.reward_choice_trainer:106) INFO: Choice token IDs: [362, 425]
2025-08-18 18:19:52 (federatedscope.core.fed_runner:275) INFO: Client 39 has been set up ... 
2025-08-18 18:19:52 (federatedscope.llm.trainer.reward_choice_trainer:106) INFO: Choice token IDs: [362, 425]
2025-08-18 18:19:55 (federatedscope.core.fed_runner:275) INFO: Client 40 has been set up ... 
2025-08-18 18:19:55 (federatedscope.llm.trainer.reward_choice_trainer:106) INFO: Choice token IDs: [362, 425]
2025-08-18 18:19:57 (federatedscope.core.fed_runner:275) INFO: Client 41 has been set up ... 
2025-08-18 18:19:58 (federatedscope.llm.trainer.reward_choice_trainer:106) INFO: Choice token IDs: [362, 425]
2025-08-18 18:20:00 (federatedscope.core.fed_runner:275) INFO: Client 42 has been set up ... 
2025-08-18 18:20:01 (federatedscope.llm.trainer.reward_choice_trainer:106) INFO: Choice token IDs: [362, 425]
2025-08-18 18:20:03 (federatedscope.core.fed_runner:275) INFO: Client 43 has been set up ... 
2025-08-18 18:20:04 (federatedscope.llm.trainer.reward_choice_trainer:106) INFO: Choice token IDs: [362, 425]
2025-08-18 18:20:06 (federatedscope.core.fed_runner:275) INFO: Client 44 has been set up ... 
2025-08-18 18:20:06 (federatedscope.llm.trainer.reward_choice_trainer:106) INFO: Choice token IDs: [362, 425]
2025-08-18 18:20:09 (federatedscope.core.fed_runner:275) INFO: Client 45 has been set up ... 
2025-08-18 18:20:09 (federatedscope.llm.trainer.reward_choice_trainer:106) INFO: Choice token IDs: [362, 425]
2025-08-18 18:20:12 (federatedscope.core.fed_runner:275) INFO: Client 46 has been set up ... 
2025-08-18 18:20:12 (federatedscope.llm.trainer.reward_choice_trainer:106) INFO: Choice token IDs: [362, 425]
2025-08-18 18:20:15 (federatedscope.core.fed_runner:275) INFO: Client 47 has been set up ... 
2025-08-18 18:20:15 (federatedscope.llm.trainer.reward_choice_trainer:106) INFO: Choice token IDs: [362, 425]
2025-08-18 18:20:18 (federatedscope.core.fed_runner:275) INFO: Client 48 has been set up ... 
2025-08-18 18:20:18 (federatedscope.llm.trainer.reward_choice_trainer:106) INFO: Choice token IDs: [362, 425]
2025-08-18 18:20:21 (federatedscope.core.fed_runner:275) INFO: Client 49 has been set up ... 
2025-08-18 18:20:21 (federatedscope.llm.trainer.reward_choice_trainer:106) INFO: Choice token IDs: [362, 425]
2025-08-18 18:20:23 (federatedscope.core.fed_runner:275) INFO: Client 50 has been set up ... 
2025-08-18 18:20:24 (federatedscope.llm.trainer.reward_choice_trainer:106) INFO: Choice token IDs: [362, 425]
2025-08-18 18:20:26 (federatedscope.core.fed_runner:275) INFO: Client 51 has been set up ... 
2025-08-18 18:20:27 (federatedscope.llm.trainer.reward_choice_trainer:106) INFO: Choice token IDs: [362, 425]
2025-08-18 18:20:29 (federatedscope.core.fed_runner:275) INFO: Client 52 has been set up ... 
2025-08-18 18:20:29 (federatedscope.llm.trainer.reward_choice_trainer:106) INFO: Choice token IDs: [362, 425]
2025-08-18 18:20:32 (federatedscope.core.fed_runner:275) INFO: Client 53 has been set up ... 
2025-08-18 18:20:32 (federatedscope.core.trainers.trainer:502) INFO: Model meta-info: <class 'federatedscope.llm.model.adapter_builder.AdapterModel'>.
2025-08-18 18:20:32 (federatedscope.core.trainers.trainer:517) INFO: Num of original para names: 336.
2025-08-18 18:20:32 (federatedscope.core.trainers.trainer:518) INFO: Num of original trainable para names: 626.
2025-08-18 18:20:32 (federatedscope.core.trainers.trainer:520) INFO: Num of preserved para names in local update: 336. 
Preserved para names in local update: {'base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight'}.
2025-08-18 18:20:32 (federatedscope.core.trainers.trainer:524) INFO: Num of filtered para names in local update: 0. 
Filtered para names in local update: set().
2025-08-18 18:20:32 (federatedscope.core.trainers.trainer:532) INFO: After register default hooks,
	the hooks_in_train is:
	{
	  "on_fit_start": [
	    "_hook_on_fit_start_numerical_precision",
	    "_hook_on_data_parallel_init",
	    "_hook_on_fit_start_init",
	    "_hook_on_fit_start_calculate_model_size"
	  ],
	  "on_epoch_start": [
	    "_hook_on_epoch_start"
	  ],
	  "on_batch_start": [
	    "_hook_on_batch_start_init"
	  ],
	  "on_batch_forward": [
	    "_hook_on_batch_forward",
	    "_hook_on_batch_forward_regularizer",
	    "_hook_on_batch_forward_flop_count"
	  ],
	  "on_batch_backward": [
	    "_hook_on_batch_backward"
	  ],
	  "on_batch_end": [
	    "_hook_on_batch_end"
	  ],
	  "on_fit_end": [
	    "_hook_on_fit_end",
	    "_hook_on_fit_end_free_space"
	  ]
	};
	the hooks_in_eval is:
            t{
	  "on_fit_start": [
	    "_hook_on_fit_start_numerical_precision",
	    "_hook_on_data_parallel_init",
	    "_hook_on_fit_start_init"
	  ],
	  "on_epoch_start": [
	    "_hook_on_epoch_start"
	  ],
	  "on_batch_start": [
	    "_hook_on_batch_start_init"
	  ],
	  "on_batch_forward": [
	    "_hook_on_batch_forward"
	  ],
	  "on_batch_end": [
	    "_hook_on_batch_end"
	  ],
	  "on_fit_end": [
	    "_hook_on_fit_end",
	    "_hook_on_fit_end_free_space"
	  ]
	}
2025-08-18 18:20:34 (federatedscope.core.workers.server:1109) INFO: ----------- Starting training (Round #0) -------------
2025-08-18 18:20:34 (federatedscope.llm.trainer.reward_choice_trainer:244) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-08-18 18:20:35 (federatedscope.llm.trainer.reward_choice_trainer:217) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1979, total=7916)
2025-08-18 18:20:35 (federatedscope.llm.trainer.trainer:343) INFO: Re-creating Accelerator for the new round.
2025-08-18 18:20:36 (federatedscope.llm.trainer.trainer:371) INFO: Round #0 - Initializing with LR: 1e-05
2025-08-18 18:20:39 (federatedscope.llm.trainer.reward_choice_trainer:270) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-08-18 18:21:03 (federatedscope.core.trainers.trainer:379) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-08-18 18:21:03 (federatedscope.llm.trainer.reward_choice_trainer:493) INFO: [train|reduce] total=480, loss_sum=348.087341, avg_loss=0.725182, acc=0.493750 (local_seen=120, local_corr=67)
2025-08-18 18:21:03 (federatedscope.llm.trainer.trainer:634) INFO: Accelerator object has been deleted.
2025-08-18 18:21:08 (federatedscope.core.workers.client:318) INFO: {'Role': 'Client #44', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.58452844619751, 'train_avg_loss': 0.7048710703849792, 'train_acc': 0.0}}
2025-08-18 18:21:08 (federatedscope.core.workers.client:328) INFO: {'Role': 'Client #44', 'Round': 0, 'Split': 'train', 'Local': False, 'Results_procs': ['train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank']}
2025-08-18 18:21:08 (federatedscope.core.workers.client:340) INFO: {'Role': 'Client #44', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 348.08734130859375, 'train_avg_loss': 0.7251819610595703, 'train_acc': 0.49375}}
2025-08-18 18:21:08 (federatedscope.core.workers.client:610) INFO: {'Role': 'Client #44', 'Round': 0, 'Results_raw': {'train_total': 480, 'train_loss': 348.08734130859375, 'train_avg_loss': 0.7251819610595703, 'train_acc': 0.49375}}
2025-08-18 18:21:08 (federatedscope.llm.trainer.reward_choice_trainer:244) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-08-18 18:21:09 (federatedscope.llm.trainer.reward_choice_trainer:217) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=353, total=1409)
2025-08-18 18:21:09 (federatedscope.llm.trainer.trainer:343) INFO: Re-creating Accelerator for the new round.
2025-08-18 18:21:09 (federatedscope.llm.trainer.trainer:371) INFO: Round #0 - Initializing with LR: 1e-05
2025-08-18 18:21:09 (federatedscope.llm.trainer.reward_choice_trainer:270) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-08-18 18:21:33 (federatedscope.core.trainers.trainer:379) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-08-18 18:21:33 (federatedscope.llm.trainer.reward_choice_trainer:493) INFO: [train|reduce] total=480, loss_sum=343.310669, avg_loss=0.715231, acc=0.514583 (local_seen=120, local_corr=63)
2025-08-18 18:21:33 (federatedscope.llm.trainer.trainer:634) INFO: Accelerator object has been deleted.
2025-08-18 18:21:34 (federatedscope.core.workers.client:318) INFO: {'Role': 'Client #33', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 86.83678704500198, 'train_avg_loss': 0.7236398920416832, 'train_acc': 0.0}}
2025-08-18 18:21:34 (federatedscope.core.workers.client:328) INFO: {'Role': 'Client #33', 'Round': 0, 'Split': 'train', 'Local': False, 'Results_procs': ['train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank']}
2025-08-18 18:21:34 (federatedscope.core.workers.client:340) INFO: {'Role': 'Client #33', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 343.3106689453125, 'train_avg_loss': 0.7152305603027344, 'train_acc': 0.5145833333333333}}
2025-08-18 18:21:34 (federatedscope.core.workers.client:610) INFO: {'Role': 'Client #33', 'Round': 0, 'Results_raw': {'train_total': 480, 'train_loss': 343.3106689453125, 'train_avg_loss': 0.7152305603027344, 'train_acc': 0.5145833333333333}}
2025-08-18 18:21:34 (federatedscope.llm.trainer.reward_choice_trainer:244) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-08-18 18:21:34 (federatedscope.llm.trainer.reward_choice_trainer:217) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=399, total=1594)
2025-08-18 18:21:34 (federatedscope.llm.trainer.trainer:343) INFO: Re-creating Accelerator for the new round.
2025-08-18 18:21:34 (federatedscope.llm.trainer.trainer:371) INFO: Round #0 - Initializing with LR: 1e-05
2025-08-18 18:21:34 (federatedscope.llm.trainer.reward_choice_trainer:270) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-08-18 18:21:57 (federatedscope.core.trainers.trainer:379) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-08-18 18:21:57 (federatedscope.llm.trainer.reward_choice_trainer:493) INFO: [train|reduce] total=480, loss_sum=350.304993, avg_loss=0.729802, acc=0.487500 (local_seen=120, local_corr=57)
2025-08-18 18:21:57 (federatedscope.llm.trainer.trainer:634) INFO: Accelerator object has been deleted.
2025-08-18 18:21:59 (federatedscope.core.workers.client:318) INFO: {'Role': 'Client #39', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 87.43879652023315, 'train_avg_loss': 0.7286566376686097, 'train_acc': 0.0}}
2025-08-18 18:21:59 (federatedscope.core.workers.client:328) INFO: {'Role': 'Client #39', 'Round': 0, 'Split': 'train', 'Local': False, 'Results_procs': ['train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank']}
2025-08-18 18:21:59 (federatedscope.core.workers.client:340) INFO: {'Role': 'Client #39', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 350.30499267578125, 'train_avg_loss': 0.7298020680745443, 'train_acc': 0.4875}}
2025-08-18 18:21:59 (federatedscope.core.workers.client:610) INFO: {'Role': 'Client #39', 'Round': 0, 'Results_raw': {'train_total': 480, 'train_loss': 350.30499267578125, 'train_avg_loss': 0.7298020680745443, 'train_acc': 0.4875}}
2025-08-18 18:21:59 (federatedscope.llm.trainer.reward_choice_trainer:244) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-08-18 18:22:00 (federatedscope.llm.trainer.reward_choice_trainer:217) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1122, total=4486)
2025-08-18 18:22:00 (federatedscope.llm.trainer.trainer:343) INFO: Re-creating Accelerator for the new round.
2025-08-18 18:22:00 (federatedscope.llm.trainer.trainer:371) INFO: Round #0 - Initializing with LR: 1e-05
2025-08-18 18:22:00 (federatedscope.llm.trainer.reward_choice_trainer:270) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-08-18 18:22:22 (federatedscope.core.trainers.trainer:379) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-08-18 18:22:22 (federatedscope.llm.trainer.reward_choice_trainer:493) INFO: [train|reduce] total=480, loss_sum=345.365326, avg_loss=0.719511, acc=0.504167 (local_seen=120, local_corr=66)
2025-08-18 18:22:22 (federatedscope.llm.trainer.trainer:634) INFO: Accelerator object has been deleted.
2025-08-18 18:22:23 (federatedscope.core.workers.client:318) INFO: {'Role': 'Client #34', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 86.14047873020172, 'train_avg_loss': 0.717837322751681, 'train_acc': 0.0}}
2025-08-18 18:22:23 (federatedscope.core.workers.client:328) INFO: {'Role': 'Client #34', 'Round': 0, 'Split': 'train', 'Local': False, 'Results_procs': ['train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank']}
2025-08-18 18:22:23 (federatedscope.core.workers.client:340) INFO: {'Role': 'Client #34', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 345.3653259277344, 'train_avg_loss': 0.7195110956827799, 'train_acc': 0.5041666666666667}}
2025-08-18 18:22:23 (federatedscope.core.workers.client:610) INFO: {'Role': 'Client #34', 'Round': 0, 'Results_raw': {'train_total': 480, 'train_loss': 345.3653259277344, 'train_avg_loss': 0.7195110956827799, 'train_acc': 0.5041666666666667}}
2025-08-18 18:22:23 (federatedscope.llm.trainer.reward_choice_trainer:244) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-08-18 18:22:24 (federatedscope.llm.trainer.reward_choice_trainer:217) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=652, total=2605)
2025-08-18 18:22:24 (federatedscope.llm.trainer.trainer:343) INFO: Re-creating Accelerator for the new round.
2025-08-18 18:22:24 (federatedscope.llm.trainer.trainer:371) INFO: Round #0 - Initializing with LR: 1e-05
2025-08-18 18:22:24 (federatedscope.llm.trainer.reward_choice_trainer:270) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-08-18 18:22:47 (federatedscope.core.trainers.trainer:379) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-08-18 18:22:47 (federatedscope.llm.trainer.reward_choice_trainer:493) INFO: [train|reduce] total=480, loss_sum=343.218933, avg_loss=0.715039, acc=0.497917 (local_seen=120, local_corr=63)
2025-08-18 18:22:47 (federatedscope.llm.trainer.trainer:634) INFO: Accelerator object has been deleted.
2025-08-18 18:22:48 (federatedscope.core.workers.client:318) INFO: {'Role': 'Client #12', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 85.45588356256485, 'train_avg_loss': 0.7121323630213737, 'train_acc': 0.0}}
2025-08-18 18:22:48 (federatedscope.core.workers.client:328) INFO: {'Role': 'Client #12', 'Round': 0, 'Split': 'train', 'Local': False, 'Results_procs': ['train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank']}
2025-08-18 18:22:48 (federatedscope.core.workers.client:340) INFO: {'Role': 'Client #12', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 343.21893310546875, 'train_avg_loss': 0.7150394439697265, 'train_acc': 0.4979166666666667}}
2025-08-18 18:22:48 (federatedscope.core.workers.client:610) INFO: {'Role': 'Client #12', 'Round': 0, 'Results_raw': {'train_total': 480, 'train_loss': 343.21893310546875, 'train_avg_loss': 0.7150394439697265, 'train_acc': 0.4979166666666667}}
2025-08-18 18:22:48 (federatedscope.core.workers.server:554) INFO: [in-place aggregation ] round=0
2025-08-18 18:22:49 (federatedscope.core.workers.server:432) INFO: ----------- Starting a new training round (Round #1) -------------
2025-08-18 18:22:50 (federatedscope.llm.trainer.reward_choice_trainer:244) INFO: [Stateless LR Controller] In Round #1, planning to set LR to 1.00e-06
2025-08-18 18:22:50 (federatedscope.llm.trainer.reward_choice_trainer:217) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=586, total=2342)
2025-08-18 18:22:50 (federatedscope.llm.trainer.trainer:343) INFO: Re-creating Accelerator for the new round.
2025-08-18 18:22:50 (federatedscope.llm.trainer.trainer:371) INFO: Round #1 - Initializing with LR: 1e-05
2025-08-18 18:22:51 (federatedscope.llm.trainer.reward_choice_trainer:270) INFO: Successfully applied new LR 1.00e-06 to the optimizer.
2025-08-18 18:23:14 (federatedscope.core.trainers.trainer:379) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-08-18 18:23:14 (federatedscope.llm.trainer.reward_choice_trainer:493) INFO: [train|reduce] total=480, loss_sum=341.445160, avg_loss=0.711344, acc=0.497917 (local_seen=120, local_corr=57)
2025-08-18 18:23:14 (federatedscope.llm.trainer.trainer:634) INFO: Accelerator object has been deleted.
2025-08-18 18:23:15 (federatedscope.core.workers.client:318) INFO: {'Role': 'Client #27', 'Round': 1, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 86.96365797519684, 'train_avg_loss': 0.724697149793307, 'train_acc': 0.0}}
2025-08-18 18:23:15 (federatedscope.core.workers.client:328) INFO: {'Role': 'Client #27', 'Round': 1, 'Split': 'train', 'Local': False, 'Results_procs': ['train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank']}
2025-08-18 18:23:15 (federatedscope.core.workers.client:340) INFO: {'Role': 'Client #27', 'Round': 1, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 341.4451599121094, 'train_avg_loss': 0.7113440831502279, 'train_acc': 0.4979166666666667}}
2025-08-18 18:23:15 (federatedscope.core.workers.client:610) INFO: {'Role': 'Client #27', 'Round': 1, 'Results_raw': {'train_total': 480, 'train_loss': 341.4451599121094, 'train_avg_loss': 0.7113440831502279, 'train_acc': 0.4979166666666667}}
2025-08-18 18:23:15 (federatedscope.llm.trainer.reward_choice_trainer:244) INFO: [Stateless LR Controller] In Round #1, planning to set LR to 1.00e-06
2025-08-18 18:23:16 (federatedscope.llm.trainer.reward_choice_trainer:217) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1236, total=4944)
2025-08-18 18:23:16 (federatedscope.llm.trainer.trainer:343) INFO: Re-creating Accelerator for the new round.
2025-08-18 18:23:16 (federatedscope.llm.trainer.trainer:371) INFO: Round #1 - Initializing with LR: 1e-05
2025-08-18 18:23:16 (federatedscope.llm.trainer.reward_choice_trainer:270) INFO: Successfully applied new LR 1.00e-06 to the optimizer.
2025-08-18 18:23:38 (federatedscope.core.trainers.trainer:379) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-08-18 18:23:38 (federatedscope.llm.trainer.reward_choice_trainer:493) INFO: [train|reduce] total=480, loss_sum=343.190643, avg_loss=0.714981, acc=0.479167 (local_seen=120, local_corr=58)
2025-08-18 18:23:38 (federatedscope.llm.trainer.trainer:634) INFO: Accelerator object has been deleted.
2025-08-18 18:23:39 (federatedscope.core.workers.client:318) INFO: {'Role': 'Client #24', 'Round': 1, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 85.82735270261765, 'train_avg_loss': 0.7152279391884804, 'train_acc': 0.0}}
2025-08-18 18:23:39 (federatedscope.core.workers.client:328) INFO: {'Role': 'Client #24', 'Round': 1, 'Split': 'train', 'Local': False, 'Results_procs': ['train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank']}
2025-08-18 18:23:39 (federatedscope.core.workers.client:340) INFO: {'Role': 'Client #24', 'Round': 1, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 343.1906433105469, 'train_avg_loss': 0.7149805068969727, 'train_acc': 0.4791666666666667}}
2025-08-18 18:23:39 (federatedscope.core.workers.client:610) INFO: {'Role': 'Client #24', 'Round': 1, 'Results_raw': {'train_total': 480, 'train_loss': 343.1906433105469, 'train_avg_loss': 0.7149805068969727, 'train_acc': 0.4791666666666667}}
2025-08-18 18:23:40 (federatedscope.llm.trainer.reward_choice_trainer:244) INFO: [Stateless LR Controller] In Round #1, planning to set LR to 1.00e-06
2025-08-18 18:23:40 (federatedscope.llm.trainer.reward_choice_trainer:217) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=173, total=691)
2025-08-18 18:23:40 (federatedscope.llm.trainer.trainer:343) INFO: Re-creating Accelerator for the new round.
2025-08-18 18:23:40 (federatedscope.llm.trainer.trainer:371) INFO: Round #1 - Initializing with LR: 1e-05
2025-08-18 18:23:41 (federatedscope.llm.trainer.reward_choice_trainer:270) INFO: Successfully applied new LR 1.00e-06 to the optimizer.
2025-08-18 18:24:06 (federatedscope.core.trainers.trainer:379) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-08-18 18:24:06 (federatedscope.llm.trainer.reward_choice_trainer:493) INFO: [train|reduce] total=480, loss_sum=338.019714, avg_loss=0.704208, acc=0.508333 (local_seen=120, local_corr=64)
2025-08-18 18:24:06 (federatedscope.llm.trainer.trainer:634) INFO: Accelerator object has been deleted.
2025-08-18 18:24:08 (federatedscope.core.workers.client:318) INFO: {'Role': 'Client #3', 'Round': 1, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.89417672157288, 'train_avg_loss': 0.6907848060131073, 'train_acc': 0.0}}
2025-08-18 18:24:08 (federatedscope.core.workers.client:328) INFO: {'Role': 'Client #3', 'Round': 1, 'Split': 'train', 'Local': False, 'Results_procs': ['train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank']}
2025-08-18 18:24:08 (federatedscope.core.workers.client:340) INFO: {'Role': 'Client #3', 'Round': 1, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 338.01971435546875, 'train_avg_loss': 0.7042077382405599, 'train_acc': 0.5083333333333333}}
2025-08-18 18:24:08 (federatedscope.core.workers.client:610) INFO: {'Role': 'Client #3', 'Round': 1, 'Results_raw': {'train_total': 480, 'train_loss': 338.01971435546875, 'train_avg_loss': 0.7042077382405599, 'train_acc': 0.5083333333333333}}
2025-08-18 18:24:08 (federatedscope.llm.trainer.reward_choice_trainer:244) INFO: [Stateless LR Controller] In Round #1, planning to set LR to 1.00e-06
2025-08-18 18:24:09 (federatedscope.llm.trainer.reward_choice_trainer:217) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1698, total=6791)
2025-08-18 18:24:09 (federatedscope.llm.trainer.trainer:343) INFO: Re-creating Accelerator for the new round.
2025-08-18 18:24:09 (federatedscope.llm.trainer.trainer:371) INFO: Round #1 - Initializing with LR: 1e-05
2025-08-18 18:24:09 (federatedscope.llm.trainer.reward_choice_trainer:270) INFO: Successfully applied new LR 1.00e-06 to the optimizer.
2025-08-18 18:24:33 (federatedscope.core.trainers.trainer:379) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-08-18 18:24:33 (federatedscope.llm.trainer.reward_choice_trainer:493) INFO: [train|reduce] total=480, loss_sum=337.310730, avg_loss=0.702731, acc=0.537500 (local_seen=120, local_corr=61)
2025-08-18 18:24:33 (federatedscope.llm.trainer.trainer:634) INFO: Accelerator object has been deleted.
2025-08-18 18:24:34 (federatedscope.core.workers.client:318) INFO: {'Role': 'Client #53', 'Round': 1, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 85.59641736745834, 'train_avg_loss': 0.7133034780621529, 'train_acc': 0.0}}
2025-08-18 18:24:34 (federatedscope.core.workers.client:328) INFO: {'Role': 'Client #53', 'Round': 1, 'Split': 'train', 'Local': False, 'Results_procs': ['train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank']}
2025-08-18 18:24:34 (federatedscope.core.workers.client:340) INFO: {'Role': 'Client #53', 'Round': 1, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 337.31072998046875, 'train_avg_loss': 0.7027306874593099, 'train_acc': 0.5375}}
2025-08-18 18:24:34 (federatedscope.core.workers.client:610) INFO: {'Role': 'Client #53', 'Round': 1, 'Results_raw': {'train_total': 480, 'train_loss': 337.31072998046875, 'train_avg_loss': 0.7027306874593099, 'train_acc': 0.5375}}
2025-08-18 18:24:34 (federatedscope.llm.trainer.reward_choice_trainer:244) INFO: [Stateless LR Controller] In Round #1, planning to set LR to 1.00e-06
2025-08-18 18:24:34 (federatedscope.llm.trainer.reward_choice_trainer:217) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=600, total=2399)
2025-08-18 18:24:34 (federatedscope.llm.trainer.trainer:343) INFO: Re-creating Accelerator for the new round.
2025-08-18 18:24:34 (federatedscope.llm.trainer.trainer:371) INFO: Round #1 - Initializing with LR: 1e-05
2025-08-18 18:24:35 (federatedscope.llm.trainer.reward_choice_trainer:270) INFO: Successfully applied new LR 1.00e-06 to the optimizer.
2025-08-18 18:24:58 (federatedscope.core.trainers.trainer:379) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-08-18 18:24:58 (federatedscope.llm.trainer.reward_choice_trainer:493) INFO: [train|reduce] total=480, loss_sum=334.828644, avg_loss=0.697560, acc=0.556250 (local_seen=120, local_corr=64)
2025-08-18 18:24:58 (federatedscope.llm.trainer.trainer:634) INFO: Accelerator object has been deleted.
2025-08-18 18:25:00 (federatedscope.core.workers.client:318) INFO: {'Role': 'Client #20', 'Round': 1, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.98421436548233, 'train_avg_loss': 0.7082017863790194, 'train_acc': 0.0}}
2025-08-18 18:25:00 (federatedscope.core.workers.client:328) INFO: {'Role': 'Client #20', 'Round': 1, 'Split': 'train', 'Local': False, 'Results_procs': ['train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank']}
2025-08-18 18:25:00 (federatedscope.core.workers.client:340) INFO: {'Role': 'Client #20', 'Round': 1, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 334.8286437988281, 'train_avg_loss': 0.697559674580892, 'train_acc': 0.55625}}
2025-08-18 18:25:00 (federatedscope.core.workers.client:610) INFO: {'Role': 'Client #20', 'Round': 1, 'Results_raw': {'train_total': 480, 'train_loss': 334.8286437988281, 'train_avg_loss': 0.697559674580892, 'train_acc': 0.55625}}
2025-08-18 18:25:00 (federatedscope.core.workers.server:554) INFO: [in-place aggregation ] round=1
2025-08-18 18:25:01 (federatedscope.core.workers.server:432) INFO: ----------- Starting a new training round (Round #2) -------------
2025-08-18 18:25:02 (federatedscope.llm.trainer.reward_choice_trainer:244) INFO: [Stateless LR Controller] In Round #2, planning to set LR to 1.00e-07
2025-08-18 18:25:02 (federatedscope.llm.trainer.reward_choice_trainer:217) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=766, total=3063)
2025-08-18 18:25:02 (federatedscope.llm.trainer.trainer:343) INFO: Re-creating Accelerator for the new round.
2025-08-18 18:25:02 (federatedscope.llm.trainer.trainer:371) INFO: Round #2 - Initializing with LR: 1e-05
2025-08-18 18:25:02 (federatedscope.llm.trainer.reward_choice_trainer:270) INFO: Successfully applied new LR 1.00e-07 to the optimizer.
2025-08-18 18:25:26 (federatedscope.core.trainers.trainer:379) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-08-18 18:25:26 (federatedscope.llm.trainer.reward_choice_trainer:493) INFO: [train|reduce] total=480, loss_sum=341.337830, avg_loss=0.711120, acc=0.514583 (local_seen=120, local_corr=70)
2025-08-18 18:25:26 (federatedscope.llm.trainer.trainer:634) INFO: Accelerator object has been deleted.
2025-08-18 18:25:28 (federatedscope.core.workers.client:318) INFO: {'Role': 'Client #26', 'Round': 2, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.25256204605103, 'train_avg_loss': 0.7021046837170919, 'train_acc': 0.0}}
2025-08-18 18:25:28 (federatedscope.core.workers.client:328) INFO: {'Role': 'Client #26', 'Round': 2, 'Split': 'train', 'Local': False, 'Results_procs': ['train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank']}
2025-08-18 18:25:28 (federatedscope.core.workers.client:340) INFO: {'Role': 'Client #26', 'Round': 2, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 341.33782958984375, 'train_avg_loss': 0.7111204783121745, 'train_acc': 0.5145833333333333}}
2025-08-18 18:25:28 (federatedscope.core.workers.client:610) INFO: {'Role': 'Client #26', 'Round': 2, 'Results_raw': {'train_total': 480, 'train_loss': 341.33782958984375, 'train_avg_loss': 0.7111204783121745, 'train_acc': 0.5145833333333333}}
2025-08-18 18:25:28 (federatedscope.llm.trainer.reward_choice_trainer:244) INFO: [Stateless LR Controller] In Round #2, planning to set LR to 1.00e-07
2025-08-18 18:25:28 (federatedscope.llm.trainer.reward_choice_trainer:217) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=637, total=2547)
2025-08-18 18:25:28 (federatedscope.llm.trainer.trainer:343) INFO: Re-creating Accelerator for the new round.
2025-08-18 18:25:28 (federatedscope.llm.trainer.trainer:371) INFO: Round #2 - Initializing with LR: 1e-05
2025-08-18 18:25:29 (federatedscope.llm.trainer.reward_choice_trainer:270) INFO: Successfully applied new LR 1.00e-07 to the optimizer.
2025-08-18 18:25:51 (federatedscope.core.trainers.trainer:379) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-08-18 18:25:51 (federatedscope.llm.trainer.reward_choice_trainer:493) INFO: [train|reduce] total=480, loss_sum=340.699768, avg_loss=0.709791, acc=0.489583 (local_seen=120, local_corr=56)
2025-08-18 18:25:51 (federatedscope.llm.trainer.trainer:634) INFO: Accelerator object has been deleted.
2025-08-18 18:25:53 (federatedscope.core.workers.client:318) INFO: {'Role': 'Client #6', 'Round': 2, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 85.53135573863983, 'train_avg_loss': 0.7127612978219986, 'train_acc': 0.0}}
2025-08-18 18:25:53 (federatedscope.core.workers.client:328) INFO: {'Role': 'Client #6', 'Round': 2, 'Split': 'train', 'Local': False, 'Results_procs': ['train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank']}
2025-08-18 18:25:53 (federatedscope.core.workers.client:340) INFO: {'Role': 'Client #6', 'Round': 2, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 340.69976806640625, 'train_avg_loss': 0.7097911834716797, 'train_acc': 0.4895833333333333}}
2025-08-18 18:25:53 (federatedscope.core.workers.client:610) INFO: {'Role': 'Client #6', 'Round': 2, 'Results_raw': {'train_total': 480, 'train_loss': 340.69976806640625, 'train_avg_loss': 0.7097911834716797, 'train_acc': 0.4895833333333333}}
2025-08-18 18:25:53 (federatedscope.llm.trainer.reward_choice_trainer:244) INFO: [Stateless LR Controller] In Round #2, planning to set LR to 1.00e-07
2025-08-18 18:25:53 (federatedscope.llm.trainer.reward_choice_trainer:217) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=536, total=2144)
2025-08-18 18:25:53 (federatedscope.llm.trainer.trainer:343) INFO: Re-creating Accelerator for the new round.
2025-08-18 18:25:53 (federatedscope.llm.trainer.trainer:371) INFO: Round #2 - Initializing with LR: 1e-05
2025-08-18 18:25:54 (federatedscope.llm.trainer.reward_choice_trainer:270) INFO: Successfully applied new LR 1.00e-07 to the optimizer.
2025-08-18 18:26:17 (federatedscope.core.trainers.trainer:379) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-08-18 18:26:17 (federatedscope.llm.trainer.reward_choice_trainer:493) INFO: [train|reduce] total=480, loss_sum=339.178375, avg_loss=0.706622, acc=0.491667 (local_seen=120, local_corr=58)
2025-08-18 18:26:17 (federatedscope.llm.trainer.trainer:634) INFO: Accelerator object has been deleted.
2025-08-18 18:26:18 (federatedscope.core.workers.client:318) INFO: {'Role': 'Client #32', 'Round': 2, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.12517821788788, 'train_avg_loss': 0.692709818482399, 'train_acc': 0.0}}
2025-08-18 18:26:18 (federatedscope.core.workers.client:328) INFO: {'Role': 'Client #32', 'Round': 2, 'Split': 'train', 'Local': False, 'Results_procs': ['train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank']}
2025-08-18 18:26:18 (federatedscope.core.workers.client:340) INFO: {'Role': 'Client #32', 'Round': 2, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 339.1783752441406, 'train_avg_loss': 0.7066216150919596, 'train_acc': 0.49166666666666664}}
2025-08-18 18:26:18 (federatedscope.core.workers.client:610) INFO: {'Role': 'Client #32', 'Round': 2, 'Results_raw': {'train_total': 480, 'train_loss': 339.1783752441406, 'train_avg_loss': 0.7066216150919596, 'train_acc': 0.49166666666666664}}
2025-08-18 18:26:18 (federatedscope.llm.trainer.reward_choice_trainer:244) INFO: [Stateless LR Controller] In Round #2, planning to set LR to 1.00e-07
2025-08-18 18:26:19 (federatedscope.llm.trainer.reward_choice_trainer:217) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1069, total=4273)
2025-08-18 18:26:19 (federatedscope.llm.trainer.trainer:343) INFO: Re-creating Accelerator for the new round.
2025-08-18 18:26:19 (federatedscope.llm.trainer.trainer:371) INFO: Round #2 - Initializing with LR: 1e-05
2025-08-18 18:26:20 (federatedscope.llm.trainer.reward_choice_trainer:270) INFO: Successfully applied new LR 1.00e-07 to the optimizer.
2025-08-18 18:26:43 (federatedscope.core.trainers.trainer:379) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-08-18 18:26:43 (federatedscope.llm.trainer.reward_choice_trainer:493) INFO: [train|reduce] total=480, loss_sum=342.532806, avg_loss=0.713610, acc=0.452083 (local_seen=120, local_corr=50)
2025-08-18 18:26:43 (federatedscope.llm.trainer.trainer:634) INFO: Accelerator object has been deleted.
2025-08-18 18:26:44 (federatedscope.core.workers.client:318) INFO: {'Role': 'Client #37', 'Round': 2, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 88.64026010036469, 'train_avg_loss': 0.7386688341697057, 'train_acc': 0.0}}
2025-08-18 18:26:44 (federatedscope.core.workers.client:328) INFO: {'Role': 'Client #37', 'Round': 2, 'Split': 'train', 'Local': False, 'Results_procs': ['train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank']}
2025-08-18 18:26:44 (federatedscope.core.workers.client:340) INFO: {'Role': 'Client #37', 'Round': 2, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 342.5328063964844, 'train_avg_loss': 0.7136100133260092, 'train_acc': 0.45208333333333334}}
2025-08-18 18:26:44 (federatedscope.core.workers.client:610) INFO: {'Role': 'Client #37', 'Round': 2, 'Results_raw': {'train_total': 480, 'train_loss': 342.5328063964844, 'train_avg_loss': 0.7136100133260092, 'train_acc': 0.45208333333333334}}
2025-08-18 18:26:44 (federatedscope.llm.trainer.reward_choice_trainer:244) INFO: [Stateless LR Controller] In Round #2, planning to set LR to 1.00e-07
2025-08-18 18:26:44 (federatedscope.llm.trainer.reward_choice_trainer:217) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=329, total=1316)
2025-08-18 18:26:44 (federatedscope.llm.trainer.trainer:343) INFO: Re-creating Accelerator for the new round.
2025-08-18 18:26:44 (federatedscope.llm.trainer.trainer:371) INFO: Round #2 - Initializing with LR: 1e-05
2025-08-18 18:26:45 (federatedscope.llm.trainer.reward_choice_trainer:270) INFO: Successfully applied new LR 1.00e-07 to the optimizer.
2025-08-18 18:27:09 (federatedscope.core.trainers.trainer:379) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-08-18 18:27:09 (federatedscope.llm.trainer.reward_choice_trainer:493) INFO: [train|reduce] total=480, loss_sum=342.881287, avg_loss=0.714336, acc=0.475000 (local_seen=120, local_corr=57)
2025-08-18 18:27:09 (federatedscope.llm.trainer.trainer:634) INFO: Accelerator object has been deleted.
2025-08-18 18:27:11 (federatedscope.core.workers.client:318) INFO: {'Role': 'Client #8', 'Round': 2, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 86.30253279209137, 'train_avg_loss': 0.7191877732674281, 'train_acc': 0.0}}
2025-08-18 18:27:11 (federatedscope.core.workers.client:328) INFO: {'Role': 'Client #8', 'Round': 2, 'Split': 'train', 'Local': False, 'Results_procs': ['train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank']}
2025-08-18 18:27:11 (federatedscope.core.workers.client:340) INFO: {'Role': 'Client #8', 'Round': 2, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 342.88128662109375, 'train_avg_loss': 0.7143360137939453, 'train_acc': 0.475}}
2025-08-18 18:27:11 (federatedscope.core.workers.client:610) INFO: {'Role': 'Client #8', 'Round': 2, 'Results_raw': {'train_total': 480, 'train_loss': 342.88128662109375, 'train_avg_loss': 0.7143360137939453, 'train_acc': 0.475}}
2025-08-18 18:27:11 (federatedscope.core.workers.server:554) INFO: [in-place aggregation ] round=2
2025-08-18 18:27:12 (federatedscope.core.workers.server:432) INFO: ----------- Starting a new training round (Round #3) -------------
2025-08-18 18:27:13 (federatedscope.core.trainers.torch_trainer:140) INFO: DDP environment detected. Adding 'module.' prefix to incoming server keys.
2025-08-18 18:27:13 (federatedscope.llm.trainer.reward_choice_trainer:244) INFO: [Stateless LR Controller] In Round #3, planning to set LR to 1.00e-08
2025-08-18 18:27:13 (federatedscope.llm.trainer.reward_choice_trainer:217) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=637, total=2547)
2025-08-18 18:27:13 (federatedscope.llm.trainer.trainer:343) INFO: Re-creating Accelerator for the new round.
2025-08-18 18:27:13 (federatedscope.llm.trainer.trainer:371) INFO: Round #3 - Initializing with LR: 1e-05
2025-08-18 18:27:13 (federatedscope.llm.trainer.reward_choice_trainer:270) INFO: Successfully applied new LR 1.00e-08 to the optimizer.
2025-08-18 18:27:37 (federatedscope.core.trainers.trainer:379) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-08-18 18:27:37 (federatedscope.llm.trainer.reward_choice_trainer:493) INFO: [train|reduce] total=480, loss_sum=340.585938, avg_loss=0.709554, acc=0.479167 (local_seen=120, local_corr=54)
2025-08-18 18:27:37 (federatedscope.llm.trainer.trainer:634) INFO: Accelerator object has been deleted.
2025-08-18 18:27:38 (federatedscope.core.workers.client:318) INFO: {'Role': 'Client #6', 'Round': 3, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 85.36072784662247, 'train_avg_loss': 0.7113393987218539, 'train_acc': 0.0}}
2025-08-18 18:27:38 (federatedscope.core.workers.client:328) INFO: {'Role': 'Client #6', 'Round': 3, 'Split': 'train', 'Local': False, 'Results_procs': ['train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank']}
2025-08-18 18:27:38 (federatedscope.core.workers.client:340) INFO: {'Role': 'Client #6', 'Round': 3, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 340.5859375, 'train_avg_loss': 0.7095540364583334, 'train_acc': 0.4791666666666667}}
2025-08-18 18:27:38 (federatedscope.core.workers.client:610) INFO: {'Role': 'Client #6', 'Round': 3, 'Results_raw': {'train_total': 480, 'train_loss': 340.5859375, 'train_avg_loss': 0.7095540364583334, 'train_acc': 0.4791666666666667}}
2025-08-18 18:27:38 (federatedscope.llm.trainer.reward_choice_trainer:244) INFO: [Stateless LR Controller] In Round #3, planning to set LR to 1.00e-08
2025-08-18 18:27:39 (federatedscope.llm.trainer.reward_choice_trainer:217) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=525, total=2100)
2025-08-18 18:27:39 (federatedscope.llm.trainer.trainer:343) INFO: Re-creating Accelerator for the new round.
2025-08-18 18:27:39 (federatedscope.llm.trainer.trainer:371) INFO: Round #3 - Initializing with LR: 1e-05
2025-08-18 18:27:39 (federatedscope.llm.trainer.reward_choice_trainer:270) INFO: Successfully applied new LR 1.00e-08 to the optimizer.
2025-08-18 18:28:03 (federatedscope.core.trainers.trainer:379) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-08-18 18:28:03 (federatedscope.llm.trainer.reward_choice_trainer:493) INFO: [train|reduce] total=480, loss_sum=335.507904, avg_loss=0.698975, acc=0.508333 (local_seen=120, local_corr=54)
2025-08-18 18:28:03 (federatedscope.llm.trainer.trainer:634) INFO: Accelerator object has been deleted.
2025-08-18 18:28:04 (federatedscope.core.workers.client:318) INFO: {'Role': 'Client #46', 'Round': 3, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 86.5910621881485, 'train_avg_loss': 0.7215921849012374, 'train_acc': 0.0}}
2025-08-18 18:28:04 (federatedscope.core.workers.client:328) INFO: {'Role': 'Client #46', 'Round': 3, 'Split': 'train', 'Local': False, 'Results_procs': ['train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank']}
2025-08-18 18:28:04 (federatedscope.core.workers.client:340) INFO: {'Role': 'Client #46', 'Round': 3, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 335.5079040527344, 'train_avg_loss': 0.6989748001098632, 'train_acc': 0.5083333333333333}}
2025-08-18 18:28:04 (federatedscope.core.workers.client:610) INFO: {'Role': 'Client #46', 'Round': 3, 'Results_raw': {'train_total': 480, 'train_loss': 335.5079040527344, 'train_avg_loss': 0.6989748001098632, 'train_acc': 0.5083333333333333}}
2025-08-18 18:28:04 (federatedscope.llm.trainer.reward_choice_trainer:244) INFO: [Stateless LR Controller] In Round #3, planning to set LR to 1.00e-08
2025-08-18 18:28:05 (federatedscope.llm.trainer.reward_choice_trainer:217) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=920, total=3679)
2025-08-18 18:28:05 (federatedscope.llm.trainer.trainer:343) INFO: Re-creating Accelerator for the new round.
2025-08-18 18:28:05 (federatedscope.llm.trainer.trainer:371) INFO: Round #3 - Initializing with LR: 1e-05
2025-08-18 18:28:05 (federatedscope.llm.trainer.reward_choice_trainer:270) INFO: Successfully applied new LR 1.00e-08 to the optimizer.
2025-08-18 18:28:29 (federatedscope.core.trainers.trainer:379) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-08-18 18:28:29 (federatedscope.llm.trainer.reward_choice_trainer:493) INFO: [train|reduce] total=480, loss_sum=340.258728, avg_loss=0.708872, acc=0.487500 (local_seen=120, local_corr=57)
2025-08-18 18:28:29 (federatedscope.llm.trainer.trainer:634) INFO: Accelerator object has been deleted.
2025-08-18 18:28:30 (federatedscope.core.workers.client:318) INFO: {'Role': 'Client #31', 'Round': 3, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.33293747901917, 'train_avg_loss': 0.694441145658493, 'train_acc': 0.0}}
2025-08-18 18:28:30 (federatedscope.core.workers.client:328) INFO: {'Role': 'Client #31', 'Round': 3, 'Split': 'train', 'Local': False, 'Results_procs': ['train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank']}
2025-08-18 18:28:30 (federatedscope.core.workers.client:340) INFO: {'Role': 'Client #31', 'Round': 3, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 340.25872802734375, 'train_avg_loss': 0.7088723500569661, 'train_acc': 0.4875}}
2025-08-18 18:28:30 (federatedscope.core.workers.client:610) INFO: {'Role': 'Client #31', 'Round': 3, 'Results_raw': {'train_total': 480, 'train_loss': 340.25872802734375, 'train_avg_loss': 0.7088723500569661, 'train_acc': 0.4875}}
2025-08-18 18:28:30 (federatedscope.core.trainers.torch_trainer:140) INFO: DDP environment detected. Adding 'module.' prefix to incoming server keys.
2025-08-18 18:28:30 (federatedscope.llm.trainer.reward_choice_trainer:244) INFO: [Stateless LR Controller] In Round #3, planning to set LR to 1.00e-08
2025-08-18 18:28:30 (federatedscope.llm.trainer.reward_choice_trainer:217) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1236, total=4944)
2025-08-18 18:28:30 (federatedscope.llm.trainer.trainer:343) INFO: Re-creating Accelerator for the new round.
2025-08-18 18:28:30 (federatedscope.llm.trainer.trainer:371) INFO: Round #3 - Initializing with LR: 1e-05
2025-08-18 18:28:31 (federatedscope.llm.trainer.reward_choice_trainer:270) INFO: Successfully applied new LR 1.00e-08 to the optimizer.
2025-08-18 18:28:55 (federatedscope.core.trainers.trainer:379) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-08-18 18:28:55 (federatedscope.llm.trainer.reward_choice_trainer:493) INFO: [train|reduce] total=480, loss_sum=343.506317, avg_loss=0.715638, acc=0.470833 (local_seen=120, local_corr=57)
2025-08-18 18:28:55 (federatedscope.llm.trainer.trainer:634) INFO: Accelerator object has been deleted.
2025-08-18 18:28:56 (federatedscope.core.workers.client:318) INFO: {'Role': 'Client #24', 'Round': 3, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 85.74245446920395, 'train_avg_loss': 0.7145204539100329, 'train_acc': 0.0}}
2025-08-18 18:28:56 (federatedscope.core.workers.client:328) INFO: {'Role': 'Client #24', 'Round': 3, 'Split': 'train', 'Local': False, 'Results_procs': ['train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank']}
2025-08-18 18:28:56 (federatedscope.core.workers.client:340) INFO: {'Role': 'Client #24', 'Round': 3, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 343.5063171386719, 'train_avg_loss': 0.7156381607055664, 'train_acc': 0.4708333333333333}}
2025-08-18 18:28:56 (federatedscope.core.workers.client:610) INFO: {'Role': 'Client #24', 'Round': 3, 'Results_raw': {'train_total': 480, 'train_loss': 343.5063171386719, 'train_avg_loss': 0.7156381607055664, 'train_acc': 0.4708333333333333}}
2025-08-18 18:28:56 (federatedscope.llm.trainer.reward_choice_trainer:244) INFO: [Stateless LR Controller] In Round #3, planning to set LR to 1.00e-08
2025-08-18 18:28:56 (federatedscope.llm.trainer.reward_choice_trainer:217) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=156, total=621)
2025-08-18 18:28:56 (federatedscope.llm.trainer.trainer:343) INFO: Re-creating Accelerator for the new round.
2025-08-18 18:28:56 (federatedscope.llm.trainer.trainer:371) INFO: Round #3 - Initializing with LR: 1e-05
2025-08-18 18:28:57 (federatedscope.llm.trainer.reward_choice_trainer:270) INFO: Successfully applied new LR 1.00e-08 to the optimizer.
2025-08-18 18:29:21 (federatedscope.core.trainers.trainer:379) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-08-18 18:29:21 (federatedscope.llm.trainer.reward_choice_trainer:493) INFO: [train|reduce] total=480, loss_sum=335.531433, avg_loss=0.699024, acc=0.522917 (local_seen=120, local_corr=66)
2025-08-18 18:29:21 (federatedscope.llm.trainer.trainer:634) INFO: Accelerator object has been deleted.
2025-08-18 18:29:23 (federatedscope.core.workers.client:318) INFO: {'Role': 'Client #11', 'Round': 3, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.48091673851013, 'train_avg_loss': 0.6873409728209178, 'train_acc': 0.0}}
2025-08-18 18:29:23 (federatedscope.core.workers.client:328) INFO: {'Role': 'Client #11', 'Round': 3, 'Split': 'train', 'Local': False, 'Results_procs': ['train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank']}
2025-08-18 18:29:23 (federatedscope.core.workers.client:340) INFO: {'Role': 'Client #11', 'Round': 3, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 335.53143310546875, 'train_avg_loss': 0.6990238189697265, 'train_acc': 0.5229166666666667}}
2025-08-18 18:29:23 (federatedscope.core.workers.client:610) INFO: {'Role': 'Client #11', 'Round': 3, 'Results_raw': {'train_total': 480, 'train_loss': 335.53143310546875, 'train_avg_loss': 0.6990238189697265, 'train_acc': 0.5229166666666667}}
2025-08-18 18:29:23 (federatedscope.core.workers.server:554) INFO: [in-place aggregation ] round=3
2025-08-18 18:29:23 (federatedscope.core.workers.server:432) INFO: ----------- Starting a new training round (Round #4) -------------
2025-08-18 18:29:24 (federatedscope.llm.trainer.reward_choice_trainer:244) INFO: [Stateless LR Controller] In Round #4, planning to set LR to 1.00e-09
2025-08-18 18:29:25 (federatedscope.llm.trainer.reward_choice_trainer:217) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1471, total=5883)
2025-08-18 18:29:25 (federatedscope.llm.trainer.trainer:343) INFO: Re-creating Accelerator for the new round.
2025-08-18 18:29:25 (federatedscope.llm.trainer.trainer:371) INFO: Round #4 - Initializing with LR: 1e-05
2025-08-18 18:29:25 (federatedscope.llm.trainer.reward_choice_trainer:270) INFO: Successfully applied new LR 1.00e-09 to the optimizer.
2025-08-18 18:29:47 (federatedscope.core.trainers.trainer:379) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-08-18 18:29:47 (federatedscope.llm.trainer.reward_choice_trainer:493) INFO: [train|reduce] total=480, loss_sum=338.601440, avg_loss=0.705420, acc=0.502083 (local_seen=120, local_corr=60)
2025-08-18 18:29:47 (federatedscope.llm.trainer.trainer:634) INFO: Accelerator object has been deleted.
2025-08-18 18:29:48 (federatedscope.core.workers.client:318) INFO: {'Role': 'Client #17', 'Round': 4, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 85.50106155872345, 'train_avg_loss': 0.7125088463226954, 'train_acc': 0.0}}
2025-08-18 18:29:48 (federatedscope.core.workers.client:328) INFO: {'Role': 'Client #17', 'Round': 4, 'Split': 'train', 'Local': False, 'Results_procs': ['train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank']}
2025-08-18 18:29:48 (federatedscope.core.workers.client:340) INFO: {'Role': 'Client #17', 'Round': 4, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 338.6014404296875, 'train_avg_loss': 0.7054196675618489, 'train_acc': 0.5020833333333333}}
2025-08-18 18:29:48 (federatedscope.core.workers.client:610) INFO: {'Role': 'Client #17', 'Round': 4, 'Results_raw': {'train_total': 480, 'train_loss': 338.6014404296875, 'train_avg_loss': 0.7054196675618489, 'train_acc': 0.5020833333333333}}
2025-08-18 18:29:48 (federatedscope.llm.trainer.reward_choice_trainer:244) INFO: [Stateless LR Controller] In Round #4, planning to set LR to 1.00e-09
2025-08-18 18:29:49 (federatedscope.llm.trainer.reward_choice_trainer:217) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1184, total=4736)
2025-08-18 18:29:49 (federatedscope.llm.trainer.trainer:343) INFO: Re-creating Accelerator for the new round.
2025-08-18 18:29:49 (federatedscope.llm.trainer.trainer:371) INFO: Round #4 - Initializing with LR: 1e-05
2025-08-18 18:29:49 (federatedscope.llm.trainer.reward_choice_trainer:270) INFO: Successfully applied new LR 1.00e-09 to the optimizer.
2025-08-18 18:30:12 (federatedscope.core.trainers.trainer:379) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-08-18 18:30:12 (federatedscope.llm.trainer.reward_choice_trainer:493) INFO: [train|reduce] total=480, loss_sum=342.155151, avg_loss=0.712823, acc=0.485417 (local_seen=120, local_corr=61)
2025-08-18 18:30:12 (federatedscope.llm.trainer.trainer:634) INFO: Accelerator object has been deleted.
2025-08-18 18:30:14 (federatedscope.core.workers.client:318) INFO: {'Role': 'Client #35', 'Round': 4, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.72972738742828, 'train_avg_loss': 0.7060810615619023, 'train_acc': 0.0}}
2025-08-18 18:30:14 (federatedscope.core.workers.client:328) INFO: {'Role': 'Client #35', 'Round': 4, 'Split': 'train', 'Local': False, 'Results_procs': ['train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank']}
2025-08-18 18:30:14 (federatedscope.core.workers.client:340) INFO: {'Role': 'Client #35', 'Round': 4, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 342.1551513671875, 'train_avg_loss': 0.712823232014974, 'train_acc': 0.48541666666666666}}
2025-08-18 18:30:14 (federatedscope.core.workers.client:610) INFO: {'Role': 'Client #35', 'Round': 4, 'Results_raw': {'train_total': 480, 'train_loss': 342.1551513671875, 'train_avg_loss': 0.712823232014974, 'train_acc': 0.48541666666666666}}
2025-08-18 18:30:14 (federatedscope.llm.trainer.reward_choice_trainer:244) INFO: [Stateless LR Controller] In Round #4, planning to set LR to 1.00e-09
2025-08-18 18:30:14 (federatedscope.llm.trainer.reward_choice_trainer:217) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=424, total=1694)
2025-08-18 18:30:14 (federatedscope.llm.trainer.trainer:343) INFO: Re-creating Accelerator for the new round.
2025-08-18 18:30:14 (federatedscope.llm.trainer.trainer:371) INFO: Round #4 - Initializing with LR: 1e-05
2025-08-18 18:30:14 (federatedscope.llm.trainer.reward_choice_trainer:270) INFO: Successfully applied new LR 1.00e-09 to the optimizer.
2025-08-18 18:30:37 (federatedscope.core.trainers.trainer:379) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-08-18 18:30:37 (federatedscope.llm.trainer.reward_choice_trainer:493) INFO: [train|reduce] total=480, loss_sum=333.399200, avg_loss=0.694582, acc=0.527083 (local_seen=120, local_corr=54)
2025-08-18 18:30:37 (federatedscope.llm.trainer.trainer:634) INFO: Accelerator object has been deleted.
2025-08-18 18:30:39 (federatedscope.core.workers.client:318) INFO: {'Role': 'Client #43', 'Round': 4, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 86.48392760753632, 'train_avg_loss': 0.7206993967294693, 'train_acc': 0.0}}
2025-08-18 18:30:39 (federatedscope.core.workers.client:328) INFO: {'Role': 'Client #43', 'Round': 4, 'Split': 'train', 'Local': False, 'Results_procs': ['train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank']}
2025-08-18 18:30:39 (federatedscope.core.workers.client:340) INFO: {'Role': 'Client #43', 'Round': 4, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 333.3992004394531, 'train_avg_loss': 0.694581667582194, 'train_acc': 0.5270833333333333}}
2025-08-18 18:30:39 (federatedscope.core.workers.client:610) INFO: {'Role': 'Client #43', 'Round': 4, 'Results_raw': {'train_total': 480, 'train_loss': 333.3992004394531, 'train_avg_loss': 0.694581667582194, 'train_acc': 0.5270833333333333}}
2025-08-18 18:30:39 (federatedscope.llm.trainer.reward_choice_trainer:244) INFO: [Stateless LR Controller] In Round #4, planning to set LR to 1.00e-09
2025-08-18 18:30:40 (federatedscope.llm.trainer.reward_choice_trainer:217) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=56, total=224)
2025-08-18 18:30:40 (federatedscope.llm.trainer.trainer:343) INFO: Re-creating Accelerator for the new round.
2025-08-18 18:30:40 (federatedscope.llm.trainer.trainer:371) INFO: Round #4 - Initializing with LR: 1e-05
2025-08-18 18:30:40 (federatedscope.llm.trainer.reward_choice_trainer:270) INFO: Successfully applied new LR 1.00e-09 to the optimizer.
2025-08-18 18:31:05 (federatedscope.core.trainers.trainer:379) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-08-18 18:31:05 (federatedscope.llm.trainer.reward_choice_trainer:493) INFO: [train|reduce] total=480, loss_sum=337.991089, avg_loss=0.704148, acc=0.514583 (local_seen=120, local_corr=71)
2025-08-18 18:31:05 (federatedscope.llm.trainer.trainer:634) INFO: Accelerator object has been deleted.
2025-08-18 18:31:07 (federatedscope.core.workers.client:318) INFO: {'Role': 'Client #22', 'Round': 4, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.0336982011795, 'train_avg_loss': 0.6919474850098292, 'train_acc': 0.0}}
2025-08-18 18:31:07 (federatedscope.core.workers.client:328) INFO: {'Role': 'Client #22', 'Round': 4, 'Split': 'train', 'Local': False, 'Results_procs': ['train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank']}
2025-08-18 18:31:07 (federatedscope.core.workers.client:340) INFO: {'Role': 'Client #22', 'Round': 4, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 337.9910888671875, 'train_avg_loss': 0.7041481018066407, 'train_acc': 0.5145833333333333}}
2025-08-18 18:31:07 (federatedscope.core.workers.client:610) INFO: {'Role': 'Client #22', 'Round': 4, 'Results_raw': {'train_total': 480, 'train_loss': 337.9910888671875, 'train_avg_loss': 0.7041481018066407, 'train_acc': 0.5145833333333333}}
2025-08-18 18:31:07 (federatedscope.llm.trainer.reward_choice_trainer:244) INFO: [Stateless LR Controller] In Round #4, planning to set LR to 1.00e-09
2025-08-18 18:31:07 (federatedscope.llm.trainer.reward_choice_trainer:217) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=272, total=1088)
2025-08-18 18:31:07 (federatedscope.llm.trainer.trainer:343) INFO: Re-creating Accelerator for the new round.
2025-08-18 18:31:07 (federatedscope.llm.trainer.trainer:371) INFO: Round #4 - Initializing with LR: 1e-05
2025-08-18 18:31:07 (federatedscope.llm.trainer.reward_choice_trainer:270) INFO: Successfully applied new LR 1.00e-09 to the optimizer.
2025-08-18 18:31:33 (federatedscope.core.trainers.trainer:379) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-08-18 18:31:33 (federatedscope.llm.trainer.reward_choice_trainer:493) INFO: [train|reduce] total=480, loss_sum=338.065369, avg_loss=0.704303, acc=0.506250 (local_seen=120, local_corr=61)
2025-08-18 18:31:33 (federatedscope.llm.trainer.trainer:634) INFO: Accelerator object has been deleted.
2025-08-18 18:31:34 (federatedscope.core.workers.client:318) INFO: {'Role': 'Client #7', 'Round': 4, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 86.58694726228714, 'train_avg_loss': 0.7215578938523929, 'train_acc': 0.0}}
2025-08-18 18:31:34 (federatedscope.core.workers.client:328) INFO: {'Role': 'Client #7', 'Round': 4, 'Split': 'train', 'Local': False, 'Results_procs': ['train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank']}
2025-08-18 18:31:34 (federatedscope.core.workers.client:340) INFO: {'Role': 'Client #7', 'Round': 4, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 338.06536865234375, 'train_avg_loss': 0.7043028513590495, 'train_acc': 0.50625}}
2025-08-18 18:31:34 (federatedscope.core.workers.client:610) INFO: {'Role': 'Client #7', 'Round': 4, 'Results_raw': {'train_total': 480, 'train_loss': 338.06536865234375, 'train_avg_loss': 0.7043028513590495, 'train_acc': 0.50625}}
2025-08-18 18:31:34 (federatedscope.core.workers.server:554) INFO: [in-place aggregation ] round=4
2025-08-18 18:31:35 (federatedscope.core.workers.server:432) INFO: ----------- Starting a new training round (Round #5) -------------
2025-08-18 18:31:36 (federatedscope.llm.trainer.reward_choice_trainer:244) INFO: [Stateless LR Controller] In Round #5, planning to set LR to 1.00e-10
2025-08-18 18:31:37 (federatedscope.llm.trainer.reward_choice_trainer:217) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=476, total=1901)
2025-08-18 18:31:37 (federatedscope.llm.trainer.trainer:343) INFO: Re-creating Accelerator for the new round.
2025-08-18 18:31:37 (federatedscope.llm.trainer.trainer:371) INFO: Round #5 - Initializing with LR: 1e-05
2025-08-18 18:31:37 (federatedscope.llm.trainer.reward_choice_trainer:270) INFO: Successfully applied new LR 1.00e-10 to the optimizer.
2025-08-18 18:31:59 (federatedscope.core.trainers.trainer:379) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-08-18 18:31:59 (federatedscope.llm.trainer.reward_choice_trainer:493) INFO: [train|reduce] total=480, loss_sum=333.325439, avg_loss=0.694428, acc=0.531250 (local_seen=120, local_corr=64)
2025-08-18 18:31:59 (federatedscope.llm.trainer.trainer:634) INFO: Accelerator object has been deleted.
2025-08-18 18:32:02 (federatedscope.core.workers.client:318) INFO: {'Role': 'Client #45', 'Round': 5, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.00154340267181, 'train_avg_loss': 0.7000128616889317, 'train_acc': 0.0}}
2025-08-18 18:32:02 (federatedscope.core.workers.client:328) INFO: {'Role': 'Client #45', 'Round': 5, 'Split': 'train', 'Local': False, 'Results_procs': ['train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank']}
2025-08-18 18:32:02 (federatedscope.core.workers.client:340) INFO: {'Role': 'Client #45', 'Round': 5, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 333.325439453125, 'train_avg_loss': 0.6944279988606771, 'train_acc': 0.53125}}
2025-08-18 18:32:02 (federatedscope.core.workers.client:610) INFO: {'Role': 'Client #45', 'Round': 5, 'Results_raw': {'train_total': 480, 'train_loss': 333.325439453125, 'train_avg_loss': 0.6944279988606771, 'train_acc': 0.53125}}
2025-08-18 18:32:02 (federatedscope.core.trainers.torch_trainer:140) INFO: DDP environment detected. Adding 'module.' prefix to incoming server keys.
2025-08-18 18:32:02 (federatedscope.llm.trainer.reward_choice_trainer:244) INFO: [Stateless LR Controller] In Round #5, planning to set LR to 1.00e-10
2025-08-18 18:32:02 (federatedscope.llm.trainer.reward_choice_trainer:217) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=424, total=1694)
2025-08-18 18:32:02 (federatedscope.llm.trainer.trainer:343) INFO: Re-creating Accelerator for the new round.
2025-08-18 18:32:02 (federatedscope.llm.trainer.trainer:371) INFO: Round #5 - Initializing with LR: 1e-05
2025-08-18 18:32:02 (federatedscope.llm.trainer.reward_choice_trainer:270) INFO: Successfully applied new LR 1.00e-10 to the optimizer.
2025-08-18 18:32:25 (federatedscope.core.trainers.trainer:379) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-08-18 18:32:25 (federatedscope.llm.trainer.reward_choice_trainer:493) INFO: [train|reduce] total=480, loss_sum=332.917053, avg_loss=0.693577, acc=0.535417 (local_seen=120, local_corr=55)
2025-08-18 18:32:25 (federatedscope.llm.trainer.trainer:634) INFO: Accelerator object has been deleted.
2025-08-18 18:32:27 (federatedscope.core.workers.client:318) INFO: {'Role': 'Client #43', 'Round': 5, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 86.21974205970764, 'train_avg_loss': 0.7184978504975637, 'train_acc': 0.0}}
2025-08-18 18:32:27 (federatedscope.core.workers.client:328) INFO: {'Role': 'Client #43', 'Round': 5, 'Split': 'train', 'Local': False, 'Results_procs': ['train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank']}
2025-08-18 18:32:27 (federatedscope.core.workers.client:340) INFO: {'Role': 'Client #43', 'Round': 5, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 332.91705322265625, 'train_avg_loss': 0.6935771942138672, 'train_acc': 0.5354166666666667}}
2025-08-18 18:32:27 (federatedscope.core.workers.client:610) INFO: {'Role': 'Client #43', 'Round': 5, 'Results_raw': {'train_total': 480, 'train_loss': 332.91705322265625, 'train_avg_loss': 0.6935771942138672, 'train_acc': 0.5354166666666667}}
2025-08-18 18:32:27 (federatedscope.llm.trainer.reward_choice_trainer:244) INFO: [Stateless LR Controller] In Round #5, planning to set LR to 1.00e-10
2025-08-18 18:32:28 (federatedscope.llm.trainer.reward_choice_trainer:217) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=72, total=285)
2025-08-18 18:32:28 (federatedscope.llm.trainer.trainer:343) INFO: Re-creating Accelerator for the new round.
2025-08-18 18:32:28 (federatedscope.llm.trainer.trainer:371) INFO: Round #5 - Initializing with LR: 1e-05
2025-08-18 18:32:28 (federatedscope.llm.trainer.reward_choice_trainer:270) INFO: Successfully applied new LR 1.00e-10 to the optimizer.
2025-08-18 18:32:52 (federatedscope.core.trainers.trainer:379) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-08-18 18:32:52 (federatedscope.llm.trainer.reward_choice_trainer:493) INFO: [train|reduce] total=480, loss_sum=344.560608, avg_loss=0.717835, acc=0.502083 (local_seen=120, local_corr=66)
2025-08-18 18:32:52 (federatedscope.llm.trainer.trainer:634) INFO: Accelerator object has been deleted.
2025-08-18 18:32:53 (federatedscope.core.workers.client:318) INFO: {'Role': 'Client #5', 'Round': 5, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.39125645160675, 'train_avg_loss': 0.6949271370967229, 'train_acc': 0.0}}
2025-08-18 18:32:53 (federatedscope.core.workers.client:328) INFO: {'Role': 'Client #5', 'Round': 5, 'Split': 'train', 'Local': False, 'Results_procs': ['train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank']}
2025-08-18 18:32:53 (federatedscope.core.workers.client:340) INFO: {'Role': 'Client #5', 'Round': 5, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 344.56060791015625, 'train_avg_loss': 0.7178345998128255, 'train_acc': 0.5020833333333333}}
2025-08-18 18:32:53 (federatedscope.core.workers.client:610) INFO: {'Role': 'Client #5', 'Round': 5, 'Results_raw': {'train_total': 480, 'train_loss': 344.56060791015625, 'train_avg_loss': 0.7178345998128255, 'train_acc': 0.5020833333333333}}
2025-08-18 18:32:53 (federatedscope.core.trainers.torch_trainer:140) INFO: DDP environment detected. Adding 'module.' prefix to incoming server keys.
2025-08-18 18:32:53 (federatedscope.llm.trainer.reward_choice_trainer:244) INFO: [Stateless LR Controller] In Round #5, planning to set LR to 1.00e-10
2025-08-18 18:32:53 (federatedscope.llm.trainer.reward_choice_trainer:217) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=600, total=2399)
2025-08-18 18:32:53 (federatedscope.llm.trainer.trainer:343) INFO: Re-creating Accelerator for the new round.
2025-08-18 18:32:53 (federatedscope.llm.trainer.trainer:371) INFO: Round #5 - Initializing with LR: 1e-05
2025-08-18 18:32:54 (federatedscope.llm.trainer.reward_choice_trainer:270) INFO: Successfully applied new LR 1.00e-10 to the optimizer.
2025-08-18 18:33:17 (federatedscope.core.trainers.trainer:379) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-08-18 18:33:17 (federatedscope.llm.trainer.reward_choice_trainer:493) INFO: [train|reduce] total=480, loss_sum=332.544952, avg_loss=0.692802, acc=0.568750 (local_seen=120, local_corr=67)
2025-08-18 18:33:17 (federatedscope.llm.trainer.trainer:634) INFO: Accelerator object has been deleted.
2025-08-18 18:33:18 (federatedscope.core.workers.client:318) INFO: {'Role': 'Client #20', 'Round': 5, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.53478491306305, 'train_avg_loss': 0.7044565409421921, 'train_acc': 0.0}}
2025-08-18 18:33:18 (federatedscope.core.workers.client:328) INFO: {'Role': 'Client #20', 'Round': 5, 'Split': 'train', 'Local': False, 'Results_procs': ['train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank']}
2025-08-18 18:33:18 (federatedscope.core.workers.client:340) INFO: {'Role': 'Client #20', 'Round': 5, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 332.5449523925781, 'train_avg_loss': 0.6928019841512044, 'train_acc': 0.56875}}
2025-08-18 18:33:18 (federatedscope.core.workers.client:610) INFO: {'Role': 'Client #20', 'Round': 5, 'Results_raw': {'train_total': 480, 'train_loss': 332.5449523925781, 'train_avg_loss': 0.6928019841512044, 'train_acc': 0.56875}}
2025-08-18 18:33:18 (federatedscope.core.trainers.torch_trainer:140) INFO: DDP environment detected. Adding 'module.' prefix to incoming server keys.
2025-08-18 18:33:18 (federatedscope.llm.trainer.reward_choice_trainer:244) INFO: [Stateless LR Controller] In Round #5, planning to set LR to 1.00e-10
2025-08-18 18:33:19 (federatedscope.llm.trainer.reward_choice_trainer:217) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1979, total=7916)
2025-08-18 18:33:19 (federatedscope.llm.trainer.trainer:343) INFO: Re-creating Accelerator for the new round.
2025-08-18 18:33:19 (federatedscope.llm.trainer.trainer:371) INFO: Round #5 - Initializing with LR: 1e-05
2025-08-18 18:33:19 (federatedscope.llm.trainer.reward_choice_trainer:270) INFO: Successfully applied new LR 1.00e-10 to the optimizer.
2025-08-18 18:33:45 (federatedscope.core.trainers.trainer:379) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-08-18 18:33:45 (federatedscope.llm.trainer.reward_choice_trainer:493) INFO: [train|reduce] total=480, loss_sum=339.685791, avg_loss=0.707679, acc=0.531250 (local_seen=120, local_corr=67)
2025-08-18 18:33:45 (federatedscope.llm.trainer.trainer:634) INFO: Accelerator object has been deleted.
2025-08-18 18:33:47 (federatedscope.core.workers.client:318) INFO: {'Role': 'Client #44', 'Round': 5, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.41772216558456, 'train_avg_loss': 0.6868143513798713, 'train_acc': 0.0}}
2025-08-18 18:33:47 (federatedscope.core.workers.client:328) INFO: {'Role': 'Client #44', 'Round': 5, 'Split': 'train', 'Local': False, 'Results_procs': ['train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank']}
2025-08-18 18:33:47 (federatedscope.core.workers.client:340) INFO: {'Role': 'Client #44', 'Round': 5, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 339.685791015625, 'train_avg_loss': 0.7076787312825521, 'train_acc': 0.53125}}
2025-08-18 18:33:47 (federatedscope.core.workers.client:610) INFO: {'Role': 'Client #44', 'Round': 5, 'Results_raw': {'train_total': 480, 'train_loss': 339.685791015625, 'train_avg_loss': 0.7076787312825521, 'train_acc': 0.53125}}
2025-08-18 18:33:47 (federatedscope.core.workers.server:554) INFO: [in-place aggregation ] round=5
2025-08-18 18:33:47 (federatedscope.core.workers.server:432) INFO: ----------- Starting a new training round (Round #6) -------------
2025-08-18 18:33:48 (federatedscope.core.trainers.torch_trainer:140) INFO: DDP environment detected. Adding 'module.' prefix to incoming server keys.
2025-08-18 18:33:48 (federatedscope.llm.trainer.reward_choice_trainer:244) INFO: [Stateless LR Controller] In Round #6, planning to set LR to 1.00e-10
2025-08-18 18:33:48 (federatedscope.llm.trainer.reward_choice_trainer:217) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=920, total=3679)
2025-08-18 18:33:48 (federatedscope.llm.trainer.trainer:343) INFO: Re-creating Accelerator for the new round.
2025-08-18 18:33:48 (federatedscope.llm.trainer.trainer:371) INFO: Round #6 - Initializing with LR: 1e-05
2025-08-18 18:33:49 (federatedscope.llm.trainer.reward_choice_trainer:270) INFO: Successfully applied new LR 1.00e-10 to the optimizer.
2025-08-18 18:34:13 (federatedscope.core.trainers.trainer:379) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-08-18 18:34:13 (federatedscope.llm.trainer.reward_choice_trainer:493) INFO: [train|reduce] total=480, loss_sum=340.269745, avg_loss=0.708895, acc=0.481250 (local_seen=120, local_corr=56)
2025-08-18 18:34:13 (federatedscope.llm.trainer.trainer:634) INFO: Accelerator object has been deleted.
2025-08-18 18:34:16 (federatedscope.core.workers.client:318) INFO: {'Role': 'Client #31', 'Round': 6, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.5495622754097, 'train_avg_loss': 0.6962463522950808, 'train_acc': 0.0}}
2025-08-18 18:34:16 (federatedscope.core.workers.client:328) INFO: {'Role': 'Client #31', 'Round': 6, 'Split': 'train', 'Local': False, 'Results_procs': ['train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank']}
2025-08-18 18:34:16 (federatedscope.core.workers.client:340) INFO: {'Role': 'Client #31', 'Round': 6, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 340.2697448730469, 'train_avg_loss': 0.7088953018188476, 'train_acc': 0.48125}}
2025-08-18 18:34:16 (federatedscope.core.workers.client:610) INFO: {'Role': 'Client #31', 'Round': 6, 'Results_raw': {'train_total': 480, 'train_loss': 340.2697448730469, 'train_avg_loss': 0.7088953018188476, 'train_acc': 0.48125}}
2025-08-18 18:34:16 (federatedscope.core.trainers.torch_trainer:140) INFO: DDP environment detected. Adding 'module.' prefix to incoming server keys.
2025-08-18 18:34:16 (federatedscope.llm.trainer.reward_choice_trainer:244) INFO: [Stateless LR Controller] In Round #6, planning to set LR to 1.00e-10
2025-08-18 18:34:16 (federatedscope.llm.trainer.reward_choice_trainer:217) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=272, total=1088)
2025-08-18 18:34:16 (federatedscope.llm.trainer.trainer:343) INFO: Re-creating Accelerator for the new round.
2025-08-18 18:34:16 (federatedscope.llm.trainer.trainer:371) INFO: Round #6 - Initializing with LR: 1e-05
2025-08-18 18:34:16 (federatedscope.llm.trainer.reward_choice_trainer:270) INFO: Successfully applied new LR 1.00e-10 to the optimizer.
2025-08-18 18:34:40 (federatedscope.core.trainers.trainer:379) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-08-18 18:34:40 (federatedscope.llm.trainer.reward_choice_trainer:493) INFO: [train|reduce] total=480, loss_sum=338.137604, avg_loss=0.704453, acc=0.504167 (local_seen=120, local_corr=60)
2025-08-18 18:34:40 (federatedscope.llm.trainer.trainer:634) INFO: Accelerator object has been deleted.
2025-08-18 18:34:41 (federatedscope.core.workers.client:318) INFO: {'Role': 'Client #7', 'Round': 6, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 86.91364419460297, 'train_avg_loss': 0.7242803682883581, 'train_acc': 0.0}}
2025-08-18 18:34:41 (federatedscope.core.workers.client:328) INFO: {'Role': 'Client #7', 'Round': 6, 'Split': 'train', 'Local': False, 'Results_procs': ['train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank']}
2025-08-18 18:34:41 (federatedscope.core.workers.client:340) INFO: {'Role': 'Client #7', 'Round': 6, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 338.1376037597656, 'train_avg_loss': 0.7044533411661784, 'train_acc': 0.5041666666666667}}
2025-08-18 18:34:41 (federatedscope.core.workers.client:610) INFO: {'Role': 'Client #7', 'Round': 6, 'Results_raw': {'train_total': 480, 'train_loss': 338.1376037597656, 'train_avg_loss': 0.7044533411661784, 'train_acc': 0.5041666666666667}}
2025-08-18 18:34:41 (federatedscope.core.trainers.torch_trainer:140) INFO: DDP environment detected. Adding 'module.' prefix to incoming server keys.
2025-08-18 18:34:41 (federatedscope.llm.trainer.reward_choice_trainer:244) INFO: [Stateless LR Controller] In Round #6, planning to set LR to 1.00e-10
2025-08-18 18:34:42 (federatedscope.llm.trainer.reward_choice_trainer:217) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1471, total=5883)
2025-08-18 18:34:42 (federatedscope.llm.trainer.trainer:343) INFO: Re-creating Accelerator for the new round.
2025-08-18 18:34:42 (federatedscope.llm.trainer.trainer:371) INFO: Round #6 - Initializing with LR: 1e-05
2025-08-18 18:34:42 (federatedscope.llm.trainer.reward_choice_trainer:270) INFO: Successfully applied new LR 1.00e-10 to the optimizer.
2025-08-18 18:35:05 (federatedscope.core.trainers.trainer:379) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-08-18 18:35:05 (federatedscope.llm.trainer.reward_choice_trainer:493) INFO: [train|reduce] total=480, loss_sum=338.766571, avg_loss=0.705764, acc=0.504167 (local_seen=120, local_corr=60)
2025-08-18 18:35:05 (federatedscope.llm.trainer.trainer:634) INFO: Accelerator object has been deleted.
2025-08-18 18:35:07 (federatedscope.core.workers.client:318) INFO: {'Role': 'Client #17', 'Round': 6, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 85.28795349597931, 'train_avg_loss': 0.7107329457998276, 'train_acc': 0.0}}
2025-08-18 18:35:07 (federatedscope.core.workers.client:328) INFO: {'Role': 'Client #17', 'Round': 6, 'Split': 'train', 'Local': False, 'Results_procs': ['train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank']}
2025-08-18 18:35:07 (federatedscope.core.workers.client:340) INFO: {'Role': 'Client #17', 'Round': 6, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 338.7665710449219, 'train_avg_loss': 0.7057636896769206, 'train_acc': 0.5041666666666667}}
2025-08-18 18:35:07 (federatedscope.core.workers.client:610) INFO: {'Role': 'Client #17', 'Round': 6, 'Results_raw': {'train_total': 480, 'train_loss': 338.7665710449219, 'train_avg_loss': 0.7057636896769206, 'train_acc': 0.5041666666666667}}
2025-08-18 18:35:07 (federatedscope.core.trainers.torch_trainer:140) INFO: DDP environment detected. Adding 'module.' prefix to incoming server keys.
2025-08-18 18:35:07 (federatedscope.llm.trainer.reward_choice_trainer:244) INFO: [Stateless LR Controller] In Round #6, planning to set LR to 1.00e-10
2025-08-18 18:35:07 (federatedscope.llm.trainer.reward_choice_trainer:217) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1122, total=4486)
2025-08-18 18:35:07 (federatedscope.llm.trainer.trainer:343) INFO: Re-creating Accelerator for the new round.
2025-08-18 18:35:07 (federatedscope.llm.trainer.trainer:371) INFO: Round #6 - Initializing with LR: 1e-05
2025-08-18 18:35:08 (federatedscope.llm.trainer.reward_choice_trainer:270) INFO: Successfully applied new LR 1.00e-10 to the optimizer.
2025-08-18 18:35:32 (federatedscope.core.trainers.trainer:379) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-08-18 18:35:32 (federatedscope.llm.trainer.reward_choice_trainer:493) INFO: [train|reduce] total=480, loss_sum=342.408234, avg_loss=0.713350, acc=0.495833 (local_seen=120, local_corr=62)
2025-08-18 18:35:32 (federatedscope.llm.trainer.trainer:634) INFO: Accelerator object has been deleted.
2025-08-18 18:35:34 (federatedscope.core.workers.client:318) INFO: {'Role': 'Client #34', 'Round': 6, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 85.31778120994568, 'train_avg_loss': 0.7109815100828807, 'train_acc': 0.0}}
2025-08-18 18:35:34 (federatedscope.core.workers.client:328) INFO: {'Role': 'Client #34', 'Round': 6, 'Split': 'train', 'Local': False, 'Results_procs': ['train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank']}
2025-08-18 18:35:34 (federatedscope.core.workers.client:340) INFO: {'Role': 'Client #34', 'Round': 6, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 342.4082336425781, 'train_avg_loss': 0.713350486755371, 'train_acc': 0.49583333333333335}}
2025-08-18 18:35:34 (federatedscope.core.workers.client:610) INFO: {'Role': 'Client #34', 'Round': 6, 'Results_raw': {'train_total': 480, 'train_loss': 342.4082336425781, 'train_avg_loss': 0.713350486755371, 'train_acc': 0.49583333333333335}}
2025-08-18 18:35:34 (federatedscope.core.trainers.torch_trainer:140) INFO: DDP environment detected. Adding 'module.' prefix to incoming server keys.
2025-08-18 18:35:34 (federatedscope.llm.trainer.reward_choice_trainer:244) INFO: [Stateless LR Controller] In Round #6, planning to set LR to 1.00e-10
2025-08-18 18:35:35 (federatedscope.llm.trainer.reward_choice_trainer:217) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=56, total=224)
2025-08-18 18:35:35 (federatedscope.llm.trainer.trainer:343) INFO: Re-creating Accelerator for the new round.
2025-08-18 18:35:35 (federatedscope.llm.trainer.trainer:371) INFO: Round #6 - Initializing with LR: 1e-05
2025-08-18 18:35:35 (federatedscope.llm.trainer.reward_choice_trainer:270) INFO: Successfully applied new LR 1.00e-10 to the optimizer.
2025-08-18 18:36:01 (federatedscope.core.trainers.trainer:379) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-08-18 18:36:01 (federatedscope.llm.trainer.reward_choice_trainer:493) INFO: [train|reduce] total=480, loss_sum=338.296295, avg_loss=0.704784, acc=0.491667 (local_seen=120, local_corr=71)
2025-08-18 18:36:01 (federatedscope.llm.trainer.trainer:634) INFO: Accelerator object has been deleted.
2025-08-18 18:36:02 (federatedscope.core.workers.client:318) INFO: {'Role': 'Client #22', 'Round': 6, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.77883350849152, 'train_avg_loss': 0.6898236125707626, 'train_acc': 0.0}}
2025-08-18 18:36:02 (federatedscope.core.workers.client:328) INFO: {'Role': 'Client #22', 'Round': 6, 'Split': 'train', 'Local': False, 'Results_procs': ['train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank']}
2025-08-18 18:36:02 (federatedscope.core.workers.client:340) INFO: {'Role': 'Client #22', 'Round': 6, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 338.2962951660156, 'train_avg_loss': 0.7047839482625325, 'train_acc': 0.49166666666666664}}
2025-08-18 18:36:02 (federatedscope.core.workers.client:610) INFO: {'Role': 'Client #22', 'Round': 6, 'Results_raw': {'train_total': 480, 'train_loss': 338.2962951660156, 'train_avg_loss': 0.7047839482625325, 'train_acc': 0.49166666666666664}}
2025-08-18 18:36:02 (federatedscope.core.workers.server:554) INFO: [in-place aggregation ] round=6
2025-08-18 18:36:03 (federatedscope.core.workers.server:432) INFO: ----------- Starting a new training round (Round #7) -------------
2025-08-18 18:36:04 (federatedscope.llm.trainer.reward_choice_trainer:244) INFO: [Stateless LR Controller] In Round #7, planning to set LR to 1.00e-10
2025-08-18 18:36:04 (federatedscope.llm.trainer.reward_choice_trainer:217) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=395, total=1580)
2025-08-18 18:36:04 (federatedscope.llm.trainer.trainer:343) INFO: Re-creating Accelerator for the new round.
2025-08-18 18:36:04 (federatedscope.llm.trainer.trainer:371) INFO: Round #7 - Initializing with LR: 1e-05
2025-08-18 18:36:05 (federatedscope.llm.trainer.reward_choice_trainer:270) INFO: Successfully applied new LR 1.00e-10 to the optimizer.
2025-08-18 18:36:28 (federatedscope.core.trainers.trainer:379) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-08-18 18:36:28 (federatedscope.llm.trainer.reward_choice_trainer:493) INFO: [train|reduce] total=480, loss_sum=333.033813, avg_loss=0.693820, acc=0.558333 (local_seen=120, local_corr=68)
2025-08-18 18:36:28 (federatedscope.llm.trainer.trainer:634) INFO: Accelerator object has been deleted.
2025-08-18 18:36:30 (federatedscope.core.workers.client:318) INFO: {'Role': 'Client #51', 'Round': 7, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.0319853425026, 'train_avg_loss': 0.6919332111875216, 'train_acc': 0.0}}
2025-08-18 18:36:30 (federatedscope.core.workers.client:328) INFO: {'Role': 'Client #51', 'Round': 7, 'Split': 'train', 'Local': False, 'Results_procs': ['train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank']}
2025-08-18 18:36:30 (federatedscope.core.workers.client:340) INFO: {'Role': 'Client #51', 'Round': 7, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 333.0338134765625, 'train_avg_loss': 0.6938204447428385, 'train_acc': 0.5583333333333333}}
2025-08-18 18:36:30 (federatedscope.core.workers.client:610) INFO: {'Role': 'Client #51', 'Round': 7, 'Results_raw': {'train_total': 480, 'train_loss': 333.0338134765625, 'train_avg_loss': 0.6938204447428385, 'train_acc': 0.5583333333333333}}
2025-08-18 18:36:30 (federatedscope.llm.trainer.reward_choice_trainer:244) INFO: [Stateless LR Controller] In Round #7, planning to set LR to 1.00e-10
2025-08-18 18:36:30 (federatedscope.llm.trainer.reward_choice_trainer:217) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=699, total=2793)
2025-08-18 18:36:30 (federatedscope.llm.trainer.trainer:343) INFO: Re-creating Accelerator for the new round.
2025-08-18 18:36:30 (federatedscope.llm.trainer.trainer:371) INFO: Round #7 - Initializing with LR: 1e-05
2025-08-18 18:36:30 (federatedscope.llm.trainer.reward_choice_trainer:270) INFO: Successfully applied new LR 1.00e-10 to the optimizer.
2025-08-18 18:36:55 (federatedscope.core.trainers.trainer:379) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-08-18 18:36:55 (federatedscope.llm.trainer.reward_choice_trainer:493) INFO: [train|reduce] total=480, loss_sum=343.250427, avg_loss=0.715105, acc=0.489583 (local_seen=120, local_corr=62)
2025-08-18 18:36:55 (federatedscope.llm.trainer.trainer:634) INFO: Accelerator object has been deleted.
2025-08-18 18:36:56 (federatedscope.core.workers.client:318) INFO: {'Role': 'Client #1', 'Round': 7, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 85.22551184892654, 'train_avg_loss': 0.7102125987410546, 'train_acc': 0.0}}
2025-08-18 18:36:56 (federatedscope.core.workers.client:328) INFO: {'Role': 'Client #1', 'Round': 7, 'Split': 'train', 'Local': False, 'Results_procs': ['train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank']}
2025-08-18 18:36:56 (federatedscope.core.workers.client:340) INFO: {'Role': 'Client #1', 'Round': 7, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 343.25042724609375, 'train_avg_loss': 0.7151050567626953, 'train_acc': 0.4895833333333333}}
2025-08-18 18:36:56 (federatedscope.core.workers.client:610) INFO: {'Role': 'Client #1', 'Round': 7, 'Results_raw': {'train_total': 480, 'train_loss': 343.25042724609375, 'train_avg_loss': 0.7151050567626953, 'train_acc': 0.4895833333333333}}
2025-08-18 18:36:56 (federatedscope.llm.trainer.reward_choice_trainer:244) INFO: [Stateless LR Controller] In Round #7, planning to set LR to 1.00e-10
2025-08-18 18:36:57 (federatedscope.llm.trainer.reward_choice_trainer:217) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1162, total=4647)
2025-08-18 18:36:57 (federatedscope.llm.trainer.trainer:343) INFO: Re-creating Accelerator for the new round.
2025-08-18 18:36:57 (federatedscope.llm.trainer.trainer:371) INFO: Round #7 - Initializing with LR: 1e-05
2025-08-18 18:36:57 (federatedscope.llm.trainer.reward_choice_trainer:270) INFO: Successfully applied new LR 1.00e-10 to the optimizer.
2025-08-18 18:37:23 (federatedscope.core.trainers.trainer:379) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-08-18 18:37:23 (federatedscope.llm.trainer.reward_choice_trainer:493) INFO: [train|reduce] total=480, loss_sum=343.761658, avg_loss=0.716170, acc=0.497917 (local_seen=120, local_corr=66)
2025-08-18 18:37:23 (federatedscope.llm.trainer.trainer:634) INFO: Accelerator object has been deleted.
2025-08-18 18:37:25 (federatedscope.core.workers.client:318) INFO: {'Role': 'Client #25', 'Round': 7, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.84521996974945, 'train_avg_loss': 0.7070434997479121, 'train_acc': 0.0}}
2025-08-18 18:37:25 (federatedscope.core.workers.client:328) INFO: {'Role': 'Client #25', 'Round': 7, 'Split': 'train', 'Local': False, 'Results_procs': ['train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank']}
2025-08-18 18:37:25 (federatedscope.core.workers.client:340) INFO: {'Role': 'Client #25', 'Round': 7, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 343.76165771484375, 'train_avg_loss': 0.7161701202392579, 'train_acc': 0.4979166666666667}}
2025-08-18 18:37:25 (federatedscope.core.workers.client:610) INFO: {'Role': 'Client #25', 'Round': 7, 'Results_raw': {'train_total': 480, 'train_loss': 343.76165771484375, 'train_avg_loss': 0.7161701202392579, 'train_acc': 0.4979166666666667}}
2025-08-18 18:37:25 (federatedscope.core.trainers.torch_trainer:140) INFO: DDP environment detected. Adding 'module.' prefix to incoming server keys.
2025-08-18 18:37:25 (federatedscope.llm.trainer.reward_choice_trainer:244) INFO: [Stateless LR Controller] In Round #7, planning to set LR to 1.00e-10
2025-08-18 18:37:25 (federatedscope.llm.trainer.reward_choice_trainer:217) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=600, total=2399)
2025-08-18 18:37:25 (federatedscope.llm.trainer.trainer:343) INFO: Re-creating Accelerator for the new round.
2025-08-18 18:37:25 (federatedscope.llm.trainer.trainer:371) INFO: Round #7 - Initializing with LR: 1e-05
2025-08-18 18:37:26 (federatedscope.llm.trainer.reward_choice_trainer:270) INFO: Successfully applied new LR 1.00e-10 to the optimizer.
2025-08-18 18:37:49 (federatedscope.core.trainers.trainer:379) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-08-18 18:37:49 (federatedscope.llm.trainer.reward_choice_trainer:493) INFO: [train|reduce] total=480, loss_sum=332.908844, avg_loss=0.693560, acc=0.562500 (local_seen=120, local_corr=66)
2025-08-18 18:37:49 (federatedscope.llm.trainer.trainer:634) INFO: Accelerator object has been deleted.
2025-08-18 18:37:51 (federatedscope.core.workers.client:318) INFO: {'Role': 'Client #20', 'Round': 7, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.54291927814484, 'train_avg_loss': 0.7045243273178736, 'train_acc': 0.0}}
2025-08-18 18:37:51 (federatedscope.core.workers.client:328) INFO: {'Role': 'Client #20', 'Round': 7, 'Split': 'train', 'Local': False, 'Results_procs': ['train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank', 'train_total', 'train_loss', 'train_avg_loss', 'train_acc', 'rank']}
2025-08-18 18:37:51 (federatedscope.core.workers.client:340) INFO: {'Role': 'Client #20', 'Round': 7, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 332.9088439941406, 'train_avg_loss': 0.6935600916544596, 'train_acc': 0.5625}}
2025-08-18 18:37:51 (federatedscope.core.workers.client:610) INFO: {'Role': 'Client #20', 'Round': 7, 'Results_raw': {'train_total': 480, 'train_loss': 332.9088439941406, 'train_avg_loss': 0.6935600916544596, 'train_acc': 0.5625}}
2025-08-18 18:37:51 (federatedscope.llm.trainer.reward_choice_trainer:244) INFO: [Stateless LR Controller] In Round #7, planning to set LR to 1.00e-10
2025-08-18 18:37:51 (federatedscope.llm.trainer.reward_choice_trainer:217) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3638, total=14550)
2025-08-18 18:37:51 (federatedscope.llm.trainer.trainer:343) INFO: Re-creating Accelerator for the new round.
2025-08-18 18:37:51 (federatedscope.llm.trainer.trainer:371) INFO: Round #7 - Initializing with LR: 1e-05
2025-08-18 18:37:52 (federatedscope.llm.trainer.reward_choice_trainer:270) INFO: Successfully applied new LR 1.00e-10 to the optimizer.
